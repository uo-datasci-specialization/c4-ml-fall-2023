---
title: "An Overview of Linear Regression and Bias-Variance Tradeoff in Predictive Modeling"
subtitle: ""
author: "Cengiz Zopluoglu"
institute: "College of Education, University of Oregon"
date: "Oct 17, 2022 <br> Eugene, OR"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    #self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<style>

.blockquote {
  border-left: 5px solid #007935;
  background: #f9f9f9;
  padding: 10px;
  padding-left: 30px;
  margin-left: 16px;
  margin-right: 0;
  border-radius: 0px 4px 4px 0px;
}

#infobox {
  padding: 1em 1em 1em 4em;
  margin-bottom: 10px;
  border: 2px solid black;
  border-radius: 10px;
  background: #E6F6DC 5px center/3em no-repeat;
}

.centering[
  float: center;
]

.left-column2 {
  width: 50%;
  height: 92%;
  float: left;
  padding-top: 1em;
}

.right-column2 {
  width: 50%;
  float: right;
  padding-top: 1em;
}

.remark-code {
  font-size: 18px;
}

.tiny .remark-code { /*Change made here*/
  font-size: 75% !important;
}

.tiny2 .remark-code { /*Change made here*/
  font-size: 50% !important;
}

.indent {
  margin-left: 3em;
}

.single {
  line-height: 1 ;
}


.double {
  line-height: 2 ;
}

.title-slide h1 {
  padding-top: 0px;
  font-size: 40px;
  text-align: center;
  padding-bottom: 18px;
  margin-bottom: 18px;
}

.title-slide h2 {
  font-size: 30px;
  text-align: center;
  padding-top: 0px;
  margin-top: 0px;
}

.title-slide h3 {
  font-size: 30px;
  color: #26272A;
  text-align: center;
  text-shadow: none;
  padding: 10px;
  margin: 10px;
  line-height: 1.2;
}

</style>

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, 
       xaringanthemer,DT,dplyr,gridExtra, plotly)

#i_am("B:/UO Teaching/EDUC614/Winter22/Slide Template/template.rmd")

red_pink <- "#e64173"
turquoise = "#20B2AA"
orange = "#FFA500"
red = "#fb6107"
blue = "#3b3b9a"
green = "#8bb174"
grey_light = "grey70"
grey_mid = "grey50"
grey_dark = "grey20"
purple = "#6A5ACD"
slate = "#314f4f"

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".red-pink" = list(color= "red_pink"),
  ".grey-light" = list(color= "grey_light"),
  ".purple" = list(color = "purple"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 6.75,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})

options(knitr.table.format = "html")

options(width = 120)

options(max.print = 100)

require(here)
```

### Today's Goals:

- An Overview of Linear Regression
  
  - Model Description
  
  - Model Estimation
  
  - Performance Evaluation

- Understanding the concept of bias - variance tradeoff for predictive models

- How to balance the model bias and variance when building predictive models

---

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

<center>

# An Overview of Linear Regression
---

- The prediction algorithms are classified into two main categories: *supervised* and *unsupervised*. 

- **Supervised algorithms** are used when the dataset has an actual outcome of interest to predict (labels), and the goal is to build the "best" model predicting the outcome of interest. 

- **Unsupervised algorithms** are used when the dataset doesn't have an outcome of interest. The goal is typically to identify similar groups of observations (rows of data) or similar groups of variables (columns of data) in data. 

- This course will cover several *supervised* algorithms and Linear Regression is one of the most straightforward supervised algorithms and the easiest to interpret.

---


## Model Description

The linear regression model with $P$ predictors and an outcome variable $Y$ can be written as

$$Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \epsilon$$

In this model, 
  
  - $Y$ represents the observed value for the outcome of an observation, 
  
  - $X_{p}$ represents the observed value of the $p^{th}$ variable for the same observation, 
  
  - $\beta_p$ is the associated model parameter for the $p^{th}$ variable,
  
  - and $\epsilon$ is the model error (residual) for the observation.

This model includes only the main effects of each predictor.

---

- The previous model can be easily extended by including quadratic or higher-order polynomial terms for all (or a specific subset of) predictors. 

- A model with the first-order, second-order, and third-order polynomial terms for all predictors can be written as 

$$Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \sum_{k=1}^{P} \beta_{k+P}X_{k}^2 + \sum_{m=1}^{P} \beta_{m+2P}X_{m}^3 + \epsilon$$
- Example: A model with only main effects

$$Y = \beta_0  + \beta_1X_{1} + \beta_2X_{2} + \beta_3X_{3}+ \epsilon.$$

- Example: A model with polynomial terms up to the 3rd degree added:

$$Y = \beta_0  + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \\ \beta_4X_1^2 + \beta_5X_2^2 + \beta_6X_3^2+ \\ \beta_{7}X_1^3 + \beta_{8}X_2^3 + \beta_{9}X_3^3$$


---

- The effect of predictor variables on the outcome variable is sometimes not additive. 

- When the effect of one predictor on the response variable depends on the levels of another predictor,  non-additive effects (a.k.a. interaction effects) can also be added to the model. 

- The interaction effects can be first-order interactions (interaction between two variables, e.g., $X_1*X_2$), second-order interactions ($X_1*X_2*X_3$), or higher orders.  I

- For instance, the model below also adds the first-order interactions.

$$Y = \beta_0  + \sum_{p=1}^{P} \beta_pX_{p} + \sum_{k=1}^{P} \beta_{k+P}X_{k}^2 + \sum_{m=1}^{P} \beta_{m+2P}X_{m}^3 + \sum_{i=1}^{P}\sum_{j=i+1}^{P}\beta_{i,j}X_iX_j + \epsilon$$
- A model with both interaction terms and polynomial terms up to the 3rd degree added:

$$Y = \beta_0  + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \\ \beta_4X_1^2 + \beta_5X_2^2 + \beta_6X_3^2+ \\ \beta_{7}X_1^3 + \beta_{8}X_2^3 + \beta_{9}X_3^3+ \\ \beta_{1,2}X_1X_2+ \beta_{1,3}X_1X_3 + \beta_{2,3}X_2X_3 + \epsilon$$

---

## Model Estimation

- Suppose that we would like to predict the target readability score for a given text from the Feature 220.

- Note that there are 768 features extracted from the NLP model as numerical embeddings. For the sake of simplicity, we will only use one of them (Feature 220).

- Below is a scatterplot to show the relationship between these two variables for a random sample of 20 observations. 

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE}

readability_sub <- read.csv(here('data/readability_sub.csv'),header=TRUE)

```

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=6,fig.height=3}

ggplot(data=readability_sub,aes(x=V220,y=target))+
  geom_point()+
  xlab('Feature 220')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(-1,1))+
  ylim(c(-4,2))

```

---

- Consider a simple linear regression model

  - Outcome: the readability score is the outcome ( $Y$ )
  
  - Predictor: Feature 220 ( $X$ ) 
  
- Our regression model is

$$Y = \beta_0  + \beta_1X + \epsilon$$

- The set of coefficients, { $\beta_0,\beta_1$ } , represents a linear line. 

- We can write any set of { $\beta_0,\beta_1$ } coefficients and use it as our model. 

- For instance, suppose we guesstimate that these coefficients are { $\beta_0,\beta_1$ } = {-1.5,2}. Then, my model would be

$$Y = -1.5  + 2X + \epsilon$$
---

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=8,fig.height=6}

ggplot(data=readability_sub,aes(x=V220,y=target))+
  geom_point()+
  xlab('Feature 220')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(-1,1))+
  ylim(c(-4,2))+
  geom_abline(intercept=-1.5,slope=2,lty=2)
  
```

---

We can predict the target readability score for any observation in the dataset using this model. 

$$Y_{(1)} = -1.5  + 2X_{(1)} + \epsilon_{(1)}.$$

$$\hat{Y}_{(1)} =  -1.5 + 2*(-0.139) = -1.778$$

$$\hat{\epsilon}_{(1)} = -2.062 - (-1.778) =  -0.284 $$

The discrepancy between the observed value and the model prediction is the model error (residual) for the first observation and captured in the $\epsilon_{(1)}$ term in the model.

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=6,fig.height=3.5}

x1 = readability_sub[1,c('V220')]
y1 = readability_sub[1,c('V220')]*2 - 1.5

ggplot(data=readability_sub,aes(x=V220,y=target))+
    geom_point(col='gray')+
  xlab('Feature 220')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(-1,1))+
  ylim(c(-4,2))+
  geom_abline(intercept=-1.5,slope=2,lty=2)+
  geom_point(x=x1,y=y1,color='blue',cex=2)+
  geom_point(x=readability_sub[1,c('V220')],y=readability_sub[1,c('target')],color='black',cex=2)+
  geom_segment(x=x1,y=y1,xend=x1,yend=readability_sub[1,c('target')])
  
  
```

---

We can do the same thing for the second observation.

$$Y_{(2)} = -1.5  + 2X_{(2)} + \epsilon_{(2)}.$$

$$\hat{Y}_{(2)} =  -1.5 + 2*(0.218) = -1.065$$

$$\hat{\epsilon}_{(2)} = 0.583 - (-1.065) =  1.648 $$

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=6,fig.height=3.5}

x1 = readability_sub[2,c('V220')]
y1 = readability_sub[2,c('V220')]*2 - 1.5

ggplot(data=readability_sub,aes(x=V220,y=target))+
    geom_point(col='gray')+
  xlab('Feature 220')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(-1,1))+
  ylim(c(-4,2))+
  geom_abline(intercept=-1.5,slope=2,lty=2)+
  geom_point(x=x1,y=y1,color='blue',cex=2)+
  geom_point(x=readability_sub[2,c('V220')],y=readability_sub[2,c('target')],color='black',cex=2)+
  geom_segment(x=x1,y=y1,xend=x1,yend=readability_sub[2,c('target')])
```

---

Using a similar approach, we can calculate the model error for every observation.

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=8,fig.height=8}

d <-  readability_sub[,c('V220','target')]

d$predicted <- -1.5 + 2*d$V220
d$error     <- d$target - d$predicted
```

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=8,fig.height=6}

p <- ggplot(data=d,aes(x=V220,y=target))+
  geom_point()+
  xlab('Feature 220')+
  ylab('Readability Score')+
  theme_bw()+
  xlim(c(-1,1))+
  ylim(c(-4,2))+
  geom_abline(intercept=-1.5,slope=2,lty=2,color='gray')

for(i in 1:nrow(d)){
 p <- p + geom_segment(x=d[i,1],y=d[i,2],xend=d[i,1],yend=d[i,3],lty=2)
}  

p
```

---

.single[

```{r, echo=TRUE,eval=FALSE,message=FALSE, warning=FALSE,fig.width=8,fig.height=8}
d <-  readability_sub[,c('V220','target')]

d$predicted <- -1.5 + 2*d$V220
d$error     <- d$target - d$predicted
round(d,3)
```
]
.pull-left[

.single[

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,comment=''}
round(d,3)
```
]

]

.pull-right[


$$SSR = \sum_{i=1}^{N}(Y_{(i)} - (\beta_0+\beta_1X_{(i)}))^2$$

$$SSR = \sum_{i=1}^{N}(Y_{(i)} - \hat{Y}_{(i)})^2$$

$$SSR = \sum_{i=1}^{N}(\epsilon_{(i)})^2$$

$$ SSR = 17.767$$

For the set of coefficients { $\beta_0,\beta_1$ } = {-1.5,2}, SSR is equal to 17.767.

Could you find another set of coefficients that can do a better job in terms of prediction (smaller SSR)?
]

---

**Thought Experiment**

- Suppose the potential range for my intercept, $\beta_0$, is from -10 to 10, and we will consider every single possible value from -10 to 10 with increments of .1. 

- Also, suppose the potential range for my slope, $\beta_1$, is from -5 to 5, and we will consider every single possible value from -5 to 5 with increments of .01. 

- Note that every single possible combination of $\beta_0$ and $\beta_1$ indicates a different model

- How many possible set of coefficients are there? 

- Can we try every single possible set of coefficients  and compute the SSR?

---

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=8,fig.height=6}

grid <- read.csv(here('data/grid_regression.csv'),header=TRUE)

plot_ly(grid, x = ~b0, y = ~b1, z = ~SSR, 
        marker = list(color = ~SSR,
                      showscale = TRUE,
                      cmin=min(grid$SSR)*5,
                      cmax=min(grid$SSR),cauto=F),
        width=600,height=600) %>% 
  add_markers()


```

---

**Optimization**

- In our simple demonstration, estimating the best set of coefficients is an optimization problem with the following loss function:

$$Loss = \sum_{i=1}^{N}(Y_{(i)} - (\beta_0+\beta_1X_{(i)}))^2$$

- For a standard linear regression problem, a typical loss function is the **sum of squared residuals**, and we try to pick the set of coefficients that minimize this quantity.

- Numerical approximations are used in most cases for optimization problems.

- In the case of standard linear regression, we can mathematically obtain the complete solution without any numerical approximation.

---

### Matrix Solution

We can find the best set of coefficients for most regression problems with a simple matrix operation. 

First, let's rewrite the regression problem in matrix form. 

<br>

$$Y_{(1)} = \beta_0  + \beta_1X_{(1)} + \epsilon_{(1)}$$

$$Y_{(2)} = \beta_0  + \beta_1X_{(2)} + \epsilon_{(2)}$$

$$Y_{(3)} = \beta_0  + \beta_1X_{(3)} + \epsilon_{(3)}$$
$$Y_{(4)} = \beta_0  + \beta_1X_{(4)} + \epsilon_{(4)}$$

$$Y_{(5)} = \beta_0  + \beta_1X_{(5)} + \epsilon_{(5)}$$

$$Y_{(6)} = \beta_0  + \beta_1X_{(6)} + \epsilon_{(6)}$$

$$...$$
$$...$$
$$...$$
$$Y_{(20)} = \beta_0  + \beta_1X_{(20)} + \epsilon_{(20)}$$
---

We can write all of these equations in a much simpler format as

$$ \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, $$

- $\mathbf{Y}$ is an N x 1 column vector of observed values for the outcome variable, 

- $\mathbf{X}$ is an N x (P+1) **design matrix** for the set of predictor variables, including an intercept term, 

- $\boldsymbol{\beta}$ is a (P+1) x 1 column vector of regression coefficients, 

- and  $\boldsymbol{\epsilon}$ is an N x 1 column vector of residuals. 

---

These matrix elements would look like the following for the problem above with our small dataset.

.pull-left[

![](regression_matrix.png)

]

.pull-right[

![](data_regression_matrix.gif)
]

---

It can be shown that the set of $\boldsymbol{\beta}$ coefficients that yields the minimum sum of squared residuals for this model can be analytically found using the following matrix operation.

$$\hat{\boldsymbol{\beta}} = (\mathbf{X^T}\mathbf{X})^{-1}\mathbf{X^T}\mathbf{Y}$$

Suppose we apply this matrix operation for the previous example.

.single[
```{r, echo=TRUE,eval=TRUE,message=FALSE, warning=FALSE,comment=''}

Y <-  as.matrix(readability_sub$target)
X <-  as.matrix(cbind(1,readability_sub$V220))

beta <- solve(t(X)%*%X)%*%t(X)%*%Y

beta 
```
]

The best set of  { $\beta_0,\beta_1$ } coefficients to predict the readability score with the least amount of error using Feature 220 as a predictor is { $\beta_0,\beta_1$ } = {-1.108, 2.049}

These estimates are also known as the **least square estimates**, and the best linear unbiased estimators (BLUE) for the given regression model.

---

Once we find the best estimates for the model coefficients, we can also calculate the model predicted values and residual sum of squares for the given model and dataset.

$$\boldsymbol{\hat{Y}} = \mathbf{X} \hat{\boldsymbol{\beta}} $$

$$ \boldsymbol{\hat{\epsilon}} = \boldsymbol{Y} - \hat{\boldsymbol{Y}} $$

$$ SSR = \boldsymbol{\hat{\epsilon}^T} \boldsymbol{\hat{\epsilon}} $$

.single[
```{r, echo=TRUE,eval=TRUE,message=FALSE, warning=FALSE, comment=''}

Y_hat <-  X%*%beta

E <- Y - Y_hat

SSR <- t(E)%*%E

SSR
```
]

---

- The matrix formulation is generalized to a regression model for more than one predictor. 

- When there are more predictors in the model, the dimensions of the design matrix, $\mathbf{X}$, and regression coefficient matrix, $\boldsymbol{\beta}$, will be different, but the matrix calculations will be identical. 

- Assume that we would like to expand our model by adding another predictor, Feature 166 as the second predictor. Our new model will be

$$Y_{(i)} = \beta_0  + \beta_1X_{1(i)} + \beta_2X_{2(i)} + \epsilon_{(i)}$$

- $X_1$ represents Feature 220 and $X_2$ represents Feature 166. 

- Now, we are looking for the best set of three coefficients, { $\beta_0, \beta_1, \beta_2$ } that would yield the least error in predicting the readability.

---

.single[
```{r, echo=TRUE,eval=TRUE}

Y <-  as.matrix(readability_sub$target)
X <-  as.matrix(cbind(1,readability_sub[,c('V220','V166')]))
```
]

.pull-left[

.single[
```{r, echo=TRUE,eval=TRUE,comment=''}
X
```
]
]

.pull-right[

.single[
```{r, echo=TRUE,eval=TRUE,comment=''}
Y
```
]

]

---

.single[

```{r, echo=TRUE,eval=TRUE,comment=''}

beta <- solve(t(X)%*%X)%*%t(X)%*%Y

beta 

```

<br>

```{r, echo=TRUE,eval=TRUE,comment=''}


Y_hat <-  X%*%beta

E <- Y - Y_hat

SSR <- t(E)%*%E

SSR
```
]


---

### `lm()` function

- While learning the inner mechanics of how numbers work behind the scenes is always exciting, it is handy to use already existing packages and tools to do all these computations. 

- A simple go-to function for fitting a linear regression to predict a continuous outcome is the `lm()` function.

**Model 1: Predicting readability scores from Feature 220**

.single[.tiny[
```{r, echo=TRUE,eval=TRUE,message=FALSE, comment=''}

mod <- lm(target ~ 1 + V220,data=readability_sub)

summary(mod)

```
]]

---

**Model 2: Predicting readability scores from Feature 220 and Feature 166**


.single[.tiny[
```{r, echo=TRUE,eval=TRUE,message=FALSE, comment=''}

mod <- lm(target ~ 1 + V220 + V166,data=readability_sub)

summary(mod)

```
]]

---

## Performance Evaluation

### Accuracy Metrics

- **Mean Absolute Error(MAE)**

$$ MAE = \frac{\sum_{i=1}^{N} \left | e_i \right |}{N}$$

- **Mean Squared Error(MSE)**

$$ MSE = \frac{\sum_{i=1}^{N} e_i^{2}}{N}$$

- **Root Mean Squared Error (RMSE)**

$$ RMSE = \sqrt{\frac{\sum_{i=1}^{N} e_i^{2}}{N}}$$

---

If we take predictions from the second model, we can calculate these performance metrics for the small demo dataset using the following code.

.single[.tiny2[
```{r, echo=TRUE,eval=TRUE, comment=''}

mod <- lm(target ~ 1 + V220 + V166,data=readability_sub)

readability_sub$pred <- predict(mod)

readability_sub[,c('target','pred')]

# Mean absolute error

  mean(abs(readability_sub$target - readability_sub$pred))
  
# Mean squared error

  mean((readability_sub$target - readability_sub$pred)^2)

# Root mean squared error

  sqrt(mean((readability_sub$target - readability_sub$pred)^2))
  
```
]]

---

### Proportional Reduction in Total Amount of Error (R-squared)

$SSR_{null}$ : sum of squared residuals when we use only mean to predict the outcome (intercept-only model)

$$SSR_{null} = \sum_{i=1}^{N} (y-\bar{y})^2$$

In our case, if we use the mean to predict the outcome for each observation, the sum of squared error would be equal to 17.733.

.single[
```{r, echo=TRUE,eval=TRUE,comment=''}

y_bar <- mean(readability_sub$target)

ssr_null <- sum((readability_sub$target-y_bar)^2)

ssr_null
```
]

---

Instead, if we rely on our model (Feature 220 + Feature 166) to predict the outcome, the sum of squared error would be equal to 14.495.

```{r, echo=TRUE,eval=TRUE, comment=''}

ssr_model <- sum((readability_sub$target - readability_sub$pred)^2)

ssr_model
```

The total amount of prediction error is reduced by about 18.3% when we use our model instead of a simple null model. This can be used as a performance measure for any model.

$$1-\frac{SSR_{model}}{SSR_{null}}$$

```{r, echo=TRUE,eval=TRUE,comment=''}
1 - (ssr_model/ssr_null)
```


---

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

<center>

# Bias - Variance Tradeoff for Predictive Models
---

## How many parameters does it take to draw an elephant?

.pull-left[
```{r, echo=FALSE,eval=TRUE,class.source='klippy',class.source = 'fold-show',message=FALSE, warning=FALSE,fig.width=4,fig.height=4}

knitr::include_graphics('elephant.png')

```
]
.pull-right[

- https://kourentzes.shinyapps.io/FitElephant/

- Increase the number of parameters in this model from 1 to 70, and the model predictions will start to look like an elephant. 

- Explore to find the number of parameters you would use to model an elephant. 

  - Start manipulating the **p** (number of parameters) and examine how the model predicted contour changes. 
  
  - Stop when you believe you can convince someone else that it looks like an elephant.
]

---

## Bias - Variance Tradeoff

When we use a model to predict an outcome, there are two primary sources of error: 

- **Model Error**: No model is a complete representation of truth underlying observed data, every model is misspecified. Conceptually, we can define the model error as the distance between the model and the true generating mechanism underlying data. Technically, for a given set of predictors, it is the difference between the expected value predicted by the model and the true value underlying data. The term **bias** is also commonly used for model error.

- **Sampling Error**: Given that the amount of data is fixed during any modeling process, it will decrease the stability of parameter estimates for models with increasing complexity across samples drawn from the same population. Consequently, this will increase the variance of predictions (more variability of a predicted value across different samples) for a given set of the same predictors. The terms **estimation error** or **variance** is also used for sampling error.

The essence of any modeling activity is to balance these two sources of error and find a stable model (generalizable across different samples) with the least amount of bias.

---

## A simple Monte Carlo experimentation 

Suppose that there is a true generating model underlying some observed data. This model is

$$y = e^{(x-0.3)^2} - 1 + \epsilon,$$
- $x$ is a predictor variable equally spaced and ranges from 0 to 1, 

- $\epsilon$ is a random error component that follows a normal distribution with a mean of zero and a standard deviation of 0.1, 

- and $y$ is the outcome variable. 

Suppose we simulate a small observed data following this model with a sample size of 20. Then, we use a straightforward linear model to represent the observed simulated data.

$$
y = \beta_0 + \beta_1x + \epsilon
$$

---

.single[.tiny[

```{r, echo=TRUE,eval=TRUE,message=FALSE, warning=FALSE,comment=''}

set.seed(09282021)

N = 20

x <- seq(0,1,length=20)

e <- rnorm(20,0,.1)

y <- exp((x-0.3)^2) - 1 + e


mod <- lm(y ~ 1 + x)
mod

round(predict(mod),3)
```

]]
---

.pull-left[

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=6,fig.height=6}

ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  geom_point(aes(x=x,y=y))+
  geom_line(aes(x=x,y=predict(mod)),lty=2,col='gray')+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))

```

]

.pull-right[

- The solid line represents the true nature of the relationship between $x$ and $y$. 

- The observed data points do not lie on this line due to the random error component (noise). 

- If we use a simple linear model, the gray dashed line represents the predicted relationship between $x$ and $y$. 

]

- This demonstration only represents a single dataset. Suppose that we repeat the same process ten times. 

- We will produce ten different datasets with the same size (N=20) using the same predictor values, $x$, and true data generating model. Then, we will fit a simple linear model to each of these ten datasets. 

---

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=6,fig.height=6}

set.seed(09282021)

E  <- vector('list',10)
Y  <- vector('list',10)
M1 <- vector('list',10)

N = 20

x <- seq(0,1,length=N)

for(i in 1:10){
  
  E[[i]]  <- rnorm(N,0,.1)
  Y[[i]]  <- exp((x-0.3)^2) - 1 + E[[i]]
  
  M1[[i]] <- lm(Y[[i]] ~ 1 + x)
}
```

```{r, echo=FALSE,eval=TRUE,message=FALSE,warning=FALSE,fig.width=6,fig.height=6}

p.1 <- ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))

for(i in 1:10){
  p.1 <- p.1 + geom_line(aes_string(x=x,y=predict(M1[[i]])),col='gray',lty=2)
}

p.1
```

---
<br>

What can you say about the bias and variance of model predictions?

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE}

out <- matrix(nrow=20,ncol=10)
for(i in 1:10){
  out[,i] <- predict(M1[[i]])
}

y.true <- exp((x-.3)^2)-1
Y.true <- matrix(y.true,nrow=20,ncol=30,byrow=FALSE)

yy <- as.data.frame(cbind(x,y.true,out))
yy$Average <- rowMeans(yy[,3:11])
yy$SD <- apply(yy[,3:11],1,sd)


colnames(yy) <- c('x','y (TRUE)',1:10,'Mean','SD')

round(yy,3) %>%
  kbl() %>%
  kable_minimal() %>%
  add_header_above(c(" " = 2,"Model Predicted Value Across 10 Replications" = 10," " =2)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                font_size = 10) 


yy.linear <- yy[,c('x','y (TRUE)','Mean','SD')]
colnames(yy.linear) <- c('x','y','Mean','SD')
```

---

Let's do the same experiment by fitting a more complex 6th-degree polynomial to the same datasets with the same underlying true model.

$$
y = \beta_0 + \beta_1x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6 + \epsilon
$$

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE}

set.seed(09282021)

E  <- vector('list',10)
Y  <- vector('list',10)
M6 <- vector('list',10)

N = 20

x <- seq(0,1,length=N)

for(i in 1:10){
  
  E[[i]]  <- rnorm(N,0,.1)
  Y[[i]]  <- exp((x-0.3)^2) - 1 + E[[i]]
  
  M6[[i]] <- lm(Y[[i]] ~ 1 + x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
}
```

```{r, echo=FALSE,eval=TRUE,message=FALSE,warning=FALSE,fig.width=5,fig.height=5}

p.6 <- ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))

for(i in 1:10){
  p.6 <- p.6 + geom_line(aes_string(x=x,y=predict(M6[[i]])),col='gray',lty=2)
}

p.6
```

---

How are these numbers different than the numbers when we fitted a simple linear model?

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE}

out <- matrix(nrow=20,ncol=10)
for(i in 1:10){
  out[,i] <- predict(M6[[i]])
}

y.true <- exp((x-.3)^2)-1
Y.true <- matrix(y.true,nrow=20,ncol=30,byrow=FALSE)

yy <- as.data.frame(cbind(x,y.true,out))
yy$Average <- rowMeans(yy[,3:11])
yy$SD <- apply(yy[,3:11],1,sd)


colnames(yy) <- c('x','y (TRUE)',1:10,'Mean','SD')

round(yy,3) %>%
  kbl() %>%
  kable_minimal() %>%
  add_header_above(c(" " = 2,"Model Predicted Value Across 10 Replications" = 10," " =2)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                font_size = 10) 

yy.poly <- yy[,c('x','y (TRUE)','Mean','SD')]
colnames(yy.poly) <- c('x','y','Mean','SD')
```

---

```{r, echo=FALSE,eval=TRUE,message=FALSE,warning=FALSE,fig.width=9,fig.height=6}

p1 <- ggplot(data=yy.linear,aes(x=x,y=y))+
  geom_line()+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(-0.05,1.05))+
  ylim(c(-0.25,1))+
  geom_point(aes(x=x,y=Mean))+
  geom_errorbar(aes(ymin=Mean - 2*SD, 
                    ymax=Mean + 2*SD), width=.02)+
  ggtitle('Linear Model')

p2 <- ggplot(data=yy.poly,aes(x=x,y=y))+
  geom_line()+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(-0.05,1.05))+
  ylim(c(-0.25,1))+
  geom_point(aes(x=x,y=Mean))+
  geom_errorbar(aes(ymin=Mean - 2*SD, 
                    ymax=Mean + 2*SD), width=.02)+
  ggtitle('6th Degree Polynomial Model')

grid.arrange(p1,p2,nrow=1)
```

---

We can expand our experiment and examine a range of models from linear to the 6th-degree polynomial.

$$y = \beta_0 + \beta_1x + \epsilon$$

$$y = \beta_0 + \beta_1x + \beta_2 x^2 + \epsilon$$

$$y = \beta_0 + \beta_1x + \beta_2 x^2 + \beta_3 x^3 + \epsilon$$

$$y = \beta_0 + \beta_1x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \epsilon$$

$$y = \beta_0 + \beta_1x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5  + \epsilon$$

$$y = \beta_0 + \beta_1x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 + \beta_5 x^5 + \beta_6 x^6 + \epsilon$$

---

```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE}

set.seed(09282021)

E  <- vector('list',10)
Y  <- vector('list',10)
M1 <- vector('list',10)
M2 <- vector('list',10)
M3 <- vector('list',10)
M4 <- vector('list',10)
M5 <- vector('list',10)
M6 <- vector('list',10)


N = 20

x <- seq(0,1,length=N)

for(i in 1:10){
  
  E[[i]]  <- rnorm(N,0,.1)
  Y[[i]]  <- exp((x-0.3)^2) - 1 + E[[i]]
  
  M1[[i]] <- lm(Y[[i]] ~ 1 + x)
  M2[[i]] <- lm(Y[[i]] ~ 1 + x + I(x^2))
  M3[[i]] <- lm(Y[[i]] ~ 1 + x + I(x^2) + I(x^3))
  M4[[i]] <- lm(Y[[i]] ~ 1 + x + I(x^2) + I(x^3) + I(x^4))
  M5[[i]] <- lm(Y[[i]] ~ 1 + x + I(x^2) + I(x^3) + I(x^4) + I(x^5))
  M6[[i]] <- lm(Y[[i]] ~ 1 + x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
}
```

```{r, echo=FALSE,eval=TRUE,message=FALSE,warning=FALSE,fig.width=6,fig.height=6}

N = 20
x <- seq(0,1,length=N)
y.true <- exp((x-.3)^2)-1
Y.true <- matrix(y.true,nrow=20,ncol=30,byrow=FALSE)

##################################################
out <- matrix(nrow=20,ncol=10)
for(i in 1:10){
  out[,i] <- predict(M1[[i]])
}

yy <- as.data.frame(cbind(x,y.true,out))
yy$Mean <- rowMeans(yy[,3:11])
yy$SD <- apply(yy[,3:11],1,sd)

yy.1 <- yy[,c('x','y.true','Mean','SD')]
colnames(yy.1) <- c('x','y','Mean','SD')

##################################################

out <- matrix(nrow=20,ncol=10)
for(i in 1:10){
  out[,i] <- predict(M2[[i]])
}

yy <- as.data.frame(cbind(x,y.true,out))
yy$Mean <- rowMeans(yy[,3:11])
yy$SD <- apply(yy[,3:11],1,sd)

yy.2 <- yy[,c('x','y.true','Mean','SD')]
colnames(yy.2) <- c('x','y','Mean','SD')

##################################################

out <- matrix(nrow=20,ncol=10)
for(i in 1:10){
  out[,i] <- predict(M3[[i]])
}

yy <- as.data.frame(cbind(x,y.true,out))
yy$Mean <- rowMeans(yy[,3:11])
yy$SD <- apply(yy[,3:11],1,sd)

yy.3 <- yy[,c('x','y.true','Mean','SD')]
colnames(yy.3) <- c('x','y','Mean','SD')


##################################################

out <- matrix(nrow=20,ncol=10)
for(i in 1:10){
  out[,i] <- predict(M4[[i]])
}

yy <- as.data.frame(cbind(x,y.true,out))
yy$Mean <- rowMeans(yy[,3:11])
yy$SD <- apply(yy[,3:11],1,sd)

yy.4 <- yy[,c('x','y.true','Mean','SD')]
colnames(yy.4) <- c('x','y','Mean','SD')


##################################################

out <- matrix(nrow=20,ncol=10)
for(i in 1:10){
  out[,i] <- predict(M5[[i]])
}

yy <- as.data.frame(cbind(x,y.true,out))
yy$Mean <- rowMeans(yy[,3:11])
yy$SD <- apply(yy[,3:11],1,sd)

yy.5 <- yy[,c('x','y.true','Mean','SD')]
colnames(yy.5) <- c('x','y','Mean','SD')


##################################################

out <- matrix(nrow=20,ncol=10)
for(i in 1:10){
  out[,i] <- predict(M6[[i]])
}

yy <- as.data.frame(cbind(x,y.true,out))
yy$Mean <- rowMeans(yy[,3:11])
yy$SD <- apply(yy[,3:11],1,sd)

yy.6 <- yy[,c('x','y.true','Mean','SD')]
colnames(yy.6) <- c('x','y','Mean','SD')

```


```{r, echo=FALSE,eval=TRUE,message=FALSE, warning=FALSE,fig.width=9,fig.height=6}

p1 <- ggplot(data=yy.1,aes(x=x,y=y))+
  geom_line()+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(-0.05,1.05))+
  ylim(c(-0.25,1))+
  geom_point(aes(x=x,y=Mean))+
  geom_errorbar(aes(ymin=Mean - 2*SD, 
                    ymax=Mean + 2*SD), width=.02)+
  ggtitle('Linear Model')

p2 <- ggplot(data=yy.2,aes(x=x,y=y))+
  geom_line()+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(-0.05,1.05))+
  ylim(c(-0.25,1))+
  geom_point(aes(x=x,y=Mean))+
  geom_errorbar(aes(ymin=Mean - 2*SD, 
                    ymax=Mean + 2*SD), width=.02)+
  ggtitle('Quadratic Model')

p3 <- ggplot(data=yy.3,aes(x=x,y=y))+
  geom_line()+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(-0.05,1.05))+
  ylim(c(-0.25,1))+
  geom_point(aes(x=x,y=Mean))+
  geom_errorbar(aes(ymin=Mean - 2*SD, 
                    ymax=Mean + 2*SD), width=.02)+
  ggtitle('Qubic Model')

p4 <- ggplot(data=yy.4,aes(x=x,y=y))+
  geom_line()+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(-0.05,1.05))+
  ylim(c(-0.25,1))+
  geom_point(aes(x=x,y=Mean))+
  geom_errorbar(aes(ymin=Mean - 2*SD, 
                    ymax=Mean + 2*SD), width=.02)+
  ggtitle('4th Degree Polynomial Model')

p5 <- ggplot(data=yy.5,aes(x=x,y=y))+
  geom_line()+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(-0.05,1.05))+
  ylim(c(-0.25,1))+
  geom_point(aes(x=x,y=Mean))+
  geom_errorbar(aes(ymin=Mean - 2*SD, 
                    ymax=Mean + 2*SD), width=.02)+
  ggtitle('5th Degree Polynomial Model')

p6 <- ggplot(data=yy.6,aes(x=x,y=y))+
  geom_line()+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(-0.05,1.05))+
  ylim(c(-0.25,1))+
  geom_point(aes(x=x,y=Mean))+
  geom_errorbar(aes(ymin=Mean - 2*SD, 
                    ymax=Mean + 2*SD), width=.02)+
  ggtitle('6th Degree Polynomial Model')


p.1 <- ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))+
  ggtitle('Linear Model')

for(i in 1:10){
  p.1 <- p.1 + geom_line(aes_string(x=x,y=predict(M1[[i]])),col='gray',lty=2)
}


p.2 <- ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))+
  ggtitle('Quadratic Model')

for(i in 1:10){
  p.2 <- p.2 + geom_line(aes_string(x=x,y=predict(M2[[i]])),col='gray',lty=2)
}

p.3 <- ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))+
  ggtitle('Qubic Model')

for(i in 1:10){
  p.3 <- p.3 + geom_line(aes_string(x=x,y=predict(M3[[i]])),col='gray',lty=2)
}

p.4 <- ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))+
  ggtitle('Quartic Model')

for(i in 1:10){
  p.4 <- p.4 + geom_line(aes_string(x=x,y=predict(M4[[i]])),col='gray',lty=2)
}

p.5 <- ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))+
  ggtitle('Quintic Model')

for(i in 1:10){
  p.5 <- p.5 + geom_line(aes_string(x=x,y=predict(M5[[i]])),col='gray',lty=2)
}

p.6 <- ggplot()+
  geom_function(fun = function(x) exp((x-.3)^2)-1)+
  theme_bw()+
  xlab('x')+
  ylab('y')+
  xlim(c(0,1))+
  ylim(c(-0.25,1))+
  ggtitle('Sextic Model')

for(i in 1:10){
  p.6 <- p.6 + geom_line(aes_string(x=x,y=predict(M6[[i]])),col='gray',lty=2)
}

grid.arrange(p1,p2,p3,p4,p5,p6,nrow=2,ncol=3)
```

If you had to choose one of these models for one of the simulated datasets, which one would you choose? Why?

---

Could you find that sweet spot that provides a reasonable representation of data and a reasonable amount of generalizability (consistent/stable predictions for observations other than the ones you used to develop the model)? 

```{r, echo=FALSE,eval=TRUE,fig.align='center',fig.height=6,fig.width=8}

knitr::include_graphics('bias-variance_reduced.png',dpi = 100)

```

---

## Use of Resampling Methods to Balance Model Bias and Model Variance

- Certain strategies are applied to avoid overfitting and find the sweet spot between model bias and model variance. 

- This process is nicely illustrated in [Boehmke and Greenwell (2020, Figure 2.1)](https://bradleyboehmke.github.io/HOML/process.html)

```{r, echo=FALSE,eval=TRUE,fig.align='center',fig.height=3,fig.width=8}

knitr::include_graphics('modeling_process2.png')

```

---

- **Resampling strategies**:

  - 80-20 or 70-30 splits for training vs. test data
  
  - Simple random sampling
  
  - Stratified sampling
  
  - Down-sampling and up-sampling
  
- **k-fold** cross validation when training the model:

  - the training sample is randomly partitioned into *k* sets of equal size. 
  
  - A model is fitted to *k*-1 folds, and the remaining fold is used to test the model performance. 
  
  - Repeated *k* times by treating a different fold as a hold-out set. 
  
  - The performance evaluation metric is aggregated (e.g., average) across *k* replications to get a *k*-fold cross-validation estimate of the performance evaluation metric
  
- Be mindful about data leakage for longitudinal data when creating training and test splits and partitions for the k-fold cross-validation

---

Please review the following notebook to see a demonstration of how to apply concepts discussed so far. This notebook builds a naive model to predict the readability score for a given text.

[Building a linear prediction model with and without cross-validation using the caret package](https://www.kaggle.com/code/uocoeeds/building-a-prediction-model-with-cross-validation)