{
  "articles": [
    {
      "path": "index.html",
      "title": "Applied Machine Learning for Educational Data Science",
      "description": "The fourth course in the Educational Data Science specialization at the University of Oregon\n",
      "author": [],
      "contents": "\r\nThis course focuses on applied machine learning (ML), with an\r\nemphasis on supervised This course is the fourth in a sequence of\r\ncourses on educational data science (EDS), taught using free and\r\nopen-source statistical programming languages. EDLD 654: Machine\r\nLearning for Educational Data Science aims to teach how to apply several\r\npredictive modeling approaches to educational and social science\r\ndatasets, emphasizing supervised learning methods that have emerged over\r\nthe last several decades. The primary goal of these methods is to create\r\nmodels capable of making accurate predictions, which generally implies\r\nless emphasis on statistical inference.\r\nBy the end of this course, students will be able to\r\npre-process continuous, categorical, and text data to extract\r\nmeaningful features to include in a predictive model,\r\ndescribe the framework of supervised learning methods, modeling\r\nprocess, and how it differs from standard inferential\r\nstatistics,\r\nconstruct various supervised learning models for both\r\nclassification- and regression-based problems, including linear and\r\nlogistic regression (for prediction rather than inference), penalized\r\nregression (ridge/lasso), various decision tree models (including bagged\r\ntrees and random forests), and k-nearest neighbor models,\r\ndiscuss the bias-variance tradeoff in supervised learning and\r\napply the concept in making decisions about model selection,\r\nmeasure and contrast the performance of various models.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:03:42-08:00"
    },
    {
      "path": "lecture-1.html",
      "title": "Introduction to Toy Datasets",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "06/23/2022",
      "contents": "\r\n\r\nContents\r\nReadability\r\nRecidivism\r\nInstalling\r\nReticulate, Miniconda, and Sentence Transformers\r\n\r\n\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:03:44 ]\r\nThere are two datasets we will use throughout this course. The first\r\ndataset has a continuous outcome and the second dataset has a binary\r\noutcome. We will apply several methods and algorithms to these two\r\ndatasets during the course. We will have an opportunity to compare and\r\ncontrast the prediction outcomes from several models and methods on the\r\nsame datasets.\r\nThis section provides some background information and context for\r\nthese two datasets.\r\nReadability\r\nThe readability dataset comes from a recent Kaggle\r\nCompetition (CommonLit Readability Prize). You can directly download\r\nthe training dataset from the competition website, or you can import it\r\nfrom the course website.\r\n\r\n\r\nreadability <- read.csv(here('data/readability.csv'),header=TRUE)\r\n\r\nstr(readability)\r\n\r\n'data.frame':   2834 obs. of  6 variables:\r\n $ id            : chr  \"c12129c31\" \"85aa80a4c\" \"b69ac6792\" \"dd1000b26\" ...\r\n $ url_legal     : chr  \"\" \"\" \"\" \"\" ...\r\n $ license       : chr  \"\" \"\" \"\" \"\" ...\r\n $ excerpt       : chr  \"When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an inte\"| __truncated__ \"All through dinner time, Mrs. Fayre was somewhat silent, her eyes resting on Dolly with a wistful, uncertain ex\"| __truncated__ \"As Roger had predicted, the snow departed as quickly as it came, and two days after their sleigh ride there was\"| __truncated__ \"And outside before the palace a great garden was walled round, filled full of stately fruit-trees, gray olives \"| __truncated__ ...\r\n $ target        : num  -0.34 -0.315 -0.58 -1.054 0.247 ...\r\n $ standard_error: num  0.464 0.481 0.477 0.45 0.511 ...\r\n\r\nThere is a total of 2834 observations. Each observation represents a\r\nreading passage. The most significant variables are the\r\nexcerpt and target columns. The excerpt column\r\nincludes plain text data, and the target column includes a corresponding\r\nmeasure of readability for each excerpt.\r\n\r\n\r\nreadability[1,]$excerpt\r\n\r\n[1] \"When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.\"\r\n\r\nreadability[1,]$target\r\n\r\n[1] -0.3402591\r\n\r\nAccording\r\nto the data owner, ‘the target value is the result of a\r\nBradley-Terry analysis of more than 111,000 pairwise comparisons between\r\nexcerpts. Teachers spanning grades 3-12 served as the raters for these\r\ncomparisons.’ A lower target value indicates a more challenging\r\ntext to read. The highest target score is equivalent to the 3rd-grade\r\nlevel, while the lowest target score is equivalent to the 12th-grade\r\nlevel. The purpose is to develop a model that predicts a readability\r\nscore for a given text to identify an appropriate reading level.\r\nIn the following weeks, we will talk a little bit about the\r\npre-trained language models (e.g., RoBerta). Our coverage of\r\nthis material will be at the surface level. We will primarily cover how\r\nwe obtain numerical vector representations (sentence embeddings) for\r\ngiven text input from a pre-trained language model using Python through\r\nR. Then, we will use the sentence embeddings as features to predict the\r\ntarget score in this dataset using various modeling frameworks.\r\nRecidivism\r\nThe Recidivism dataset comes from The National Institute of Justice’s\r\n(NIJ) Recidivism\r\nForecasting Challenge. The challenge aims to increase public safety\r\nand improve the fair administration of justice across the United States.\r\nThis challenge had three stages of prediction, and all three stages\r\nrequire modeling a binary outcome (recidivated vs. not recidivated in\r\nYear 1, Year 2, and Year 3). In this class, we will only work on the\r\nsecond stage and develop a model for predicting the probability of an\r\nindividual’s recidivism in the second year after initial release.\r\nYou can download the training dataset directly from the\r\ncompetition website, or from the course website. Either way, please\r\nread the Terms\r\nof Use at this link before working with this dataset.\r\n\r\n\r\nrecidivism <- read.csv(here('data/recidivism_full.csv'),header=TRUE)\r\n\r\nstr(recidivism)\r\n\r\n'data.frame':   25835 obs. of  54 variables:\r\n $ ID                                               : int  1 2 3 4 5 6 7 8 9 10 ...\r\n $ Gender                                           : chr  \"M\" \"M\" \"M\" \"M\" ...\r\n $ Race                                             : chr  \"BLACK\" \"BLACK\" \"BLACK\" \"WHITE\" ...\r\n $ Age_at_Release                                   : chr  \"43-47\" \"33-37\" \"48 or older\" \"38-42\" ...\r\n $ Residence_PUMA                                   : int  16 16 24 16 16 17 18 16 5 16 ...\r\n $ Gang_Affiliated                                  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Supervision_Risk_Score_First                     : int  3 6 7 7 4 5 2 5 7 5 ...\r\n $ Supervision_Level_First                          : chr  \"Standard\" \"Specialized\" \"High\" \"High\" ...\r\n $ Education_Level                                  : chr  \"At least some college\" \"Less than HS diploma\" \"At least some college\" \"Less than HS diploma\" ...\r\n $ Dependents                                       : chr  \"3 or more\" \"1\" \"3 or more\" \"1\" ...\r\n $ Prison_Offense                                   : chr  \"Drug\" \"Violent/Non-Sex\" \"Drug\" \"Property\" ...\r\n $ Prison_Years                                     : chr  \"More than 3 years\" \"More than 3 years\" \"1-2 years\" \"1-2 years\" ...\r\n $ Prior_Arrest_Episodes_Felony                     : chr  \"6\" \"7\" \"6\" \"8\" ...\r\n $ Prior_Arrest_Episodes_Misd                       : chr  \"6 or more\" \"6 or more\" \"6 or more\" \"6 or more\" ...\r\n $ Prior_Arrest_Episodes_Violent                    : chr  \"1\" \"3 or more\" \"3 or more\" \"0\" ...\r\n $ Prior_Arrest_Episodes_Property                   : chr  \"3\" \"0\" \"2\" \"3\" ...\r\n $ Prior_Arrest_Episodes_Drug                       : chr  \"3\" \"3\" \"2\" \"3\" ...\r\n $ Prior_Arrest_Episodes_PPViolationCharges         : chr  \"4\" \"5 or more\" \"5 or more\" \"3\" ...\r\n $ Prior_Arrest_Episodes_DVCharges                  : logi  FALSE TRUE TRUE FALSE TRUE FALSE ...\r\n $ Prior_Arrest_Episodes_GunCharges                 : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Prior_Conviction_Episodes_Felony                 : chr  \"3 or more\" \"3 or more\" \"3 or more\" \"3 or more\" ...\r\n $ Prior_Conviction_Episodes_Misd                   : chr  \"3\" \"4 or more\" \"2\" \"4 or more\" ...\r\n $ Prior_Conviction_Episodes_Viol                   : logi  FALSE TRUE TRUE FALSE TRUE FALSE ...\r\n $ Prior_Conviction_Episodes_Prop                   : chr  \"2\" \"0\" \"1\" \"3 or more\" ...\r\n $ Prior_Conviction_Episodes_Drug                   : chr  \"2 or more\" \"2 or more\" \"2 or more\" \"2 or more\" ...\r\n $ Prior_Conviction_Episodes_PPViolationCharges     : logi  FALSE TRUE FALSE FALSE FALSE FALSE ...\r\n $ Prior_Conviction_Episodes_DomesticViolenceCharges: logi  FALSE TRUE TRUE FALSE FALSE FALSE ...\r\n $ Prior_Conviction_Episodes_GunCharges             : logi  FALSE TRUE FALSE FALSE FALSE FALSE ...\r\n $ Prior_Revocations_Parole                         : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Prior_Revocations_Probation                      : logi  FALSE FALSE FALSE TRUE FALSE FALSE ...\r\n $ Condition_MH_SA                                  : logi  TRUE FALSE TRUE TRUE TRUE FALSE ...\r\n $ Condition_Cog_Ed                                 : logi  TRUE FALSE TRUE TRUE TRUE FALSE ...\r\n $ Condition_Other                                  : logi  FALSE FALSE FALSE FALSE TRUE TRUE ...\r\n $ Violations_ElectronicMonitoring                  : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Violations_Instruction                           : logi  FALSE TRUE TRUE FALSE FALSE FALSE ...\r\n $ Violations_FailToReport                          : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Violations_MoveWithoutPermission                 : logi  FALSE FALSE TRUE FALSE FALSE TRUE ...\r\n $ Delinquency_Reports                              : chr  \"0\" \"4 or more\" \"4 or more\" \"0\" ...\r\n $ Program_Attendances                              : chr  \"6\" \"0\" \"6\" \"6\" ...\r\n $ Program_UnexcusedAbsences                        : chr  \"0\" \"0\" \"0\" \"0\" ...\r\n $ Residence_Changes                                : chr  \"2\" \"2\" \"0\" \"3 or more\" ...\r\n $ Avg_Days_per_DrugTest                            : num  612 35.7 93.7 25.4 23.1 ...\r\n $ DrugTests_THC_Positive                           : num  0 0 0.333 0 0 ...\r\n $ DrugTests_Cocaine_Positive                       : num  0 0 0 0 0 0 0 0 NA 0 ...\r\n $ DrugTests_Meth_Positive                          : num  0 0 0.1667 0 0.0588 ...\r\n $ DrugTests_Other_Positive                         : num  0 0 0 0 0 0 0 0 NA 0 ...\r\n $ Percent_Days_Employed                            : num  0.489 0.425 0 1 0.204 ...\r\n $ Jobs_Per_Year                                    : num  0.448 2 0 0.719 0.929 ...\r\n $ Employment_Exempt                                : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n $ Recidivism_Within_3years                         : logi  FALSE TRUE TRUE FALSE TRUE FALSE ...\r\n $ Recidivism_Arrest_Year1                          : logi  FALSE FALSE FALSE FALSE TRUE FALSE ...\r\n $ Recidivism_Arrest_Year2                          : logi  FALSE FALSE TRUE FALSE FALSE FALSE ...\r\n $ Recidivism_Arrest_Year3                          : logi  FALSE TRUE FALSE FALSE FALSE FALSE ...\r\n $ Training_Sample                                  : int  1 1 1 1 1 0 1 0 1 1 ...\r\n\r\nThere are 25,835 observations in the training set and 54 variables,\r\nincluding a unique ID variable, four outcome variables (Recidivism in\r\nYear 1, Recidivism in Year 2, and Recidivism in Year 3, Recidivism\r\nwithin three years), and a filter variable to indicate whether an\r\nobservation was included in the training dataset or test dataset. The\r\nremaining 48 variables are potential predictive features. A complete\r\nlist of these variables can be found at this\r\nlink.\r\nWe will work on developing a model to predict the outcome variable\r\nRecidivism_Arrest_Year2 using the 48 potential predictive\r\nvariables. Before moving forward, we must remove the individuals who had\r\nalready recidivated in Year 1. As you can see below, about 29.9% of the\r\nindividuals recidivated in Year 1. I am removing these individuals from\r\nthe dataset.\r\n\r\n\r\ntable(recidivism$Recidivism_Arrest_Year1)\r\n\r\n\r\nFALSE  TRUE \r\n18111  7724 \r\n\r\nrecidivism2 <- recidivism[recidivism$Recidivism_Arrest_Year1 == FALSE,]\r\n\r\n\r\nI will also recode some variables before saving the new dataset for\r\nlater use in class.\r\nFirst, some variables in the dataset are coded as TRUE and FALSE.\r\nWhen these variables are imported into R, R automatically recognizes\r\nthem as logical variables. I will recode all these variables such that\r\nFALSE = 0 and TRUE = 1.\r\n\r\n\r\n# Find the columns recognized as logical\r\n\r\n  cols <- sapply(recidivism, is.logical)\r\n\r\n# Convert them to numeric 0s and 1s\r\n\r\n  recidivism2[,cols] <- lapply(recidivism2[,cols], as.numeric)\r\n\r\n\r\nSecond, the highest value for some variables are coded as 3\r\nor more, 4 or more, 10 or\r\nmore, etc. These variables can be considered as numeric, but R\r\nrecognizes them as character vectors due to phrase or\r\nmore for the highest value. We will recode these variables so\r\n‘X or more’ will be equal to X.\r\n\r\n\r\nrequire(dplyr)\r\n\r\n# Dependents\r\n\r\n  recidivism2$Dependents <- recode(recidivism2$Dependents,\r\n                                   '0'=0,\r\n                                   '1'=1,\r\n                                   '2'=2,\r\n                                   '3 or more'=3)\r\n\r\n# Prior Arrest Episodes Felony\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Felony <- recode(recidivism2$Prior_Arrest_Episodes_Felony,\r\n                                                     '0'=0,\r\n                                                     '1'=1,\r\n                                                     '2'=2,\r\n                                                     '3'=3,\r\n                                                     '4'=4,\r\n                                                     '5'=5,\r\n                                                     '6'=6,\r\n                                                     '7'=7,\r\n                                                     '8'=8,\r\n                                                     '9'=9,\r\n                                                     '10 or more'=10)\r\n# Prior Arrest Episods Misd\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Misd <- recode(recidivism2$Prior_Arrest_Episodes_Misd,\r\n                                                   '0'=0,\r\n                                                   '1'=1,\r\n                                                   '2'=2,\r\n                                                   '3'=3,\r\n                                                   '4'=4,\r\n                                                   '5'=5,\r\n                                                   '6 or more'=6)\r\n  \r\n# Prior Arrest Episodes Violent\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Violent <- recode(recidivism2$Prior_Arrest_Episodes_Violent,\r\n                                                      '0'=0,\r\n                                                      '1'=1,\r\n                                                      '2'=2,\r\n                                                      '3 or more'=3)\r\n\r\n# Prior Arrest Episods Property\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Property <- recode(recidivism2$Prior_Arrest_Episodes_Property,\r\n                                                       '0'=0,\r\n                                                       '1'=1,\r\n                                                       '2'=2,\r\n                                                       '3'=3,\r\n                                                       '4'=4,\r\n                                                       '5 or more'=5)\r\n  \r\n# Prior Arrest Episods Drug\r\n\r\n  recidivism2$Prior_Arrest_Episodes_Drug <- recode(recidivism2$Prior_Arrest_Episodes_Drug,\r\n                                                   '0'=0,\r\n                                                   '1'=1,\r\n                                                   '2'=2,\r\n                                                   '3'=3,\r\n                                                   '4'=4,\r\n                                                   '5 or more'=5) \r\n# Prior Arrest Episods PPViolationCharges\r\n\r\n  recidivism2$Prior_Arrest_Episodes_PPViolationCharges <- recode(recidivism2$Prior_Arrest_Episodes_PPViolationCharges,\r\n                                                                 '0'=0,\r\n                                                                 '1'=1,\r\n                                                                 '2'=2,\r\n                                                                 '3'=3,\r\n                                                                 '4'=4,\r\n                                                                 '5 or more'=5)  \r\n  \r\n# Prior Conviction Episodes Felony\r\n\r\n  recidivism2$Prior_Conviction_Episodes_Felony <- recode(recidivism2$Prior_Conviction_Episodes_Felony,\r\n                                                         '0'=0,\r\n                                                         '1'=1,\r\n                                                         '2'=2,\r\n                                                         '3 or more'=3)\r\n\r\n# Prior Conviction Episodes Misd\r\n\r\n  recidivism2$Prior_Conviction_Episodes_Misd <- recode(recidivism2$Prior_Conviction_Episodes_Misd,\r\n                                                       '0'=0,\r\n                                                       '1'=1,\r\n                                                       '2'=2,\r\n                                                       '3'=3,\r\n                                                       '4 or more'=4)\r\n  \r\n# Prior Conviction Episodes Prop\r\n\r\n  recidivism2$Prior_Conviction_Episodes_Prop <- recode(recidivism2$Prior_Conviction_Episodes_Prop,\r\n                                                       '0'=0,\r\n                                                       '1'=1,\r\n                                                       '2'=2,\r\n                                                       '3 or more'=3)\r\n\r\n# Prior Conviction Episodes Drug\r\n\r\n  recidivism2$Prior_Conviction_Episodes_Drug <- recode(recidivism2$Prior_Conviction_Episodes_Drug,\r\n                                                       '0'=0,\r\n                                                       '1'=1,\r\n                                                       '2 or more'=2)\r\n\r\n# Delinquency Reports\r\n\r\n  recidivism2$Delinquency_Reports <- recode(recidivism2$Delinquency_Reports,\r\n                                            '0'=0,\r\n                                            '1'=1,\r\n                                            '2'=2,\r\n                                            '3'=3,\r\n                                            '4 or more'=4)\r\n\r\n# Program Attendances\r\n\r\n  recidivism2$Program_Attendances <- recode(recidivism2$Program_Attendances,\r\n                                            '0'=0,\r\n                                            '1'=1,\r\n                                            '2'=2,\r\n                                            '3'=3,\r\n                                            '4'=4,\r\n                                            '5'=5,\r\n                                            '6'=6,\r\n                                            '7'=7,\r\n                                            '8'=8,\r\n                                            '9'=9,\r\n                                            '10 or more'=10)\r\n\r\n# Program Unexcused Absences\r\n\r\n  recidivism2$Program_UnexcusedAbsences <- recode(recidivism2$Program_UnexcusedAbsences,\r\n                                                  '0'=0,\r\n                                                  '1'=1,\r\n                                                  '2'=2,\r\n                                                  '3 or more'=3)\r\n\r\n# Residence Changes\r\n\r\n  recidivism2$Residence_Changes <- recode(recidivism2$Residence_Changes,\r\n                                          '0'=0,\r\n                                          '1'=1,\r\n                                          '2'=2,\r\n                                          '3 or more'=3)  \r\n#############################################################\r\n  \r\nstr(recidivism2)  \r\n\r\n'data.frame':   18111 obs. of  54 variables:\r\n $ ID                                               : int  1 2 3 4 6 7 8 11 13 15 ...\r\n $ Gender                                           : chr  \"M\" \"M\" \"M\" \"M\" ...\r\n $ Race                                             : chr  \"BLACK\" \"BLACK\" \"BLACK\" \"WHITE\" ...\r\n $ Age_at_Release                                   : chr  \"43-47\" \"33-37\" \"48 or older\" \"38-42\" ...\r\n $ Residence_PUMA                                   : int  16 16 24 16 17 18 16 5 18 5 ...\r\n $ Gang_Affiliated                                  : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Supervision_Risk_Score_First                     : int  3 6 7 7 5 2 5 3 3 7 ...\r\n $ Supervision_Level_First                          : chr  \"Standard\" \"Specialized\" \"High\" \"High\" ...\r\n $ Education_Level                                  : chr  \"At least some college\" \"Less than HS diploma\" \"At least some college\" \"Less than HS diploma\" ...\r\n $ Dependents                                       : num  3 1 3 1 0 2 3 1 1 1 ...\r\n $ Prison_Offense                                   : chr  \"Drug\" \"Violent/Non-Sex\" \"Drug\" \"Property\" ...\r\n $ Prison_Years                                     : chr  \"More than 3 years\" \"More than 3 years\" \"1-2 years\" \"1-2 years\" ...\r\n $ Prior_Arrest_Episodes_Felony                     : num  6 7 6 8 4 10 6 3 8 9 ...\r\n $ Prior_Arrest_Episodes_Misd                       : num  6 6 6 6 0 6 6 6 4 3 ...\r\n $ Prior_Arrest_Episodes_Violent                    : num  1 3 3 0 1 1 3 2 0 2 ...\r\n $ Prior_Arrest_Episodes_Property                   : num  3 0 2 3 3 5 1 1 5 2 ...\r\n $ Prior_Arrest_Episodes_Drug                       : num  3 3 2 3 0 1 2 1 2 4 ...\r\n $ Prior_Arrest_Episodes_PPViolationCharges         : num  4 5 5 3 0 5 5 3 1 4 ...\r\n $ Prior_Arrest_Episodes_DVCharges                  : num  0 1 1 0 0 0 0 1 0 0 ...\r\n $ Prior_Arrest_Episodes_GunCharges                 : num  0 0 0 0 0 1 0 0 0 1 ...\r\n $ Prior_Conviction_Episodes_Felony                 : num  3 3 3 3 1 3 1 0 1 3 ...\r\n $ Prior_Conviction_Episodes_Misd                   : num  3 4 2 4 0 1 4 3 0 2 ...\r\n $ Prior_Conviction_Episodes_Viol                   : num  0 1 1 0 0 0 1 0 0 1 ...\r\n $ Prior_Conviction_Episodes_Prop                   : num  2 0 1 3 2 3 0 0 2 1 ...\r\n $ Prior_Conviction_Episodes_Drug                   : num  2 2 2 2 0 0 2 0 1 1 ...\r\n $ Prior_Conviction_Episodes_PPViolationCharges     : num  0 1 0 0 0 1 1 1 0 1 ...\r\n $ Prior_Conviction_Episodes_DomesticViolenceCharges: num  0 1 1 0 0 0 0 0 0 0 ...\r\n $ Prior_Conviction_Episodes_GunCharges             : num  0 1 0 0 0 1 0 0 0 0 ...\r\n $ Prior_Revocations_Parole                         : num  0 0 0 0 0 0 0 1 0 0 ...\r\n $ Prior_Revocations_Probation                      : num  0 0 0 1 0 0 0 0 0 0 ...\r\n $ Condition_MH_SA                                  : num  1 0 1 1 0 0 0 1 0 1 ...\r\n $ Condition_Cog_Ed                                 : num  1 0 1 1 0 0 1 1 0 1 ...\r\n $ Condition_Other                                  : num  0 0 0 0 1 0 0 0 0 1 ...\r\n $ Violations_ElectronicMonitoring                  : num  0 0 0 0 0 0 0 1 0 0 ...\r\n $ Violations_Instruction                           : num  0 1 1 0 0 0 0 1 0 0 ...\r\n $ Violations_FailToReport                          : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Violations_MoveWithoutPermission                 : num  0 0 1 0 1 0 0 0 0 0 ...\r\n $ Delinquency_Reports                              : num  0 4 4 0 0 0 0 0 0 0 ...\r\n $ Program_Attendances                              : num  6 0 6 6 0 0 0 9 0 6 ...\r\n $ Program_UnexcusedAbsences                        : num  0 0 0 0 0 0 0 2 0 0 ...\r\n $ Residence_Changes                                : num  2 2 0 3 3 1 0 2 1 1 ...\r\n $ Avg_Days_per_DrugTest                            : num  612 35.7 93.7 25.4 474.6 ...\r\n $ DrugTests_THC_Positive                           : num  0 0 0.333 0 0 ...\r\n $ DrugTests_Cocaine_Positive                       : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ DrugTests_Meth_Positive                          : num  0 0 0.167 0 0 ...\r\n $ DrugTests_Other_Positive                         : num  0 0 0 0 0 ...\r\n $ Percent_Days_Employed                            : num  0.489 0.425 0 1 0.674 ...\r\n $ Jobs_Per_Year                                    : num  0.448 2 0 0.719 0.308 ...\r\n $ Employment_Exempt                                : num  0 0 0 0 0 0 0 1 0 1 ...\r\n $ Recidivism_Within_3years                         : num  0 1 1 0 0 1 0 1 0 0 ...\r\n $ Recidivism_Arrest_Year1                          : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Recidivism_Arrest_Year2                          : num  0 0 1 0 0 0 0 1 0 0 ...\r\n $ Recidivism_Arrest_Year3                          : num  0 1 0 0 0 1 0 0 0 0 ...\r\n $ Training_Sample                                  : int  1 1 1 1 0 1 0 1 1 0 ...\r\n\r\nNow, we export the final version of the dataset.\r\n\r\n\r\nwrite.csv(recidivism2, \r\n          here('data/recidivism_y1 removed and recoded.csv'),\r\n          row.names = FALSE)\r\n\r\n\r\nIn future weeks, we will work with this version of the dataset.\r\nInstalling\r\nReticulate, Miniconda, and Sentence Transformers\r\nYou will need to install the reticulate package and\r\nsentence_transformers module for the following weeks. You\r\ncan run the following code in your computer to get prepared for the\r\nfollowing weeks. Note that you only have to run the following code once\r\nto install the necessary packages.\r\nIf you are having troubles about installing these packages in your\r\ncomputer, I highly recommend using a Kaggle R notebook which these\r\npackages are already installed (I will give more information about this\r\nin class).\r\n\r\n\r\n# Install the reticulate package\r\n\r\ninstall.packages(pkgs = 'reticulate',\r\n                 dependencies = TRUE)\r\n\r\n\r\n\r\n# Install Miniconda\r\n\r\ninstall_miniconda()\r\n\r\n\r\nOnce you install the reticulate package, run the following code to\r\nget python configurations and make sure everything is properly\r\ninstalled.\r\n\r\n\r\n# Load the reticulate package\r\n\r\nrequire(reticulate)\r\n\r\nconda_list()\r\n\r\n          name\r\n1    Anaconda3\r\n2 r-reticulate\r\n                                                                          python\r\n1                                       C:\\\\Users\\\\cengiz\\\\Anaconda3\\\\python.exe\r\n2 C:\\\\Users\\\\cengiz\\\\AppData\\\\Local\\\\r-miniconda\\\\envs\\\\r-reticulate\\\\python.exe\r\n\r\nYou should see r-reticulate under the name column as one\r\nof your virtual Python environment. Finally, you will also need to\r\ninstall the sentence transformers module. The following code will\r\ninstall the sentence transformers module to the virtual Python\r\nenvironment r-reticulate.\r\n\r\n\r\n# Install the sentence transformer module \r\n\r\nuse_condaenv('r-reticulate')\r\n\r\nconda_install(envname  = 'r-reticulate',\r\n              packages = 'sentence_transformers',\r\n              pip      = TRUE)\r\n\r\n[1] \"sentence_transformers\"\r\n\r\n  # try pip=FALSE, if it gives an error message\r\n\r\n\r\nOnce you install the Python packages using the code above, you can\r\nrun the following code. If you are seeing the same output as below, you\r\nshould be all set to explore some very exciting NLP tools using the\r\nReadability dataset.\r\n\r\n\r\nrequire(reticulate)\r\n\r\n# Import the sentence transformer module\r\n\r\nreticulate::import('sentence_transformers')\r\n\r\nModule(sentence_transformers)\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:04:08-08:00"
    },
    {
      "path": "lecture-2a.html",
      "title": "Data Preprocessing I: Continuous and Categorical Data",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "07/06/2022",
      "contents": "\r\n\r\nContents\r\n1. Scales of\r\nMeasurement and Types of Variables\r\n2. Processing Categorical\r\nVariables\r\n2.1 One-hot encoding (Dummy\r\nVariables)\r\n2.2. Label\r\nencoding\r\n2.3.\r\nPolynomial Contrasts\r\n\r\n3. Processing Cyclic\r\nVariables\r\n4. Processing Continuous\r\nVariables\r\n4.1. Centering and\r\nScaling (Standardization)\r\n4.2. Box-Cox transformation\r\n4.3.\r\nLogit Transformation\r\n4.4. Basis\r\nExpansions\r\n\r\n5.\r\nHandling Missing Data\r\n5.1. Creating an\r\nindicator variable for missingness\r\n5.2. Imputation\r\n\r\n6. Wrapping-up using the\r\nrecipes package\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:04:09 ]\r\n1. Scales of\r\nMeasurement and Types of Variables\r\nIt is important to understand the nature of variables and how they\r\nwere measured and represented in a dataset. In social sciences, in\r\nparticular psychology, there is a methodological consensus about the\r\nframework provided by Stevens (1946), also see\r\nMichell\r\n(2002) for an in-depth discussion. According to Stevens’ definition,\r\nthere are four levels of measurement: nominal, ordinal, interval, and\r\nratio. Whether a variable has a nominal, ordinal, interval, or ratio\r\nscale depends on the character of the empirical operations performed\r\nwhile constructing the variable.\r\nNominal scale: Variables with a nominal scale cannot be\r\nmeaningfully added, subtracted, divided, or multiplied. Also, there is\r\nno hierarchical order among the assigned values. Most variables\r\ncontaining labels for individual observations can be considered nominal,\r\ne.g., hair color, city, state, and ethnicity.\r\nOrdinal scale: Variables with an ordinal scale also represent\r\nlabels; however, there is a meaningful hierarchy among the assigned\r\nvalues. For instance, if a variable is coded as Low, Medium, and High,\r\nthey are simply labels. Still, we know that High represents something\r\nmore than Medium and Medium represents something higher than Low (High\r\n> Medium > Low). On the other hand, the distance between the\r\nassigned values does not necessarily represent the same amount of\r\ndifference. Other examples of variables that can be considered as\r\nordinal are letter grades (A-F), scores from Likert type items (Strongly\r\nagree, agree, disagree, strongly disagree), education status(high\r\nschool, college, master’s, Ph.D.), cancer stage (stage1, stage2,\r\nstage3), order of finish in a competition (1st, 2nd, 3rd).\r\nInterval scale: Variables with an ordinal scale represent\r\nquantities with equal measurement units but don’t have an absolute zero\r\npoint. For instance, a typical example of an interval scale is\r\ntemperature measured on the Fahrenheit scale. The difference between 20F\r\nand 30F is the same as between 60F and 70F. However, 0F does not\r\nindicate the absence of heat.\r\nRatio scale: Variables with a ratio scale represent quantities\r\nwith equal measurement units and have an absolute zero. Due to the\r\nexistence of an absolute zero point that represents ‘nothing,’ the ratio\r\nof measurements is also meaningful. Typical examples are height, mass,\r\ndistance, and length.\r\nBelow table provides a summary of properties for each scale.\r\n\r\nIndicating Difference\r\nIndicating Direction of Difference\r\nIndicating Amount of Difference\r\nHas absolute zero\r\nNominal\r\nX\r\n\r\n\r\n\r\nOrdinal\r\nX\r\nX\r\n\r\n\r\nInterval\r\nX\r\nX\r\nX\r\n\r\nRatio\r\nX\r\nX\r\nX\r\nX\r\nIn this class, we classify the variables in two types:\r\nCategorical and Continuous. The\r\nvariables with a nominal or ordinal scale are considered as\r\nCategorical and the variables with an interval or ratio\r\nscale are considered as Continuous.\r\n2. Processing Categorical\r\nVariables\r\nWhen categorical predictors are in a dataset, it is essential to\r\ntransform them into numerical codes. When encoding categorical\r\npredictors, we try to preserve as much information as possible from\r\ntheir labels. Therefore, different strategies may be used for\r\ncategorical variables with different ordinal scales.\r\n2.1 One-hot encoding (Dummy\r\nVariables)\r\nA dummy variable is a synthetic variable with two outcomes (0 and 1)\r\nrepresenting a group membership. When there is a nominal variable with\r\nN levels, it is typical to create N dummy variables to\r\nrepresent the information in the nominal variable. Each dummy variable\r\nrepresents membership to one of the levels in the nominal variable.\r\nThese dummy variables can be used as features in predictive models.\r\nIn its simplest case, consider the variable Race in the\r\nRecidivism dataset with two levels: Black and White. We can create two\r\ndummy variables: the first dummy variable represents whether or not an\r\nindividual is Black, and the second dummy variable represents whether or\r\nnot the individual is White.\r\n\r\nDummy Variable 1\r\nDummy Variable 2\r\nBlack\r\n1\r\n0\r\nWhite\r\n0\r\n1\r\nLet’s consider another example from the Recidivism dataset. Variable\r\nPrison_Offense has five categories: Violent/Sex,\r\nViolent/Non-Sex, Property, Drug, and Other. We can create five dummy\r\nvariables using the following coding scheme.\r\n\r\nDummy Variable 1\r\nDummy Variable 2\r\nDummy Variable 3\r\nDummy Variable 4\r\nDummy Variable 5\r\nViolent/Sex\r\n1\r\n0\r\n0\r\n0\r\n0\r\nViolent/Non-Sex\r\n0\r\n1\r\n0\r\n0\r\n0\r\nProperty\r\n0\r\n0\r\n1\r\n0\r\n0\r\nDrug\r\n0\r\n0\r\n0\r\n1\r\n0\r\nOther\r\n0\r\n0\r\n0\r\n0\r\n1\r\nNote that Prison_Offence is missing for several\r\nobservations. You can fill in the missing values before creating dummy\r\nvariables using one of the methods we will discuss later. Alternatively,\r\nwe can define Missing as the sixth category to preserve that\r\ninformation.\r\n\r\nDummy Variable 1\r\nDummy Variable 2\r\nDummy Variable 3\r\nDummy Variable 4\r\nDummy Variable 5\r\nDummy Variable 6\r\nViolent/Sex\r\n1\r\n0\r\n0\r\n0\r\n0\r\n0\r\nViolent/Non-Sex\r\n0\r\n1\r\n0\r\n0\r\n0\r\n0\r\nProperty\r\n0\r\n0\r\n1\r\n0\r\n0\r\n0\r\nDrug\r\n0\r\n0\r\n0\r\n1\r\n0\r\n0\r\nOther\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\nMissing\r\n0\r\n0\r\n0\r\n0\r\n0\r\n1\r\nIn some cases, when you have a geographical location with a\r\nreasonable number of categories (e.g., counties or cities in a state,\r\nschools in a district), you can also create dummy variables to represent\r\nthis information. In our case, the Recidivism dataset has a variable\r\ncalled Residence_PUMA indicating Public\r\nUse Microdata Area (PUMA) for the residence address at the time\r\nindividual was released. This variable has 25 unique codes (1-25);\r\nhowever, these numbers are just labels. So, one can create 25 different\r\ndummy variables to represent 25 different PUMAs.\r\n\r\n\r\nNOTE\r\n\r\nWhen you fit a typical regression model without regularization using\r\nordinary least-squares (OLS), a typical practice is to drop a dummy\r\nvariable for one of the levels. So, for instance, if there are\r\nN levels for a nominal variable, you only have to create\r\n(N-1) dummy variables, as the Nth one has redundant\r\ninformation. The information regarding the excluded category is\r\nrepresented in the intercept term. It creates a problem when you put all\r\nN dummy variables into the model because the OLS procedure\r\ntries to invert a singular matrix, and you will likely get an error\r\nmessage.\r\nOn the other hand, this is not an issue when you fit a regularized\r\nregression model, which will be the case in this class. Therefore, you\r\ndo not need to drop one of the dummy variables and can include all of\r\nthem in the analysis. In fact, it may be beneficial to keep the dummy\r\nvariables for all categories in the model when regularization is used in\r\nthe regression. Otherwise, the model may produce different predictions\r\ndepending on which category is excluded.\r\n\r\n2.2. Label encoding\r\nWhen the variable of interest is ordinal, and there is a hierarchy\r\namong the levels, we can still use one-hot encoding to create a set of\r\ndummy variables to represent the information in the ordinal variable.\r\nHowever, dummy variables will not provide information regarding the\r\ncategories’ hierarchy.\r\nFor instance, consider the variable Age_At_Release in\r\nthe Recidivism dataset. It is coded as 7 different age intervals in the\r\ndataset: 18-22, 23-27, 28-32, 33-37, 38-42, 43-47, 48 or older. One can\r\ncreate seven dummy variables to represent each category in this\r\nvariable. Alternatively, one can assign a numeric variable to each\r\ncategory that may represent the information in these categories. For\r\ninstance, one can assign numbers from 1 to 7, respectively. Or, one can\r\nchoose the midpoint of each interval to represent each category (e.g.,\r\n20,25,31,35,40,45,60).\r\nAnother example would be the variable Education Level in\r\nthe Recidivism dataset. It has three levels: At least some college, High\r\nSchool Diploma, and Less than a High School diploma. One can create\r\nthree dummy variables to represent each level. Alternatively, one can\r\nassign 1, 2, and 3, respectively. Or, one can assign a number for the\r\napproximate years of schooling for each level, such as 9, 12, and\r\n15.\r\n2.3. Polynomial Contrasts\r\nAnother way of encoding an ordinal variable is to use polynomial\r\ncontrasts. The polynomial contrasts may be helpful if one wants to\r\nexplore whether or not there is a linear, quadratic, cubic, etc., the\r\nrelationship between the predictor variable and outcome variable. You\r\ncan use the stat::poly() function in R to obtain the set of\r\npolynomial contrasts. If there are N levels in an ordinal\r\nvariable, you can get polynomials up to degree N-1.\r\nFor instance, suppose you have an ordinal variable with three levels:\r\nLow, Medium, and High. Then, stat::poly(x=1:3,degree=2)\r\nwill return the polynomial contrasts for the linear and quadratic terms.\r\nNotice that the input for the poly function is a vector of\r\nnumeric values corresponding to the ordinal variable levels and the\r\ndegree of the requested polynomial terms. For this example, it creates\r\ntwo sets of vectors to represent this ordinal variable. Note that the\r\nsum of the squares within each column equals 1, and the dot product of\r\nthe contrast vectors equals 0. In other words, the polynomial terms\r\nrepresent a set of orthonormal vectors.\r\n\r\nLinear\r\nQuadratic\r\nLow\r\n-0.707\r\n0.408\r\nMedium\r\n0\r\n-0.816\r\nHigh\r\n0.707\r\n0.408\r\n\r\n\r\nctr <- poly(1:3,2)\r\nround(ctr,3)\r\n\r\n          1      2\r\n[1,] -0.707  0.408\r\n[2,]  0.000 -0.816\r\n[3,]  0.707  0.408\r\nattr(,\"coefs\")\r\nattr(,\"coefs\")$alpha\r\n[1] 2 2\r\n\r\nattr(,\"coefs\")$norm2\r\n[1] 1.0000000 3.0000000 2.0000000 0.6666667\r\n\r\nattr(,\"degree\")\r\n[1] 1 2\r\nattr(,\"class\")\r\n[1] \"poly\"   \"matrix\"\r\n\r\nsum(ctr[,1]^2)\r\n\r\n[1] 1\r\n\r\nsum(ctr[,2]^2)\r\n\r\n[1] 1\r\n\r\nsum(ctr[,1]*ctr[,2])\r\n\r\n[1] 0.00000000000000006410345\r\n\r\n\r\n\r\n\r\nIf we consider the variable Age_At_Release with 7\r\ndifferent levels, then we can have polynomial terms up to the 6th\r\ndegree.\r\n\r\n\r\nctr <- poly(1:7,6)\r\nround(ctr,3)\r\n\r\n          1      2      3      4      5      6\r\n[1,] -0.567  0.546 -0.408  0.242 -0.109  0.033\r\n[2,] -0.378  0.000  0.408 -0.564  0.436 -0.197\r\n[3,] -0.189 -0.327  0.408  0.081 -0.546  0.493\r\n[4,]  0.000 -0.436  0.000  0.483  0.000 -0.658\r\n[5,]  0.189 -0.327 -0.408  0.081  0.546  0.493\r\n[6,]  0.378  0.000 -0.408 -0.564 -0.436 -0.197\r\n[7,]  0.567  0.546  0.408  0.242  0.109  0.033\r\nattr(,\"coefs\")\r\nattr(,\"coefs\")$alpha\r\n[1] 4 4 4 4 4 4\r\n\r\nattr(,\"coefs\")$norm2\r\n[1]   1.0000   7.0000  28.0000  84.0000 216.0000 452.5714 685.7143\r\n[8] 561.0390\r\n\r\nattr(,\"degree\")\r\n[1] 1 2 3 4 5 6\r\nattr(,\"class\")\r\n[1] \"poly\"   \"matrix\"\r\n\r\n\r\n\r\n\r\n\r\n\r\nNOTE\r\n\r\nThere are other ways of encoding nominal and ordinal variables (e.g.,\r\nHelmert contrasts), or one can come up with their own set of contrast\r\nvalues. When the goal of analysis is inference and you run analysis to\r\nrespond to a specific research question, your research question\r\ntypically dictates the type of encoding to use. You choose a coding\r\nscheme that provides you the most interpretable coefficients to respond\r\nto your research question.\r\nOn the other hand, when the goal of analysis is prediction, how you\r\nencode your categorical variable does not make much difference. In fact,\r\nthey provide very similar predictions. Below, I provide an example using\r\nAge_at_Release variable to predict the outcome using different coding\r\nschemes and report the average squared error of predictions from a\r\nlogistic regression model.\r\n\r\nEncoding\r\nAverage Squared Error\r\nIntercept-Only (NULL)\r\n0.1885789\r\nDummy Variables\r\n0.1861276\r\nLabel Encoding\r\n0.1861888\r\nPolynomial Contrasts\r\n0.1861276\r\nHelmert Contrasts\r\n0.1861276\r\n\r\nNotice that one-hot encoding, polynomial contrasts, and helmert\r\ncontrasts have identical performance. In fact, they yield the exact same\r\npredicted value for observations. Moreover, a simple label encoding with\r\na single constructed variable does (almost) as well as other encoding\r\ntypes with multiple constructed variables.\r\n\r\n3. Processing Cyclic Variables\r\nThere are sometimes variables that are cyclic by nature (e.g.,\r\nmonths, days, hour), and a type of encoding that represents their cyclic\r\nnature may be the most meaningful way to represent them instead of\r\nnumerical or categorical encoding. One way to achieve this is to create\r\ntwo new variables using a sine and cosine transformation as the\r\nfollowing:\r\n\\[x_{1} = sin(\\frac{2 \\pi\r\nx}{max(x)}),\\] \\[x_{2} = cos(\\frac{2\r\n\\pi x}{max(x)}).\\]\r\nFor instance, suppose one of the variables in a dataset is the day of\r\nthe week. We can represent its cyclic nature using the two variables as\r\ndefined the following. Once the corresponding coordinates are calculated\r\nfor each day of the week, the single variable that represents days in\r\nthe data can be replaced with these two variables representing their\r\ncoordinates in a unit circle.\r\n\r\n\r\nd <- data.frame(days = c('Mon','Tue','Wed','Thu','Fri','Sat','Sun'),\r\n                x = 1:7)\r\n\r\nd$x1 <- sin((2*pi*d$x)/7)\r\nd$x2 <- cos((2*pi*d$x)/7)\r\n\r\nd\r\n\r\n  days x                        x1         x2\r\n1  Mon 1  0.7818314824680298036341  0.6234898\r\n2  Tue 2  0.9749279121818236193420 -0.2225209\r\n3  Wed 3  0.4338837391175582314240 -0.9009689\r\n4  Thu 4 -0.4338837391175580093794 -0.9009689\r\n5  Fri 5 -0.9749279121818236193420 -0.2225209\r\n6  Sat 6 -0.7818314824680299146564  0.6234898\r\n7  Sun 7 -0.0000000000000002449213  1.0000000\r\n\r\n\r\n\r\n\r\nWe can apply the same concept to any cyclic variable. Here is another\r\nexample for the time of day.\r\n\r\n\r\nd <- data.frame(hour = 1:24)\r\n\r\nd$x1 <- sin((2*pi*d$hour)/24)\r\nd$x2 <- cos((2*pi*d$hour)/24)\r\n\r\nd\r\n\r\n   hour                        x1                         x2\r\n1     1  0.2588190451025207394764  0.96592582628906831221371\r\n2     2  0.4999999999999999444888  0.86602540378443870761060\r\n3     3  0.7071067811865474617150  0.70710678118654757273731\r\n4     4  0.8660254037844385965883  0.50000000000000011102230\r\n5     5  0.9659258262890683122137  0.25881904510252073947640\r\n6     6  1.0000000000000000000000  0.00000000000000006123032\r\n7     7  0.9659258262890683122137 -0.25881904510252062845410\r\n8     8  0.8660254037844387076106 -0.49999999999999977795540\r\n9     9  0.7071067811865475727373 -0.70710678118654746171501\r\n10   10  0.4999999999999999444888 -0.86602540378443870761060\r\n11   11  0.2588190451025210170322 -0.96592582628906820119141\r\n12   12  0.0000000000000001224606 -1.00000000000000000000000\r\n13   13 -0.2588190451025207949876 -0.96592582628906831221371\r\n14   14 -0.4999999999999997224442 -0.86602540378443881863291\r\n15   15 -0.7071067811865471286481 -0.70710678118654790580422\r\n16   16 -0.8660254037844383745437 -0.50000000000000044408921\r\n17   17 -0.9659258262890683122137 -0.25881904510252062845410\r\n18   18 -1.0000000000000000000000 -0.00000000000000018369095\r\n19   19 -0.9659258262890684232360  0.25881904510252029538719\r\n20   20 -0.8660254037844385965883  0.50000000000000011102230\r\n21   21 -0.7071067811865476837596  0.70710678118654735069271\r\n22   22 -0.5000000000000004440892  0.86602540378443837454370\r\n23   23 -0.2588190451025215721437  0.96592582628906809016911\r\n24   24 -0.0000000000000002449213  1.00000000000000000000000\r\n\r\n\r\n\r\n\r\n4. Processing Continuous\r\nVariables\r\n4.1. Centering and\r\nScaling (Standardization)\r\nCentering a variable is done by subtracting the variable’s mean from\r\nevery variable’s value, ensuring that the mean of the centered variable\r\nequals zero. Scaling a variable is dividing the value of each\r\nobservation by the variable’s standard deviation. When centering and\r\nscaling are both applied, it is called standardization.\r\nWhen we standardize a variable, we ensure that its mean is equal to\r\nzero and variance is equal to 1. Standardizing outcome and predictor\r\nvariables may be critical and necessary for specific models (e.g.,\r\nK-nearest neighbor, support vector machines, penalized regression), but\r\nit is not always necessary for other models (e.g., decision tree\r\nmodels).\r\n\r\n\r\nNOTE\r\n\r\nStandardizing a variable only changes the first and second moments of\r\na distribution (mean and variance); however, it doesn’t change the third\r\nand fourth moments of a distribution (skewness and kurtosis). Notice\r\nthat the skewness and kurtosis for both the original and the\r\nstandardized variable are 3.64 and 18.24, respectively. We only change\r\nthe mean to zero and variance to one by standardizing a variable.\r\n\r\n4.2. Box-Cox transformation\r\nVariables with extreme skewness and kurtosis may deteriorate the\r\nmodel performance for certain types of models. Therefore, it may\r\nsometimes be useful to transform a variable with extreme skewness and\r\nkurtosis such that its distribution approximates to a normal\r\ndistribution. Box-Cox transformation is a method to find an optimal\r\nparameter of \\(\\lambda\\) to apply the\r\nfollowing transformation:\r\n\\[y^{(\\lambda)}=\\left\\{\\begin{matrix}\r\n\\frac{y^{\\lambda}-1}{\\lambda} & , \\lambda \\neq 0 \\\\\r\n& \\\\\r\nln(y) & , \\lambda = 0\r\n\\end{matrix}\\right.\\]\r\nBelow is an example of transforming the a right-skewed variable using\r\nthe boxcox function from the bestNormalize\r\npackage. Notice that the skewness and the kurtosis for the transformed\r\nvariable are 0 and -0.02, respectively.\r\n\r\n\r\nrequire(bestNormalize)\r\nrequire(psych)\r\n\r\n\r\nold <- rbeta(1000,1,1000)\r\n\r\nfit <- boxcox(old,standardize=FALSE)\r\nfit\r\n\r\nNon-Standardized Box Cox Transformation with 1000 nonmissing obs.:\r\n Estimated statistics:\r\n - lambda = 0.2492177 \r\n - mean (before standardization) = -3.36775 \r\n - sd (before standardization) = 0.186852 \r\n\r\nnew <- predict(fit)\r\n \r\ndescribe(old)\r\n\r\n   vars    n mean sd median trimmed mad min  max range skew kurtosis\r\nX1    1 1000    0  0      0       0   0   0 0.01  0.01 2.31     8.54\r\n   se\r\nX1  0\r\n\r\ndescribe(new)\r\n\r\n   vars    n  mean   sd median trimmed  mad   min   max range  skew\r\nX1    1 1000 -3.37 0.19  -3.37   -3.37 0.19 -3.89 -2.75  1.13 -0.03\r\n   kurtosis   se\r\nX1    -0.22 0.01\r\n\r\n\r\n\r\n\r\n\r\n\r\nNOTE\r\n\r\nBox-Cox transformation can be used only for variables with positive\r\nvalues. Therefore, it is a good idea to first implement the Box-Cox\r\ntransformation, and then standardize a variable if both procedures will\r\nbe applied to a variable. If there is a variable with negative values or\r\na mix of both positive and negative values, the Yeo-Johnson\r\ntransformation is available as an extension of the Box-Cox\r\ntransformation. See this link for more\r\ninformation. The function yeojohnson is available in the\r\nbestNormalizer package to implement the Yeo-Johnson\r\ntransformation (See, ?bestNormalizer::yeojohnson).\r\n\r\n4.3. Logit Transformation\r\nWhen a variable is a proportion bounded between 0 and 1, the logit\r\ntransformation can be applied such that\r\n\\[\\pi^{*} =\r\nln(\\frac{\\pi}{1-\\pi}),\\]\r\nwhere \\(\\pi\\) represents a\r\nproportion. This may be particularly useful when your outcome variable\r\nis a proportion bounded between 0 and 1. When a linear model is used to\r\nmodel an outcome bounded between 0 and 1, the model predictions may\r\nexceed the reasonable range of values (predictions equal to less than\r\nzero or greater than one). Logit transformation scales variables such\r\nthat the range of values becomes \\(-\\infty\\) and \\(\\infty\\) on the logit scale. One can build\r\na model to predict logit (\\(\\pi^*\\))\r\ninstead of proportion (\\(\\pi\\)) and\r\nthen ensure that the model predictions are bounded between 0 and 1 on\r\nthe original proportion scale after a simple reverse operation for\r\npredicted values.\r\n\\[\\pi = \\frac{e^{\\pi^*}}{1+e^{\\pi^*}}\r\n\\]\r\nOne caveat of using logit transformation is that it is not defined\r\nfor 0 and 1. So, when you have values in the dataset exactly equal to 0\r\nor 1, logit transformation will return either \\(-\\infty\\) and \\(\\infty\\). In these situations, we may add\r\nor substract a very tiny constant (e.g., .0001) to force the\r\ntransformation to return a numerical value.\r\n\\[\\pi^{*} = ln(\\frac{\\pi}{1-\\pi}) =\r\nln(\\frac{0}{1-0}) = -\\infty\\] \\[\\pi^{*} = ln(\\frac{\\pi}{1-\\pi}) =\r\nln(\\frac{1}{1-1}) = \\infty\\]\r\nBelow is an example of logit transformation for a randomly generated\r\nvariable.\r\n\r\n\r\nold <- rbeta(1000,1,1000)\r\n\r\nnew <- log(old/(1-old))\r\n\r\n\r\n\r\n\r\n\r\n4.4. Basis Expansions\r\nBasis expansions are useful to address nonlinearity between a\r\ncontinuous predictor variable and outcome variable. Using the basis\r\nexpansions, one can create a set of feature variables using a nonlinear\r\nfunction of a variable x, \\(\\phi(x)\\). One simply replaces the original\r\nvariable x with the new variables obtained from \\(\\phi(x)\\). For continuous predictors, the\r\nmost commonly used expansions are polynomial basis expansions. The \\(n^{th}\\) degree polynomial basis expansion\r\ncan be represented by\r\n\\[\\phi(x) = \\beta_1x + \\beta_2x^2 +\r\n\\beta_3x^3 + ... + \\beta_nx^n .\\]\r\nSuppose we have 100 observation from a random normal variable\r\nx. The third degree polynomial basis expansion (cubic basis\r\nexpansion) can be found using the poly function as the\r\nfollowing.\r\n\r\n\r\nset.seed(654)\r\n\r\nx <- rnorm(100,0,1)\r\n\r\nhead(x)\r\n\r\n[1] -0.76031762 -0.38970450  1.68962523 -0.09423560  0.09530146\r\n[6]  0.81727228\r\n\r\nhead(poly(x,degree=3))\r\n\r\n                1           2            3\r\n[1,] -0.070492258 -0.06612854  0.056003658\r\n[2,] -0.030023304 -0.07454585 -0.003988336\r\n[3,]  0.197028288  0.28324096  0.348896805\r\n[4,]  0.002240307 -0.06560960 -0.044790680\r\n[5,]  0.022936731 -0.05256865 -0.063289287\r\n[6,]  0.101772051  0.04942613 -0.034439696\r\n\r\nSo, one can use these three new variables representing a linear,\r\nquadratic, and cubic trend in our prediction model instead of the\r\noriginal variable x. See below the relationship between the\r\noriginal variable x and the new polynomial features to replace\r\nit.\r\n\r\n\r\n\r\nFor continuous predictors, there is no limit for the degree of\r\npolynomial. The higher the degree of polynomial, the more flexible the\r\nmodel becomes, and there is a higher chance of overfitting. Typically,\r\npolynomial terms up to the 3rd or 4th degree are more than enough.\r\n5. Handling Missing Data\r\nMissing data deserves a course of its own. For a comprehensive review\r\nof how to handle and impute missing data. For certain types of models\r\nsuch as gradient boosting, missing data is not a problem, and one can\r\nleave them as is without any processing. On the other hand, some models\r\nsuch as regularized regression models require complete data and one have\r\nto deal with missing data before modeling data. It is always a good idea\r\nto run some simple descriptive analysis to understand the scope of\r\nmissing values in your dataset.\r\nThe ff_glimpse() function from the finalfit\r\npackage is a useful to get a quick look at the missing values in your\r\ndataset. See an example for the recidivism dataset\r\n\r\n\r\n\r\n\r\n\r\nrequire(finalfit)\r\n\r\nff_glimpse(recidivism)$Continuous[,c('n','missing_percent')]\r\n\r\n                                                      n\r\nID                                                18111\r\nResidence_PUMA                                    18111\r\nGang_Affiliated                                   15609\r\nSupervision_Risk_Score_First                      17819\r\nDependents                                        18111\r\nPrior_Arrest_Episodes_Felony                      18111\r\nPrior_Arrest_Episodes_Misd                        18111\r\nPrior_Arrest_Episodes_Violent                     18111\r\nPrior_Arrest_Episodes_Property                    18111\r\nPrior_Arrest_Episodes_Drug                        18111\r\nPrior_Arrest_Episodes_PPViolationCharges          18111\r\nPrior_Arrest_Episodes_DVCharges                   18111\r\nPrior_Arrest_Episodes_GunCharges                  18111\r\nPrior_Conviction_Episodes_Felony                  18111\r\nPrior_Conviction_Episodes_Misd                    18111\r\nPrior_Conviction_Episodes_Viol                    18111\r\nPrior_Conviction_Episodes_Prop                    18111\r\nPrior_Conviction_Episodes_Drug                    18111\r\nPrior_Conviction_Episodes_PPViolationCharges      18111\r\nPrior_Conviction_Episodes_DomesticViolenceCharges 18111\r\nPrior_Conviction_Episodes_GunCharges              18111\r\nPrior_Revocations_Parole                          18111\r\nPrior_Revocations_Probation                       18111\r\nCondition_MH_SA                                   18111\r\nCondition_Cog_Ed                                  18111\r\nCondition_Other                                   18111\r\nViolations_ElectronicMonitoring                   18111\r\nViolations_Instruction                            18111\r\nViolations_FailToReport                           18111\r\nViolations_MoveWithoutPermission                  18111\r\nDelinquency_Reports                               18111\r\nProgram_Attendances                               18111\r\nProgram_UnexcusedAbsences                         18111\r\nResidence_Changes                                 18111\r\nAvg_Days_per_DrugTest                             14343\r\nDrugTests_THC_Positive                            15254\r\nDrugTests_Cocaine_Positive                        15254\r\nDrugTests_Meth_Positive                           15254\r\nDrugTests_Other_Positive                          15254\r\nPercent_Days_Employed                             17649\r\nJobs_Per_Year                                     17303\r\nEmployment_Exempt                                 18111\r\nRecidivism_Within_3years                          18111\r\nRecidivism_Arrest_Year1                           18111\r\nRecidivism_Arrest_Year2                           18111\r\nRecidivism_Arrest_Year3                           18111\r\nTraining_Sample                                   18111\r\n                                                  missing_percent\r\nID                                                            0.0\r\nResidence_PUMA                                                0.0\r\nGang_Affiliated                                              13.8\r\nSupervision_Risk_Score_First                                  1.6\r\nDependents                                                    0.0\r\nPrior_Arrest_Episodes_Felony                                  0.0\r\nPrior_Arrest_Episodes_Misd                                    0.0\r\nPrior_Arrest_Episodes_Violent                                 0.0\r\nPrior_Arrest_Episodes_Property                                0.0\r\nPrior_Arrest_Episodes_Drug                                    0.0\r\nPrior_Arrest_Episodes_PPViolationCharges                      0.0\r\nPrior_Arrest_Episodes_DVCharges                               0.0\r\nPrior_Arrest_Episodes_GunCharges                              0.0\r\nPrior_Conviction_Episodes_Felony                              0.0\r\nPrior_Conviction_Episodes_Misd                                0.0\r\nPrior_Conviction_Episodes_Viol                                0.0\r\nPrior_Conviction_Episodes_Prop                                0.0\r\nPrior_Conviction_Episodes_Drug                                0.0\r\nPrior_Conviction_Episodes_PPViolationCharges                  0.0\r\nPrior_Conviction_Episodes_DomesticViolenceCharges             0.0\r\nPrior_Conviction_Episodes_GunCharges                          0.0\r\nPrior_Revocations_Parole                                      0.0\r\nPrior_Revocations_Probation                                   0.0\r\nCondition_MH_SA                                               0.0\r\nCondition_Cog_Ed                                              0.0\r\nCondition_Other                                               0.0\r\nViolations_ElectronicMonitoring                               0.0\r\nViolations_Instruction                                        0.0\r\nViolations_FailToReport                                       0.0\r\nViolations_MoveWithoutPermission                              0.0\r\nDelinquency_Reports                                           0.0\r\nProgram_Attendances                                           0.0\r\nProgram_UnexcusedAbsences                                     0.0\r\nResidence_Changes                                             0.0\r\nAvg_Days_per_DrugTest                                        20.8\r\nDrugTests_THC_Positive                                       15.8\r\nDrugTests_Cocaine_Positive                                   15.8\r\nDrugTests_Meth_Positive                                      15.8\r\nDrugTests_Other_Positive                                     15.8\r\nPercent_Days_Employed                                         2.6\r\nJobs_Per_Year                                                 4.5\r\nEmployment_Exempt                                             0.0\r\nRecidivism_Within_3years                                      0.0\r\nRecidivism_Arrest_Year1                                       0.0\r\nRecidivism_Arrest_Year2                                       0.0\r\nRecidivism_Arrest_Year3                                       0.0\r\nTraining_Sample                                               0.0\r\n\r\nff_glimpse(recidivism)$Categorical[,c('n','missing_percent')]\r\n\r\n                            n missing_percent\r\nGender                  18111             0.0\r\nRace                    18111             0.0\r\nAge_at_Release          18111             0.0\r\nSupervision_Level_First 16962             6.3\r\nEducation_Level         18111             0.0\r\nPrison_Offense          15820            12.6\r\nPrison_Years            18111             0.0\r\n\r\nWe will focus a few ideas about how to handle missing data.\r\n5.1. Creating an\r\nindicator variable for missingness\r\nWe can create a binary indicator variable for every variable to\r\nindicate missingness (0: not missing, 1: missing). It doesn’t solve the\r\nmissing data problem because we may still have to impute the missing\r\nvalues for modeling; however, an indicator variable about whether or not\r\na variable is missing may sometimes provide some information in\r\npredicting the outcome. Suppose the missingness is not random, and there\r\nis a systematic relationship between outcome and whether or not values\r\nare missing for a variable. In that case, this may provide vital\r\ninformation to bring into the model. This indicator variable would be\r\nmeaningless for variables that don’t have any missing value. Therefore,\r\none can remove them from any further consideration.\r\n5.2. Imputation\r\nA common approach to missing data is to impute missing values. Each\r\npredictor becomes an outcome of interest in imputation, and then the\r\nremaining predictors are used to build an imputation model to predict\r\nthe missing values. Below is a very naive example of how it would work\r\nif we have an outcome variable (Y) and three predictors (X1, X2, X3).\r\nFirst, missing values are estimated and replaced for each predictor\r\nusing an imputation model, and then the primary outcome of interest is\r\npredicted using the imputed X1, X2, and X3.\r\nImputation Model\r\n\r\nOutcome\r\nPredictors\r\nX1\r\nX2,X3\r\nX2\r\nX1,X3\r\nX3\r\nX1,X2\r\nPrediction Model\r\n\r\nOutcome\r\nPredictors\r\nY\r\nX1, X2, X3\r\nAn imputation model can be as simple as an intercept-only model (mean\r\nimputation). For numeric variables, missing values can be replaced with\r\na simple mean, median, or mode of the observed data. For categorical\r\nvariables, missing values can be replaced with a value randomly drawn\r\nfrom a binomial or multinomial distribution with the observed\r\nprobabilities.\r\nAn imputation model can also be as complex as desired using a\r\nregularized regression model, a decision tree model, or a K-nearest\r\nneighbors model. The main idea of a more complex prediction model is to\r\nfind other observations similar to observations with a missing value in\r\nterms of other predictors and use data from these similar observations\r\nto predict the missing values. We will rely on some built-in functions\r\nin R to impute values using such complex models. For more information\r\nabout its theoretical foundations, Applied Missing Data Analysis\r\nby Craig Enders provides comprehensive coverage of this topic.\r\n6. Wrapping-up using the\r\nrecipes package\r\nWe can manually pre-process all the variables in the dataset using\r\nthe approaches discussed earlier. However, this would be a tedious job.\r\nIt may be overwhelming to apply all the procedures simultaneously to\r\ndifferent versions of the datasets (e.g., training, test, future).\r\nInstead, we can use the recipes package to implement these\r\napproaches in a more organized and efficient way.\r\nBefore using the recipes package, there are a few things\r\nto do for this dataset.\r\nRead the original data\r\nMake a list of variables with different characteristics\r\n(categorical, continuous, proportions, etc.)\r\nMake sure the type of all categorical variables is either\r\ncharacter or factor.\r\nFor variables that represent proportions, add/subtract a small\r\nnumber to 0s/1s for logit transformation\r\n\r\n\r\n# 1) Read the original data\r\n\r\n  recidivism <- read.csv(here('data/recidivism_y1 removed and recoded.csv'),header=TRUE)\r\n\r\n  str(recidivism)\r\n\r\n'data.frame':   18111 obs. of  54 variables:\r\n $ ID                                               : int  1 2 3 4 6 7 8 11 13 15 ...\r\n $ Gender                                           : chr  \"M\" \"M\" \"M\" \"M\" ...\r\n $ Race                                             : chr  \"BLACK\" \"BLACK\" \"BLACK\" \"WHITE\" ...\r\n $ Age_at_Release                                   : chr  \"43-47\" \"33-37\" \"48 or older\" \"38-42\" ...\r\n $ Residence_PUMA                                   : int  16 16 24 16 17 18 16 5 18 5 ...\r\n $ Gang_Affiliated                                  : int  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Supervision_Risk_Score_First                     : int  3 6 7 7 5 2 5 3 3 7 ...\r\n $ Supervision_Level_First                          : chr  \"Standard\" \"Specialized\" \"High\" \"High\" ...\r\n $ Education_Level                                  : chr  \"At least some college\" \"Less than HS diploma\" \"At least some college\" \"Less than HS diploma\" ...\r\n $ Dependents                                       : int  3 1 3 1 0 2 3 1 1 1 ...\r\n $ Prison_Offense                                   : chr  \"Drug\" \"Violent/Non-Sex\" \"Drug\" \"Property\" ...\r\n $ Prison_Years                                     : chr  \"More than 3 years\" \"More than 3 years\" \"1-2 years\" \"1-2 years\" ...\r\n $ Prior_Arrest_Episodes_Felony                     : int  6 7 6 8 4 10 6 3 8 9 ...\r\n $ Prior_Arrest_Episodes_Misd                       : int  6 6 6 6 0 6 6 6 4 3 ...\r\n $ Prior_Arrest_Episodes_Violent                    : int  1 3 3 0 1 1 3 2 0 2 ...\r\n $ Prior_Arrest_Episodes_Property                   : int  3 0 2 3 3 5 1 1 5 2 ...\r\n $ Prior_Arrest_Episodes_Drug                       : int  3 3 2 3 0 1 2 1 2 4 ...\r\n $ Prior_Arrest_Episodes_PPViolationCharges         : int  4 5 5 3 0 5 5 3 1 4 ...\r\n $ Prior_Arrest_Episodes_DVCharges                  : int  0 1 1 0 0 0 0 1 0 0 ...\r\n $ Prior_Arrest_Episodes_GunCharges                 : int  0 0 0 0 0 1 0 0 0 1 ...\r\n $ Prior_Conviction_Episodes_Felony                 : int  3 3 3 3 1 3 1 0 1 3 ...\r\n $ Prior_Conviction_Episodes_Misd                   : int  3 4 2 4 0 1 4 3 0 2 ...\r\n $ Prior_Conviction_Episodes_Viol                   : int  0 1 1 0 0 0 1 0 0 1 ...\r\n $ Prior_Conviction_Episodes_Prop                   : int  2 0 1 3 2 3 0 0 2 1 ...\r\n $ Prior_Conviction_Episodes_Drug                   : int  2 2 2 2 0 0 2 0 1 1 ...\r\n $ Prior_Conviction_Episodes_PPViolationCharges     : int  0 1 0 0 0 1 1 1 0 1 ...\r\n $ Prior_Conviction_Episodes_DomesticViolenceCharges: int  0 1 1 0 0 0 0 0 0 0 ...\r\n $ Prior_Conviction_Episodes_GunCharges             : int  0 1 0 0 0 1 0 0 0 0 ...\r\n $ Prior_Revocations_Parole                         : int  0 0 0 0 0 0 0 1 0 0 ...\r\n $ Prior_Revocations_Probation                      : int  0 0 0 1 0 0 0 0 0 0 ...\r\n $ Condition_MH_SA                                  : int  1 0 1 1 0 0 0 1 0 1 ...\r\n $ Condition_Cog_Ed                                 : int  1 0 1 1 0 0 1 1 0 1 ...\r\n $ Condition_Other                                  : int  0 0 0 0 1 0 0 0 0 1 ...\r\n $ Violations_ElectronicMonitoring                  : int  0 0 0 0 0 0 0 1 0 0 ...\r\n $ Violations_Instruction                           : int  0 1 1 0 0 0 0 1 0 0 ...\r\n $ Violations_FailToReport                          : int  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Violations_MoveWithoutPermission                 : int  0 0 1 0 1 0 0 0 0 0 ...\r\n $ Delinquency_Reports                              : int  0 4 4 0 0 0 0 0 0 0 ...\r\n $ Program_Attendances                              : int  6 0 6 6 0 0 0 9 0 6 ...\r\n $ Program_UnexcusedAbsences                        : int  0 0 0 0 0 0 0 2 0 0 ...\r\n $ Residence_Changes                                : int  2 2 0 3 3 1 0 2 1 1 ...\r\n $ Avg_Days_per_DrugTest                            : num  612 35.7 93.7 25.4 474.6 ...\r\n $ DrugTests_THC_Positive                           : num  0 0 0.333 0 0 ...\r\n $ DrugTests_Cocaine_Positive                       : num  0 0 0 0 0 0 0 0 0 0 ...\r\n $ DrugTests_Meth_Positive                          : num  0 0 0.167 0 0 ...\r\n $ DrugTests_Other_Positive                         : num  0 0 0 0 0 ...\r\n $ Percent_Days_Employed                            : num  0.489 0.425 0 1 0.674 ...\r\n $ Jobs_Per_Year                                    : num  0.448 2 0 0.719 0.308 ...\r\n $ Employment_Exempt                                : int  0 0 0 0 0 0 0 1 0 1 ...\r\n $ Recidivism_Within_3years                         : int  0 1 1 0 0 1 0 1 0 0 ...\r\n $ Recidivism_Arrest_Year1                          : int  0 0 0 0 0 0 0 0 0 0 ...\r\n $ Recidivism_Arrest_Year2                          : int  0 0 1 0 0 0 0 1 0 0 ...\r\n $ Recidivism_Arrest_Year3                          : int  0 1 0 0 0 1 0 0 0 0 ...\r\n $ Training_Sample                                  : int  1 1 1 1 0 1 0 1 1 0 ...\r\n\r\n# 2) List of variable types\r\n  \r\n  outcome <- c('Recidivism_Arrest_Year2')\r\n  \r\n  id      <- c('ID')\r\n  \r\n  categorical <- c('Residence_PUMA',\r\n                   'Prison_Offense',\r\n                   'Age_at_Release',\r\n                   'Supervision_Level_First',\r\n                   'Education_Level',\r\n                   'Prison_Years',\r\n                   'Gender',\r\n                   'Race',\r\n                   'Gang_Affiliated',\r\n                   'Prior_Arrest_Episodes_DVCharges',\r\n                   'Prior_Arrest_Episodes_GunCharges',\r\n                   'Prior_Conviction_Episodes_Viol',\r\n                   'Prior_Conviction_Episodes_PPViolationCharges',\r\n                   'Prior_Conviction_Episodes_DomesticViolenceCharges',\r\n                   'Prior_Conviction_Episodes_GunCharges',\r\n                   'Prior_Revocations_Parole',\r\n                   'Prior_Revocations_Probation',\r\n                   'Condition_MH_SA',\r\n                   'Condition_Cog_Ed',\r\n                   'Condition_Other',\r\n                   'Violations_ElectronicMonitoring',\r\n                   'Violations_Instruction',\r\n                   'Violations_FailToReport',\r\n                   'Violations_MoveWithoutPermission',\r\n                   'Employment_Exempt') \r\n\r\n  numeric   <- c('Supervision_Risk_Score_First',\r\n                 'Dependents',\r\n                 'Prior_Arrest_Episodes_Felony',\r\n                 'Prior_Arrest_Episodes_Misd',\r\n                 'Prior_Arrest_Episodes_Violent',\r\n                 'Prior_Arrest_Episodes_Property',\r\n                 'Prior_Arrest_Episodes_Drug',\r\n                 'Prior_Arrest_Episodes_PPViolationCharges',\r\n                 'Prior_Conviction_Episodes_Felony',\r\n                 'Prior_Conviction_Episodes_Misd',\r\n                 'Prior_Conviction_Episodes_Prop',\r\n                 'Prior_Conviction_Episodes_Drug',\r\n                 'Delinquency_Reports',\r\n                 'Program_Attendances',\r\n                 'Program_UnexcusedAbsences',\r\n                 'Residence_Changes',\r\n                 'Avg_Days_per_DrugTest',\r\n                 'Jobs_Per_Year')\r\n  \r\n  props      <- c('DrugTests_THC_Positive',\r\n                  'DrugTests_Cocaine_Positive',\r\n                  'DrugTests_Meth_Positive',\r\n                  'DrugTests_Other_Positive',\r\n                  'Percent_Days_Employed')\r\n  \r\n# 3) Convert all nominal, ordinal, and binary variables to factors\r\n  # Leave the rest as is\r\n  \r\n  for(i in categorical){\r\n    \r\n    recidivism[,i] <- as.factor(recidivism[,i])\r\n    \r\n  }\r\n  \r\n# 4) For variables that represent proportions, add/substract a small number\r\n  # to 0s/1s for logit transformation\r\n  \r\n  for(i in props){\r\n    recidivism[,i] <- ifelse(recidivism[,i]==0,.0001,recidivism[,i])\r\n    recidivism[,i] <- ifelse(recidivism[,i]==1,.9999,recidivism[,i])\r\n  }\r\n\r\n\r\nNow, we can apply certain transformations to different types of\r\nvariables. We will use the step_***() functions from the\r\nrecipes package to implement different procedures. Below is\r\na list of functions for most procedures discussed earlier in this\r\nlecture. A more detailed list of step_***() functions can\r\nbe found the in the\r\npackage manual.\r\nstep_dummy(): creates dummy variables for one-hot\r\nencoding of categorical variables\r\nstep_indicate_na() creates an indicator variable for\r\nmissingness\r\nstep_impute_bag(), step_impute_knn(),\r\nstep_impute_linear(), and step_impute_mean():\r\nimputes missing values using an imputation model\r\nstep_BoxCox(): transforms non-negative data using\r\nBox-Cox method\r\nstep_poly,step_bs(), and\r\nstep_ns() : creates basis expansions\r\nstep_logit: applies logit transformation\r\nstep_zv: removes variables with zero variance\r\nstep_normalize: standardize variables to have a mean of\r\nzero and standard deviation of one\r\nNote that the order of procedures applied to variables is important.\r\nFor instance, there would be no meaning of using\r\nstep_indicate_na() after using\r\nstep_impute_bag() (Why?). Or, there will be a problem when\r\nyou first standardize variables using ’step_normalize()` and then apply\r\na Box-Cox transformation (Why?).\r\nFor this dataset, we will implement the following steps:\r\nCreate an indicator variable of missingness for all predictors\r\n(step_indicate_na)\r\nRemove the variables with zero variance (step_zv)\r\nImpute the missing values for all predictor variables using a mean\r\nor mode (step_impute_mean and\r\nstep_impute_mode)\r\nLogit transform the variables that represent\r\nproportions(step_logit)\r\nCreate polynomial terms up to the 2nf degree term for all numeric\r\nvariables (step_poly)\r\nStandardize numeric features\r\nOne-hot encoding of all categorical variables\r\n(step_dummy)\r\n\r\n\r\nrequire(recipes)\r\n\r\nblueprint <- recipe(x  = recidivism,\r\n                    vars  = c(categorical,numeric,props,outcome,id),\r\n                    roles = c(rep('predictor',48),'outcome','ID')) %>%\r\n  \r\n  # for all 48 predictors, create an indicator variable for missingness\r\n\r\n  step_indicate_na(all_of(categorical),all_of(numeric),all_of(props)) %>%\r\n  \r\n  # Remove the variable with zero variance, this will also remove the missingness \r\n  # variables if there is no missingess\r\n\r\n  step_zv(all_numeric()) %>%\r\n  \r\n  # Impute the missing values using mean and mode. You can instead use a \r\n  # more advanced imputation model such as bagged trees. I haven't used it due\r\n  # to time concerns\r\n  \r\n  step_impute_mean(all_of(numeric),all_of(props)) %>%\r\n  step_impute_mode(all_of(categorical)) %>%\r\n  \r\n  #Logit transformation of proportions\r\n  \r\n  step_logit(all_of(props)) %>%\r\n  \r\n  # 2nd degree polynomial terms for numeric variables and proportions\r\n  \r\n  step_poly(all_of(numeric),all_of(props),degree=2) %>%\r\n  \r\n  # Standardize the polynomial terms of numeric variables and proportions\r\n  \r\n  step_normalize(paste0(numeric,'_poly_1'),\r\n                 paste0(numeric,'_poly_2'),\r\n                 paste0(props,'_poly_1'),\r\n                 paste0(props,'_poly_2')) %>%\r\n  \r\n  # One-hot encoding for all categorical variables\r\n  \r\n  step_dummy(all_of(categorical),one_hot=TRUE)\r\n\r\n\r\n\r\nblueprint\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n        ID          1\r\n   outcome          1\r\n predictor         48\r\n\r\nOperations:\r\n\r\nCreating missing data variable indicators for all_of(categorical), all_of(numeric),...\r\nZero variance filter on all_numeric()\r\nMean imputation for all_of(numeric), all_of(props)\r\nMode imputation for all_of(categorical)\r\nLogit transformation on all_of(props)\r\nOrthogonal polynomials on all_of(numeric), all_of(props)\r\nCentering and scaling for paste0(numeric, \"_poly_1\"), paste0(nu...\r\nDummy variables from all_of(categorical)\r\n\r\nOnce the recipe is ready, we can train the blueprint on training\r\ndata. When we say ‘train’, it means that weights or statistics for\r\ncertain types of operations (e.g., standardization) are calculated based\r\non training data and saved for later use. For instance, the mean and\r\nstandard deviation of variable X from training data is calculated and\r\nlater used to standardize the same variable X in testing data or future\r\ndatasets. For now, we will train the blueprint using the whole dataset.\r\nIn future lectures, we will split data into different subsets (training\r\nvs. test), and use the training dataset to apply the blueprint.\r\n\r\n\r\nprepare <- prep(blueprint, \r\n                training = recidivism)\r\nprepare\r\n\r\nRecipe\r\n\r\nInputs:\r\n\r\n      role #variables\r\n        ID          1\r\n   outcome          1\r\n predictor         48\r\n\r\nTraining data contained 18111 data points and 7977 incomplete rows. \r\n\r\nOperations:\r\n\r\nCreating missing data variable indicators for Residence_PUMA, Prison_Offense, Age_a... [trained]\r\nZero variance filter removed na_ind_Residence_PUMA, na_ind... [trained]\r\nMean imputation for Supervision_Risk_Score_First, Depende... [trained]\r\nMode imputation for Residence_PUMA, Prison_Offense, Age_a... [trained]\r\nLogit transformation on DrugTests_THC_Positive, DrugTests_... [trained]\r\nOrthogonal polynomials on Supervision_Risk_Score_First, De... [trained]\r\nCentering and scaling for Supervision_Risk_Score_First_poly_1, ... [trained]\r\nDummy variables from Residence_PUMA, Prison_Offense, Age_at_Release,... [trained]\r\n\r\nFinally, we can apply this recipe to our dataset to obtain processed\r\nvariables according to the recipe.\r\n\r\n\r\nbaked_recidivism <- bake(prepare, new_data = recidivism)\r\n\r\nbaked_recidivism\r\n\r\n# A tibble: 18,111 x 144\r\n   Recidivism_Arrest_Year2    ID na_ind_Prison_Offen~ na_ind_Supervis~\r\n                     <int> <int>                <int>            <int>\r\n 1                       0     1                    0                0\r\n 2                       0     2                    0                0\r\n 3                       1     3                    0                0\r\n 4                       0     4                    0                0\r\n 5                       0     6                    0                0\r\n 6                       0     7                    1                0\r\n 7                       0     8                    0                0\r\n 8                       1    11                    0                0\r\n 9                       0    13                    0                0\r\n10                       0    15                    0                0\r\n# ... with 18,101 more rows, and 140 more variables:\r\n#   na_ind_Gang_Affiliated <int>,\r\n#   na_ind_Supervision_Risk_Score_First <int>,\r\n#   na_ind_Avg_Days_per_DrugTest <int>, na_ind_Jobs_Per_Year <int>,\r\n#   na_ind_DrugTests_THC_Positive <int>,\r\n#   na_ind_DrugTests_Cocaine_Positive <int>,\r\n#   na_ind_DrugTests_Meth_Positive <int>, ...\r\n\r\nNotice that there are 144 variables (one outcome, one ID, and 142\r\npredictors) in the new processed dataset. In the original dataset, there\r\nwere 48 predictors. Below is a breakdown of where these 142 variables\r\ncome from.\r\nVariable Name\r\nEncoding\r\nNumber of Categories\r\nProcess\r\nNumber of Constructed Variables\r\nGender\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nRace\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nGang affiliation\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Arrest_Episodes_DVCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Arrest_Episodes_GunCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Conviction_Episodes_Viol\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Conviction_Episodes_PPViolationCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Conviction_Episodes_DomesticViolenceCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Conviction_Episodes_GunCharges\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Revocations_Parole\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nPrior_Revocations_Probation\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nCondition_MH_SA\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nCondition_Cog_Ed\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nCondition_Other\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nViolations_ElectronicMonitoring\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nViolations_Instruction\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nViolations_FailToReport\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nViolations_MoveWithoutPermission\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\nEmployment_Exempt\r\nBinary\r\n2\r\nOne-hot encoding\r\n2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nAge_at_Release\r\nOrdinal\r\n7\r\nOne-hot encoding\r\n7\r\nSupervision_Level_First\r\nOrdinal\r\n3\r\nOne-hot encoding\r\n3\r\nEducation_Level\r\nOrdinal\r\n3\r\nOne-hot encoding\r\n3\r\nPrison_Years\r\nOrdinal\r\n4\r\nOne-hot encoding\r\n4\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nResidence_PUMA\r\nNominal\r\n25\r\nOne-hot encoding\r\n25\r\nPrison_Offense\r\nNominal\r\n5\r\nOne-hot encoding\r\n5\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSupervision_Risk_Score_First\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDependents\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Felony\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Misd\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Violent\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Property\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_Drug\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Arrest_Episodes_PPViolationCharges\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Conviction_Episodes_Felony\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Conviction_Episodes_Misd\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Conviction_Episodes_Prop\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPrior_Conviction_Episodes_Drug\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDelinquency_Reports\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nProgram_Attendances\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nProgram_UnexcusedAbsences\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nResidence_Changes\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nAvg_Days_per_DrugTest\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nJobs_Per_Year\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDrugTests_THC_Positive\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDrugTests_Cocaine_Positive\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDrugTests_Meth_Positive\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nDrugTests_Other_Positive\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\nPercent_Days_Employed\r\nNumeric\r\n\r\nPolynomials, Standardization\r\n2\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMissing value indicator variables\r\nBinary\r\n\r\n\r\n11\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nTotal\r\n142\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:04:28-08:00"
    },
    {
      "path": "lecture-2b.html",
      "title": "Data Pre-processing II: Text Data",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "07/07/2022",
      "contents": "\r\n\r\nContents\r\n1.Natural Language\r\nProcessing\r\n2.\r\nText Encoding the reticulate package and\r\nsentence_transformers\r\n3.\r\nGenerating sentence embeddings for the CommonLit Readability\r\ndataset\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:04:29 ]\r\nGenerating features from text data is very different than dealing\r\nwith continuous and categorical data. First, let’s remember the dataset\r\nwe are working with.\r\n\r\n\r\nreadability <- read.csv(here('data/readability.csv'),header=TRUE)\r\n\r\n# The first two observations\r\n\r\nreadability[1:2,c('excerpt','target')]%>%\r\n    kbl(format = 'html',\r\n        escape = FALSE) %>%\r\n    kable_paper(c(\"striped\", \"hover\"),full_width = T) %>%\r\n    kable_styling(font_size = 14)\r\n\r\n\r\nexcerpt\r\n\r\n\r\ntarget\r\n\r\n\r\nWhen the young people returned to the ballroom, it presented a decidedly\r\nchanged appearance. Instead of an interior scene, it was a winter\r\nlandscape. The floor was covered with snow-white canvas, not laid on\r\nsmoothly, but rumpled over bumps and hillocks, like a real snow field.\r\nThe numerous palms and evergreens that had decorated the room, were\r\npowdered with flour and strewn with tufts of cotton, like snow. Also\r\ndiamond dust had been lightly sprinkled on them, and glittering crystal\r\nicicles hung from the branches. At each end of the room, on the wall,\r\nhung a beautiful bear-skin rug. These rugs were for prizes, one for the\r\ngirls and one for the boys. And this was the game. The girls were\r\ngathered at one end of the room and the boys at the other, and one end\r\nwas called the North Pole, and the other the South Pole. Each player was\r\ngiven a small flag which they were to plant on reaching the Pole. This\r\nwould have been an easy matter, but each traveller was obliged to wear\r\nsnowshoes.\r\n\r\n\r\n-0.3402591\r\n\r\n\r\nAll through dinner time, Mrs. Fayre was somewhat silent, her eyes\r\nresting on Dolly with a wistful, uncertain expression. She wanted to\r\ngive the child the pleasure she craved, but she had hard work to bring\r\nherself to the point of overcoming her own objections. At last, however,\r\nwhen the meal was nearly over, she smiled at her little daughter, and\r\nsaid, “All right, Dolly, you may go.” “Oh, mother!” Dolly cried,\r\noverwhelmed with sudden delight. “Really? Oh, I am so glad! Are you sure\r\nyou’re willing?” “I’ve persuaded myself to be willing, against my will,”\r\nreturned Mrs. Fayre, whimsically. “I confess I just hate to have you go,\r\nbut I can’t bear to deprive you of the pleasure trip. And, as you say,\r\nit would also keep Dotty at home, and so, altogether, I think I shall\r\nhave to give in.” “Oh, you angel mother! You blessed lady! How good you\r\nare!” And Dolly flew around the table and gave her mother a hug that\r\nnearly suffocated her.\r\n\r\n\r\n-0.3153723\r\n\r\n\r\nThe excerpt column includes plain text data, and the target column\r\nincludes a corresponding measure of readability for each excerpt. A\r\nhigher target value indicates a more difficult text to read. What\r\nfeatures can we generate from the plain text to predict its\r\nreadability?\r\nIn the following sections, we will briefly touch on the Transformer\r\nmodels and demonstrate how to derive numerical features to represent a\r\ntext.\r\n1.Natural Language Processing\r\nNLP is a field at the intersection of linguistics and computer\r\nscience with the ultimate goal of developing algorithms and models to\r\nunderstand and use human language in a way we understand and use it. The\r\ngoal is not only to understand individual words but also the context in\r\nwhich these words are being used.\r\nThe recent advancements in the field of NLP revolutionized the\r\nlanguage models. These advanced language models take plain text, put it\r\ninto smaller pieces (tokens), then use very complex neural network\r\nmodels to convert these tokens into a numerical vector representing the\r\ntext.\r\nMost recently, a group of scholars called these models as part of Foundation Models. Below\r\nshows a development timeline of the most popular language models.\r\n\r\n\r\n\r\nBelow is a brief list of some of these NLP models and some\r\ninformation, including links to original papers. These models are very\r\nexpensive to train and use the enormous amounts of data available. For\r\ninstance, Bert/Roberta was trained using the entire Wikipedia and a Book\r\nCorpus (a total of ~ 4.7 billion words), GPT-2 was trained using 8\r\nmillion web pages, and GPT3 was trained on 45 TB of data from the\r\ninternet and books.\r\nModel\r\nDeveloper\r\nYear\r\n# of parameters\r\nEstimated Cost\r\nBert-Large\r\nGoogle AI\r\n2018\r\n336 M\r\n$\r\n7K\r\nRoberta-Large\r\nFacebook AI\r\n2019\r\n335 M\r\n?\r\nGPT2-XL\r\nOpen AI\r\n2019\r\n1.5 B\r\n$\r\n50K\r\nT5\r\nGoogle AI\r\n2020\r\n11 B\r\n$ 1.3 M\r\nGPT3\r\nOpenAI\r\n2020\r\n175 B\r\n$\r\n4.6 M\r\nAll these models except GPT3 are open source. They can be immediately\r\nutilized using open libraries (typically Python), and these models can\r\nbe customized to implement specific tasks (e.g., question answering,\r\nsentiment analysis, translation, etc.). GPT3 is the most powerful\r\ndeveloped so far, and it can only be accessed through a private API, https://beta.openai.com/.\r\nYou can explore some GPT3 applications on this website, https://gpt3demo.com/. Below are a few\r\nof them:\r\nArtificial tweets (https://thoughts.sushant-kumar.com/word)\r\nCreative writing (https://www.gwern.net/GPT-3)\r\nInterview with (artificial) Einstein (https://maraoz.com/2021/03/14/einstein-gpt3/)\r\nIf you have time, this series of\r\nYoutube videos provide some background and accessible information\r\nabout these models. In particular, Episode 2 will give a good idea about\r\nwhat these numerical embeddings represent. If you want to get in-depth\r\ncoverage of speech and language processing from scratch, this freely available\r\nbook provides a good amount of material. Finally, this free course by\r\nHugging Face provides a nice an easy introduction to transformer\r\nmodels. While the course primarily utilizes Python, it is still useful\r\nto read through to get familiar with concepts.\r\nIn this class, we will only scratch the surface and focus on the\r\ntools available to make these models accessible through R. In\r\nparticular, we will use reticulate package and\r\nsentence_transformers module (a Python package) to connect\r\nwith HuggingFace’s\r\nTransformers library and explore the word and sentence embeddings\r\nderived from the NLP models.\r\nYou can consider Hugging Face like a CRAN of pre-trained AI/ML\r\nmodels, it includes a wide variety of language models. There are\r\nthousands of pre-trained models that can be imported and used within\r\nseconds at no charge to achieve tasks like text generation, text\r\nclassification, translation, speech recognition, image classification,\r\nobject detection, etc.\r\n\r\nHugging Face Model\r\nRepository\r\n\r\n\r\n\r\n\r\n\r\n2.\r\nText Encoding the reticulate package and\r\nsentence_transformers\r\nFirst, we will load the reticulate package and\r\nsentence_transformers module. While we import the\r\nsentence_transformers module we will assign a name to save\r\nas an object so we can utilize it moving forward.\r\n\r\n\r\nrequire(reticulate)\r\n\r\nst <- import('sentence_transformers')\r\n\r\n\r\nNext, we will pick a pre-trained language model to play with. It can\r\nbe any model listed in the previous section. You have to find the\r\nassociate Huggingface page for that particular model and use the same\r\ntag used for that model. For instance, suppose we want to use the\r\nRoBERTa model. The associated Huggingface webpage for this model is https://huggingface.co/roberta-base.\r\nNotice that the tag for this model is ‘roberta-base’. Therefore, we\r\nwill use the same name to load this model. For other models, you can use\r\nthe search bar at the top. When you run the code in the next chunk the\r\nfirst time, it will install all the relevant model files (https://huggingface.co/roberta-base/tree/main) to a\r\nlocal folder on your machine. The next time you run the same code, it\r\nwill quickly load the model from the local folder since it is already\r\ninstalled on your computer.\r\n\r\n\r\nmodel.name <- 'roberta-base'\r\n\r\nroberta       <- st$models$Transformer(model.name)\r\npooling_model <- st$models$Pooling(roberta$get_word_embedding_dimension())\r\nmodel         <- st$SentenceTransformer(modules = list(roberta,pooling_model))\r\n\r\n\r\nOne important thing to be aware of about these models is the maximum\r\nnumber of characters they can process. It can be found by using the\r\nfollowing code. For instance, RoBERTa can handle a text sequence with a\r\nmaximum number of 512 characters. If we submit any text with more than\r\n512 characters, it will only process the first 512 characters.\r\n\r\n\r\nmodel$get_max_seq_length()\r\n\r\n[1] 512\r\n\r\nAnother essential characteristic is the length of the output vector\r\nwhen a language model returns numerical embeddings. The following code\r\nreveals that RoBERTa returns a vector with a length of 768.\r\n\r\n\r\nmodel$get_sentence_embedding_dimension()\r\n\r\n[1] 768\r\n\r\nIn short, RoBERTa can take any text sequence up to 512 characters as\r\ninput and then return a numerical vector with a length of 768 that\r\nrepresent this text sequence. This process is also called\r\nencoding.\r\nFor instance, we can get the embeddings for a single word ‘sofa’. The\r\nfollowing will return a vector with a length of 768.\r\n\r\n\r\nmodel$encode('sofa')\r\n\r\n  [1] -0.0356739610  0.0091452915  0.0451551154 -0.0228319988\r\n  [5]  0.4442713559 -0.2135011852  0.0167765897 -0.0311041381\r\n  [9]  0.0107637532 -0.1096956134 -0.2183851302 -0.1140032262\r\n [13]  0.1238908470 -0.0797361359  0.1692416519  0.0336829498\r\n [17] -0.0600813031  0.0623767339  0.0907270387 -0.0224188231\r\n [21] -0.0487387851  0.1461644173 -0.0409476534  0.0489805304\r\n [25] -0.0786152929 -0.0014594514  0.1224116161  0.0160395801\r\n [29] -0.0279184822 -0.0633803979 -0.2180166543 -0.1305485517\r\n [33]  0.0690473542 -0.0019865464  0.0430623516  0.0606660955\r\n [37]  0.0769219324  0.0823243707 -0.0091244802  0.0740725845\r\n [41] -0.0998382196  0.0193559173 -0.1617818475  0.0065892581\r\n [45] -0.0066350996 -0.0094996877  0.1424297392 -0.1620815396\r\n [49]  0.0353107899 -0.0427614897  0.0912168026 -0.0695206448\r\n [53] -0.0678907335  0.0852721781 -0.0525358729 -0.1284755319\r\n [57]  0.0785271227 -0.0650885701 -0.0774637461  0.0362805724\r\n [61] -0.0768732131  0.5039720535 -0.0417392813  0.0190712139\r\n [65] -0.0342553258  0.0591300614 -0.0683486015  0.2984635830\r\n [69]  0.1031861901 -0.0457865521  0.0050541554 -0.0820528343\r\n [73]  0.0670195296  0.0963044539 -0.0055563236 -0.0143450852\r\n [77]  0.0891769677 -2.4715216160 -0.1521035582  0.0507060960\r\n [81]  0.0713680387 -0.0759579390  0.6373476386  0.1234835237\r\n [85]  0.0974773467  0.0023111589  0.0176451337  0.2336518466\r\n [89] -0.0200673547  0.0513754599  0.0578624979  0.0382901579\r\n [93]  0.0388909206  0.0787570849  0.0269535370  0.0421335399\r\n [97] -0.0185404010  0.3166861236 -0.0640645251  0.0246435348\r\n[101] -0.0877356082 -0.0269688834  0.0998412147  0.0831300467\r\n[105] -0.1081224978  0.1028029099 -0.1278326511 -0.0900091380\r\n[109] -0.1370072365 -0.0222846027 -0.1231006011  0.2747447789\r\n[113]  0.1733784825 -0.0460299775 -0.0388567075  0.0479775555\r\n[117] -0.0237231664  0.0516280644  0.0417753085  0.1405762136\r\n[121]  0.0398786888  0.1218406260  0.1547902524 -0.0799253136\r\n[125] -0.2430792302  0.0188864060  0.1381260455 -0.0364184044\r\n[129] -0.0737973154  0.0934374630 -0.0591127388 -0.1949139088\r\n[133] -0.0084962826  0.1367645711  0.0814273059 -0.0439461172\r\n[137] -0.0625772253  0.0191264357  0.1192571372 -0.0691679865\r\n[141]  0.0121560153  0.0242042560  0.1154776216 -0.0031788489\r\n[145]  0.1741372645 -0.0381856300  0.0096333232 -0.0517109446\r\n[149] -0.2065927684  0.2168765217  0.1562301964 -0.1177057773\r\n[153]  0.0559231862 -0.0408602990 -0.0578732491  0.1034204736\r\n[157]  0.1974046826  0.0587377064 -0.0360219218 -0.0513394848\r\n[161]  0.0466131195 -0.1661968976  0.0014134059  0.0006979322\r\n[165] -0.0763303414  0.0016987236  0.0691462085  0.0510523655\r\n[169] -0.0452497005  0.0557559207 -0.0133551341  0.0120415296\r\n[173] -0.0284588616  0.0128517095  0.0009869952 -0.0887095928\r\n[177]  0.0629096702 -0.0487924740 -0.0417405814 -0.0330066457\r\n[181] -0.0286348350 -0.0158590674 -0.1413763165 -0.0421915129\r\n[185]  0.0825778693  0.0744538307 -0.0357263014 -0.1307719201\r\n[189] -0.1297859699  0.0350345746  0.1615838110  0.0179401282\r\n[193] -0.0293479897 -0.0170247853  0.0378078744  0.0269139335\r\n[197] -0.1672320217 -0.0403192937 -0.0556905754 -0.0660827383\r\n[201] -0.0173227638 -0.0607024953  0.1821020246  0.0161704849\r\n[205]  0.2305682898 -0.0419338197 -0.1288991123  0.0448455289\r\n[209]  0.0064716600 -0.0127650741 -0.0925187841  0.1807894111\r\n[213]  0.0710934252  0.1928009093 -0.1210669428 -0.1314722449\r\n[217] -0.0510648154 -0.4452689588  0.1560137719 -0.2141953409\r\n[221]  0.0473200753  0.0048588449 -0.1239974946 -0.1737168282\r\n[225] -0.1687250882 -0.1179471463  0.0520940572  0.0320896581\r\n[229]  0.0264194682 -0.0294446461  0.1118273884  0.1000626013\r\n[233]  0.2142880708 -0.0209907908  0.1219721958 -0.0776944458\r\n[237] -0.0059377681  0.0226883367 -0.0174616352  0.0608575717\r\n[241]  0.0128563922 -0.0122666284  0.0205464363  0.1535492539\r\n[245] -0.0558923967  0.0173551757 -0.0106957164  0.3792691827\r\n[249] -0.0184841342 -0.0758697316 -0.0823302120 -0.0419104174\r\n[253]  0.0146229882 -0.0516287647 -0.0121405581  0.0141355302\r\n[257] -0.0988652408  0.0112233525  0.0774115101  0.1352834553\r\n[261]  0.0283605196 -0.0186086819 -0.0428918973 -0.1596861631\r\n[265] -0.0418169945 -0.0641659349  0.0253907926  0.0114793517\r\n[269] -0.0815594047 -0.0027431045  0.1337629259 -0.0245217867\r\n[273]  0.0282561723 -0.0723891556  0.0336038843 -0.0491962805\r\n[277]  0.2089185268 -0.0338551402 -0.0591803417 -0.0615116805\r\n[281]  0.0708776638  0.0354636088 -0.1486814171  0.0963897035\r\n[285]  0.0737084821  0.0070018056  0.2741011083 -0.0709664822\r\n[289] -0.0759047121  0.0596543550  0.0633541495  0.1221976727\r\n[293]  0.0663689300 -0.0255231652 -0.0601295643 -0.0410300568\r\n[297]  0.0213434175 -0.0070217536  0.0664876178  0.0375904702\r\n[301]  0.0977534801 -0.0357051492 -0.0466797203  0.0041690161\r\n[305]  0.1315060258 -0.0527024977 -0.0580320247  0.0049279444\r\n[309] -0.0824159011  0.0807028711  0.0243882202  0.0074504651\r\n[313] -0.0792178810  0.0894088000  0.0450709686  0.0140973330\r\n[317] -0.1279232353  0.0452108681  0.0370465592  0.0794232488\r\n[321]  0.0484420918 -0.0275568068  0.0909292921  0.1273204237\r\n[325] -0.0044540651  0.0747541636  0.0438659079  0.0720247105\r\n[329] -0.0906757563  0.1686510891 -0.0826850086  0.4302483201\r\n[333]  0.1092576087  0.0815252513  0.1608462334  0.2359179258\r\n[337]  0.0666487738 -0.0231011249  0.1278099120  0.1643448323\r\n[341] -0.0311187897 -0.0091495812 -0.0372512080  0.0233513340\r\n[345]  0.1186399907  0.0678060576  0.0010681981 -0.0119118299\r\n[349]  0.0046485681  0.0117309950 -0.0047438201  0.0092348186\r\n[353] -0.0281289481 -0.1587170810 -0.0635980815  0.2193659097\r\n[357]  0.0223285761  0.1418938637 -0.0243899096  0.1697532982\r\n[361]  0.0401152596  0.0707843602  0.0132805128 -0.0120259793\r\n[365]  0.1322512329 -0.0730908960 -0.1636713296 -0.0757417679\r\n[369]  0.0443388149  0.1403741688  0.0351632163  0.0272783544\r\n[373] -0.0755953714  0.0670917109  0.0744388178 -0.0585914142\r\n[377]  0.0665109158  0.0554741733  0.1915322989  0.0042239279\r\n[381]  0.0241412483  0.0870938003  0.0407261923  0.0380898938\r\n[385]  0.0072629489  0.2295169532  0.0345601663  0.1345981061\r\n[389]  0.0193038937 -0.0737601519  0.0488900095 -0.0320191570\r\n[393]  0.0044139400 -0.1239226386  0.1697745770 -0.0467690229\r\n[397]  0.1255642921  0.0495714769 -0.1245959699 -0.0278481580\r\n[401] -0.0222749729  0.0756490007 -0.0419671908  0.1577912569\r\n[405]  0.1263415068  0.0471821725 -0.0791757703 -0.0848639980\r\n[409]  0.0489037707 -0.0662666783  0.0232489593  0.1550576985\r\n[413]  0.0583984405  0.0501303114 -0.1187660545 -0.1089226231\r\n[417]  0.0051808371  0.1294556260 -0.0263066702  0.0613578483\r\n[421] -0.1572146863  0.1188179851 -0.0304358322 -0.0012861080\r\n[425]  0.0322567225 -0.0624186099  0.1952621043 -0.1082454026\r\n[429]  0.0215926021  0.0031448025  0.0534361675 -0.0518163294\r\n[433] -0.0187338796  0.0358905196  0.1152749881 -0.0206493009\r\n[437] -0.0183112808 -0.0602562875 -0.0933917016  0.0276018120\r\n[441] -0.0309239589  0.0220303740 -0.0335549340  0.1210290939\r\n[445] -0.0852393284 -0.1197768748  0.0796097219 -0.0185298547\r\n[449] -0.0533546135  0.0326319411  0.0140433479  0.0901376978\r\n[453] -0.0106399208 -1.2948679924  0.0549269058  0.2028155327\r\n[457] -0.0292813182 -0.0242747366 -0.2009219229  0.0069828127\r\n[461] -0.0131280888  0.0950084254 -0.0167004801 -0.1093979999\r\n[465]  0.0556762479 -0.0704313815  0.1441860497  0.0586875789\r\n[469]  0.0131810829 -0.0256288871  0.0765602812 -0.0727991164\r\n[473]  0.0087057045 -0.0679870248 -0.0487150811 -0.0322923847\r\n[477]  0.0067269527  0.2760896981  0.1456675082 -0.1109817624\r\n[481]  0.0296200737  0.0961446464  0.1258685142  0.0100035518\r\n[485] -0.0643155798 -0.0085830279 -0.0160746835  0.1137120202\r\n[489]  0.0430779532 -0.1941830814  0.0519818999 -0.1093629897\r\n[493] -0.0540313534  0.0003336649  0.2005086243  0.0755024254\r\n[497]  0.0308930520 -0.1000531092  0.0408554040 -0.0952554420\r\n[501]  0.0385358930  0.0117233684  0.0151376864  0.0827608109\r\n[505] -0.0540767983 -0.0203211699 -0.0348391049 -0.0059033292\r\n[509]  0.0453390293  0.1632901579  0.1114752591 -0.0287137758\r\n[513]  0.0790089965  0.0042176861 -0.0153378677 -0.1155088916\r\n[517]  0.2205435038 -0.0161970071 -0.2082663924 -0.2145530879\r\n[521]  0.0687874034 -0.0167665780  0.0280568413 -0.1220358312\r\n[525] -0.2078153938 -0.0163571201 -0.0281453505  0.0477408022\r\n[529]  0.1477197260 -0.0380651988 -0.0686690956 -0.1238215193\r\n[533]  0.0485135168 -0.0375833437 -0.0017912239  0.0233479757\r\n[537]  0.1342128366  0.0435455702 -0.2266585380 -0.0229836479\r\n[541] -0.1091297865  0.0042014066  0.0208422765 -0.0898816288\r\n[545]  0.0692565739  0.0958191231 -0.0390331931  0.0037433216\r\n[549] -0.0514277034 -0.0427649952  0.0874260515 -0.2977560759\r\n[553]  0.0617639646 -0.0038292734  0.1614622623  0.0619579367\r\n[557]  0.0771042630 -0.1935174614 -0.0958970934  0.2438838184\r\n[561] -0.0766469017  0.2058261037  0.0610172190 -0.0550224334\r\n[565] -0.0674731657  0.1120143756  0.1977238506  0.0133913057\r\n[569] -0.0182942767  0.0311765596 -0.0793673620 -0.0004061926\r\n[573] -0.1822259128  0.0842015147 -0.0151702296  0.0315816365\r\n[577]  0.0143520888  0.0462132841  0.1367394030 -0.1292865872\r\n[581] -0.1191602945  0.0113026714  0.0500270873  0.0091564078\r\n[585]  0.0946571678  0.1466214955 -0.0994028375  0.0686461106\r\n[589] 10.4880447388 -0.0123514729 -0.0012773862  0.0908007324\r\n[593] -0.0558971688 -0.1602755934 -0.0279271528 -0.0967928693\r\n[597]  0.1088774577 -0.0242510661 -0.0238485262  0.0940334499\r\n[601] -0.0809688866  0.0916284621  0.0230508875  0.0944304019\r\n[605]  0.1266701519  0.0251529813  0.1540278941  0.0799024925\r\n[609] -0.0092427004 -0.1325678527 -0.0278560482  0.0196429603\r\n[613] -0.0994739532 -0.0079187285  0.0675237849  0.0466950536\r\n[617]  0.0014124722  0.2096044570  0.0467524007  0.1765374392\r\n[621] -0.0125307851 -0.0876224935 -0.0425623730 -0.0767806694\r\n[625] -0.2339251786  0.0408395901 -0.0243873447  0.1549460888\r\n[629]  0.0910378993  0.0292610079  0.0766360164 -0.0269176289\r\n[633]  0.0165668949  0.0711876675 -0.0904418379  0.0709702373\r\n[637] -0.1378541440 -0.0377802700 -0.0319465138 -0.1898437291\r\n[641]  0.0096299518  0.1230707169 -0.1166381687  0.0054549370\r\n[645]  0.2066617906 -0.0204732902  0.0302161332 -0.0745335370\r\n[649]  0.0918245688  0.0983860195 -0.0038088528  0.1738650054\r\n[653]  0.1944436133 -0.0272036064  0.1342483014 -0.0183334164\r\n[657]  0.0908793360  0.0922514722 -0.0001281835  0.0130358934\r\n[661] -0.0195043273  0.0827303976 -0.0225233156 -0.1205918044\r\n[665] -0.3059235811 -0.0033967942  0.0108995540 -0.1303331256\r\n[669]  0.0572650544  0.1016918644 -0.0377703272  0.1106682941\r\n[673]  0.0756958127  0.0101066353  0.2055893093  0.1658867449\r\n[677] -0.0501063056  0.0134348497  0.0426327512  0.0528929085\r\n[681]  0.0533999354  0.0504605025  0.1714175940  0.0124473581\r\n[685] -0.0630341023 -0.0481487587  0.0119474344 -0.0312205367\r\n[689] -0.0983770266 -0.0657716990  0.0097869402  0.0815106332\r\n[693] -0.0783884451  0.1317924112  0.1393555254  0.0091066100\r\n[697]  0.1090818569  0.0826683417 -0.0873613730  0.0406477526\r\n[701]  0.0024024630  0.0293322615 -0.0107911229 -0.0422949791\r\n[705]  0.0773078799  0.0439093374  0.0041232221 -0.0003319718\r\n[709] -0.0733481720  0.0519525483 -0.1517664492  0.0979920477\r\n[713]  0.1458352655  0.0452605821 -0.0459416397 -0.0361512005\r\n[717] -0.0092587732 -0.0254314225 -0.1035516784  0.0726805553\r\n[721]  0.0867970884  0.0103221089  0.0760935023 -0.0238303263\r\n[725] -0.0003891811 -0.0292554908 -0.1475816667  0.0020689443\r\n[729] -0.1005423963  0.1260626465  0.1335250735 -0.0404047295\r\n[733]  0.0887767822 -0.0295703020 -0.0424162671 -0.0948930085\r\n[737] -0.0572368279  0.0164915603 -0.0415635332  0.0422985479\r\n[741] -0.0588609762  0.1055561453  0.0921511501  0.0129698375\r\n[745] -0.0341896787 -0.0933145285  0.0428062342 -0.1376243681\r\n[749] -0.0494870879 -0.5013434291  0.1010253131 -0.1516790539\r\n[753] -0.1601956934 -0.0478142425 -0.0450621769 -0.0462121964\r\n[757]  0.2260329872 -0.1113839298  0.0209654514  0.0178512931\r\n[761]  0.0428405628  0.1008379236  0.0504300594  0.0481586978\r\n[765]  0.2165928781 -0.1681894660 -0.0393629372 -0.0666962191\r\n\r\nSimilarly, we can get the vector of numerical embeddings for a whole\r\nsentence.\r\n\r\n\r\nmodel$encode('I like to drink Turkish coffee')\r\n\r\n  [1] -0.01178756263  0.10278499871  0.01093375869 -0.04623499140\r\n  [5] -0.00487077981 -0.04498281330  0.08225313574 -0.03068084642\r\n  [9]  0.00254547270 -0.07485476881  0.00626886077 -0.16109354794\r\n [13]  0.07325114310 -0.00854813121  0.04085500538  0.28260263801\r\n [17]  0.11343766004  0.13471494615  0.07055664062  0.36907771230\r\n [21] -0.02329965308  0.12997290492 -0.08797073364  0.00176735315\r\n [25] -0.13596564531  0.05665922537  0.11902851611 -0.01500649936\r\n [29]  0.13772100210 -0.00496985437 -0.08434423804 -0.07868374884\r\n [33]  0.02161235362 -0.03984801471  0.04456693679  0.05849874020\r\n [37]  0.11640743166  0.02286028862 -0.00947276782  0.00502380542\r\n [41]  0.04630419612 -0.32184061408 -0.02034790069  0.01740729809\r\n [45]  0.01355001330 -0.04638009891 -0.05697692558 -0.14055319130\r\n [49]  0.07457129657 -0.01287342701 -0.01593326218  0.07576484978\r\n [53]  0.02938880213  0.04194469377 -0.05589605868  0.03272417933\r\n [57]  0.03844407946  0.05413730443  0.12071878463 -0.07491374761\r\n [61]  0.01938630454  0.54147577286 -0.15505477786  0.05381580815\r\n [65]  0.04042135552 -0.00642709574 -0.00746040652 -0.12155018747\r\n [69] -0.02452480420  0.10893430561  0.03051396087 -0.08853682876\r\n [73]  0.04928927124 -0.08943001181  0.01897253469  0.07131873071\r\n [77]  0.06871882826 -4.13642501831  0.13351145387  0.07664902508\r\n [81]  0.03651222214 -0.09673640132  0.89217787981 -0.04306836054\r\n [85]  0.04194186255 -0.05514504015 -0.02103988640  0.17559264600\r\n [89]  0.01227049716 -0.00386030739  0.04441806674 -0.03551847115\r\n [93] -0.07538413256  0.11143504083  0.04939413816  0.03816314042\r\n [97]  0.09854111820 -0.02303134464 -0.00179665210 -0.00085414201\r\n[101] -0.11806513369 -0.02063305676  0.02362795174  0.09661674500\r\n[105]  0.01085869595 -0.12519752979  0.10698405653 -0.03740838915\r\n[109] -0.08884917200  0.13770172000  0.10585375130 -0.09237524122\r\n[113]  0.14154621959 -0.00389125478 -0.05083238333  0.07898531109\r\n[117]  0.04267026484  0.05936061218 -0.06123913825  0.06296721101\r\n[121]  0.05586537346  0.03252293542  0.14736802876  0.07342395186\r\n[125] -0.25260412693 -0.08185942471  0.04785029590 -0.03871662542\r\n[129] -0.01667318121  0.04311851785 -0.04991150647 -0.60922294855\r\n[133] -0.00484278006 -0.06470596045 -0.00291156583 -0.11511138827\r\n[137] -0.01698307693 -0.03271684051 -0.05950184911 -0.12982320786\r\n[141]  0.01724223420  0.03518041596  0.02934072725  0.04171930999\r\n[145]  0.30440011621  0.02517579868 -0.07006555051 -0.23346969485\r\n[149]  0.00844197161  0.05099269748  0.03252385557 -0.04751272500\r\n[153] -0.07027619332  0.08984416723 -0.06273024529  0.14591774344\r\n[157]  0.09863515198  0.25520563126 -0.07795181870  0.43738499284\r\n[161]  0.12964035571  0.01454514638  0.01953442395 -0.02825888619\r\n[165]  0.00075821672  0.00771219516 -0.01024915092  0.04318773746\r\n[169]  0.07650490105  0.08267024904 -0.00568555202  0.03067611344\r\n[173] -0.04012656957  0.14161375165  0.01266532484 -0.13634182513\r\n[177] -0.03952530026  0.09026617557  0.00772870053 -0.01258897781\r\n[181]  0.06947132200  0.05105271935  0.00032814220  0.00196462078\r\n[185] -0.01595257409  0.11271636188  0.15407632291  0.05832265317\r\n[189]  0.11283170432  0.04487172142 -0.04882197082  0.05088605359\r\n[193] -0.02264021337  0.07404132932  0.07896973193  0.10499487817\r\n[197] -0.06692550331 -0.03697898984  0.06937737763  0.04945894331\r\n[201]  0.05113966018 -0.08194012195 -0.10721433163 -0.04897894710\r\n[205]  0.15065754950  0.10129641742  0.02087124810  0.10874668509\r\n[209]  0.04760009423 -0.06162553653 -0.09491002560 -0.12216342241\r\n[213] -0.02830585092  0.03591910750 -0.04520691186 -0.16342914104\r\n[217]  0.06836596876  0.04436348751  0.04822815955  0.41608574986\r\n[221]  0.07999478281  0.04446204752 -0.09777959436  0.00191505952\r\n[225] -0.08722338080  0.11013680696  0.14092494547 -0.07029317319\r\n[229]  0.15053604543 -0.16089598835 -0.02733201906 -0.04974053055\r\n[233] -0.10599211603  0.07982736826  0.09201110154 -0.40470686555\r\n[237]  0.05906889960 -0.01318830065  0.01357291173 -0.02731262147\r\n[241] -0.65026259422  0.08871906251  0.08232256770  0.07668064535\r\n[245]  0.04042013362  0.02460350096 -0.00144912163  0.17354218662\r\n[249]  0.14589069784 -0.04771002010  0.01597190090  0.08966311067\r\n[253]  0.04908165336 -0.11419168115 -0.08502776921 -0.08974553645\r\n[257] -0.12737628818  0.04935332388 -0.15694443882 -0.06124186516\r\n[261]  0.03666267172  0.02004068159  0.04401797429 -0.15641340613\r\n[265] -0.05760531500  0.07650154829 -0.03167368472  0.07931122184\r\n[269]  0.00901504979 -0.06132012978  0.01697015762 -0.02927378938\r\n[273] -0.04857507348  0.02371818013  0.03179942816 -0.04552056640\r\n[277]  0.17627295852 -0.02516534552 -0.05752530321 -0.01505636424\r\n[281]  0.08774642646 -0.02125256509  0.18481005728  0.08228832483\r\n[285]  0.06763889641  0.01308839396 -0.13039453328 -0.03017592058\r\n[289] -0.01873817667  0.08929710090  0.05834455416 -0.03174070269\r\n[293]  0.01390640810 -0.03864267841 -0.09253222495 -0.01728121378\r\n[297]  0.12410937250 -0.04100821540  0.04707189649  0.04714074731\r\n[301]  0.10770967603 -0.00315346196 -0.10042775422  0.08935348690\r\n[305] -0.01404160354 -0.04688004032 -0.07279075682  0.05749002844\r\n[309] -0.06967109442 -0.05545315892 -0.02552834339 -0.14972683787\r\n[313]  0.06309494376  0.15150888264 -0.00864394754  0.01057897694\r\n[317]  0.00592292240  0.00970546529  0.08703348786  0.03069057688\r\n[321] -0.03375400230 -0.06739930809  0.09072256833  0.07848568261\r\n[325]  0.11284592748 -0.08407180011  0.07819942385  0.03995769098\r\n[329]  0.07009632140  0.11188885570 -0.24580949545  0.90001469851\r\n[333]  0.12089532614 -0.01227924973 -0.06359811872  0.03523534164\r\n[337]  0.01611461677  0.08675729483  0.10483931005  0.01012298930\r\n[341]  0.00031804154 -0.00818176940 -0.16627056897  0.00469068484\r\n[345]  0.02130271308  0.07732348144  0.04926919565 -0.05670228973\r\n[349] -0.04631502181  0.10687955469 -0.03332764655  0.01406783424\r\n[353]  0.09957949817 -0.15667900443 -0.00317216013  0.01619793475\r\n[357]  0.13785058260 -0.03695226461  0.01658136211 -0.06813907623\r\n[361]  0.08336272836  0.16922046244  0.00247423816 -0.08443702757\r\n[365]  0.05174981058  0.09740118682  0.26251202822  0.13505578041\r\n[369] -0.04359325022  0.09619217366  0.03377174959 -0.00218817405\r\n[373] -0.01017327979 -0.08413492888 -0.05180362985 -0.07273145020\r\n[377]  0.05030424520  0.08751272410  0.16093406081 -0.03390208632\r\n[381] -0.01043029781 -0.03548621386 -0.01699107513  0.01912876405\r\n[385] -0.07973151654  0.04320683703  0.01989768445 -0.12756203115\r\n[389] -0.03098621033  0.00650840718 -0.07768756151  0.10089147091\r\n[393] -0.07954857498 -0.01790400781  0.01669675484  0.05395016819\r\n[397]  0.03829964250  0.14281614125  0.20642946661 -0.05247383565\r\n[401]  0.04683664814  0.10827600211 -0.06957665086 -0.07571008056\r\n[405] -0.05944715440 -0.01344989147 -0.07810124010 -0.04226115346\r\n[409] -0.15824672580 -0.03212283924  0.09794007242  0.09059050679\r\n[413]  0.01046781149  0.00020543486 -0.06280449033  0.02410167083\r\n[417] -0.05018439144 -0.06793823093  0.00573871192 -0.03653944656\r\n[421]  0.02161061764 -0.01924452372  0.04822736979  0.02471604571\r\n[425]  0.12235145271  0.04452153668  0.13469699025  0.01670137979\r\n[429] -0.02470082045 -0.05284336954 -0.10143402964  0.00922968425\r\n[433]  0.03362737596 -0.04423018172 -0.00413170224 -0.01949813962\r\n[437]  0.00942119583 -0.20146682858 -0.05510338396  0.01366847288\r\n[441] -0.04772661254  0.07698956877  0.06640637666  0.02397813834\r\n[445] -0.14183472097 -0.04575728625 -0.15320336819 -0.00788022485\r\n[449]  0.02135960758 -0.01345346309  0.10530500114 -0.01646697707\r\n[453] -0.03888763115 -1.08305180073 -0.00083741685  0.11160721630\r\n[457]  0.02770528942 -0.01054597273 -0.06605579704  0.04412012547\r\n[461]  0.08192968369  0.04005576298 -0.05635560304  0.05495367944\r\n[465]  0.01402659342 -0.15851949155 -0.03023152798  0.19572809339\r\n[469]  0.01863588765  0.06902951002 -0.01266082004  0.02299657091\r\n[473] -0.00470078411 -0.07729942352  0.01735690981 -0.00720282644\r\n[477]  0.07002413273  0.09092012048  0.03493273631  0.06802241504\r\n[481] -0.01505678706  0.02954206988 -0.01877148636  0.03480780497\r\n[485] -0.10423323512 -0.07202734798 -0.01704173721  0.01502756495\r\n[489]  0.04665532708 -0.06358810514 -0.01287578978 -0.05168087035\r\n[493] -0.01075313706 -0.00320516946  0.19674751163  0.04860872403\r\n[497] -0.41238608956  0.01066310983  0.32675898075 -0.06341576576\r\n[501]  0.02628972009  0.01365064457  0.07002934068 -0.04391351342\r\n[505]  0.07600034028  0.14016662538 -0.03555902466 -0.04067653418\r\n[509]  0.09995693713  0.03394508362  0.05223489180 -0.25556266308\r\n[513]  0.00745464209 -0.01409167238  0.02850508317  0.12841719389\r\n[517]  0.02112706937  0.09976664931  0.02270895615 -0.25903543830\r\n[521] -0.03593028709  0.07121115178 -0.01665394753 -0.01663682610\r\n[525] -0.05931027979 -0.06853275001  0.06115171313  0.03058695793\r\n[529]  0.13712780178 -0.09377148747 -0.06594183296 -0.06134198606\r\n[533]  0.07561255246 -0.00017064391 -0.16877844930  0.04635048658\r\n[537]  0.04027160257  0.13218209147 -0.03819207847  0.07642486691\r\n[541] -0.02910984680  0.12374015898  0.05625952035  0.02131085657\r\n[545]  0.18835608661  0.04942709580  0.12962301075  0.00504925707\r\n[549]  0.04721711576 -0.08219789714 -0.04640511796 -0.12387338281\r\n[553] -0.04727662727  0.01413739100  0.05141320080 -0.04458754510\r\n[557]  0.03157661483  0.12638874352 -0.00524097309  0.08917185664\r\n[561]  0.01452164445 -0.35762175918 -0.01392915286  0.04477771372\r\n[565] -0.03660982102  0.01903883368  0.09178100526  0.05326319113\r\n[569] -0.01747901738 -0.05025598407 -0.01110013016 -0.05249111727\r\n[573] -0.03659915924  0.01587794907 -0.01250017527  0.02968469635\r\n[577]  0.10908561200  0.14437158406 -0.14502903819  0.05590865389\r\n[581]  0.07278285921 -0.04392333329  0.01164120249  0.17424646020\r\n[585]  0.05410258844  0.07681672275  0.07232151926 -0.06711075455\r\n[589] 10.07463550568 -0.01011656411  0.01704577543  0.10929690301\r\n[593]  0.06012175232  0.05607127026  0.02889964171 -0.04166502133\r\n[597] -0.00205596257  0.09235295653 -0.04838714749  0.05196825787\r\n[601] -0.02444161847  0.01730557531  0.01771085337  0.01625886187\r\n[605] -0.28358867764  0.08354127407  0.10569129139  0.01898630336\r\n[609]  0.12275204062  0.03809059784  0.03666390106  0.12680123746\r\n[613] -0.15032386780  0.21821279824  0.18539962173  0.05414790660\r\n[617] -0.00749016646  0.08893932402  0.07488542795  0.05545224249\r\n[621]  0.02915835194  0.00856588595  0.13660992682  0.15385082364\r\n[625] -0.06263649464  0.08125825971 -0.00002325047  0.23185446858\r\n[629]  0.03506195918 -0.09679391980  0.01323065255  0.00519346818\r\n[633]  0.03788876534  0.05919850618  0.09609610587  0.05751947314\r\n[637]  0.03957244754  0.01264949236  0.01535425242 -0.06162425131\r\n[641]  0.02908030897  0.14106319845 -0.01901833341  0.11276496202\r\n[645] -0.03936489671  0.00099390326 -0.00366679300 -0.01762092672\r\n[649] -0.00928323530  0.07514175028  0.06850954145 -0.02549809590\r\n[653]  0.08762409538  0.08077415079  0.06784472615  0.08132497966\r\n[657] -0.01183548942 -0.00072143879  0.02146123536  0.11198233068\r\n[661] -0.07392378896  0.03819230571  0.01816147566 -0.04830361903\r\n[665] -0.19510628283 -0.04690299183 -0.07854089141 -0.05419109017\r\n[669]  0.02331295051 -0.00938901678  0.07046185434  0.09199322015\r\n[673]  0.21238797903  0.02792359702  0.11243189871  0.02591974288\r\n[677] -0.00615646644  0.03513993695  0.14261384308  0.07364098728\r\n[681]  0.10220479965  0.01609489322  0.02398936078 -0.02556908131\r\n[685]  0.00179124018 -0.02348809317  0.08397649974 -0.08257681876\r\n[689] -0.04396900907  0.04995858297 -0.05033384636  0.05998511612\r\n[693]  0.11537389457 -0.01593917795 -0.06055822968 -0.16152115166\r\n[697] -0.01114141382  0.02739814483 -0.01101099607 -0.04443185404\r\n[701] -0.00682633510  0.04704710469 -0.02496541105 -0.08497899026\r\n[705] -0.06010052189  0.06924650073 -0.03039424494  0.07574173063\r\n[709] -0.00660725730  0.03008056991 -0.02600940131  0.03524253145\r\n[713]  0.03677238151 -0.05480838194  0.04589191079 -0.05034032464\r\n[717] -0.02219916880  0.00385648594  0.05007091165  0.08368641138\r\n[721]  0.05869836733 -0.04817376286  0.00500276731  0.01969382726\r\n[725]  0.02232522331 -0.00405918434 -0.03342300653 -0.05019075796\r\n[729] -0.05556573719 -0.13569174707  0.05072744191 -0.48665577173\r\n[733]  0.04441469908  0.02524072118 -0.14115256071  0.17032654583\r\n[737]  0.07977185398 -0.06868468225 -0.03465748578 -0.05387150124\r\n[741] -0.05741538852 -0.02516146377  0.15718893707  0.01091605332\r\n[745] -0.04058590904 -0.01980805956  0.02870250493 -0.01669016480\r\n[749] -0.03890883923 -0.60106384754  0.03322814405  0.02858895808\r\n[753]  0.18336550891  0.12473639101  0.01609408110  0.08886212856\r\n[757]  0.07076863199  0.00996986963 -0.06644231826  0.00503187347\r\n[761] -0.02441483364 -0.03701018542  0.04051119834  0.03330204263\r\n[765]  0.10524793714  0.11114552617  0.07725711912 -0.14208619297\r\n\r\nThe input can be many sentences. For instance, if I submit a vector\r\nof three sentences as an input, the model returns a 3 x 768 matrix\r\ncontainng sentence embeddings. Each row contains the embeddings for a\r\nsentence.\r\n\r\n\r\nmy.sentences <- c('The weather today is great.',\r\n                  'I live in Eugene.',\r\n                  'I am a graduate student.')\r\n\r\nembeddings <- model$encode(my.sentences)\r\n\r\ndim(embeddings)\r\n\r\n[1]   3 768\r\n\r\nhead(embeddings)\r\n\r\n             [,1]       [,2]        [,3]        [,4]      [,5]\r\n[1,]  0.017466936 0.18936765  0.05655781 -0.10007770 0.3477369\r\n[2,] -0.116279386 0.15185139  0.02549255 -0.18140650 0.3024701\r\n[3,]  0.004130477 0.09689271 -0.02288255  0.01698708 0.1297401\r\n            [,6]        [,7]        [,8]        [,9]        [,10]\r\n[1,] 0.161116362  0.01379415 -0.06983393  0.21350072 -0.086530611\r\n[2,] 0.007331764  0.03502100  0.09465420 -0.02342363  0.003951203\r\n[3,] 0.100442544 -0.03860680  0.05823556 -0.10520023 -0.094153374\r\n           [,11]       [,12]        [,13]       [,14]       [,15]\r\n[1,] -0.03210545  0.18931536 -0.082832702 -0.04288674 -0.05687425\r\n[2,] -0.06632143 -0.14414917 -0.007048744 -0.07298838  0.11057709\r\n[3,] -0.13373426 -0.02797971 -0.043431252  0.06593540  0.06813642\r\n          [,16]      [,17]       [,18]        [,19]     [,20]\r\n[1,] -0.1995602 0.12531294 -0.25097796 -0.007122722 0.1272842\r\n[2,]  0.1434624 0.02711638  0.06095798  0.015504715 0.3171880\r\n[3,]  0.1352124 0.05530372 -0.05192044  0.129772440 0.1445196\r\n           [,21]      [,22]       [,23]        [,24]       [,25]\r\n[1,] -0.02212711 0.02939240 -0.06768320 -0.031360045 -0.02374818\r\n[2,] -0.11055427 0.17259933  0.06908871  0.007150877 -0.09578121\r\n[3,] -0.19155622 0.08215951 -0.04485236  0.025491279 -0.07901977\r\n           [,26]        [,27]       [,28]      [,29]       [,30]\r\n[1,] -0.01850537 -0.073677890  0.08197053 0.04476932 -0.01092185\r\n[2,]  0.11460673  0.007118554  0.06077866 0.07339842  0.07566937\r\n[3,]  0.14188063 -0.059332389 -0.07300105 0.02767119  0.05440608\r\n          [,31]       [,32]      [,33]      [,34]      [,35]\r\n[1,]  0.0516919 -0.12200159 0.06565627 0.08309685 -0.0631547\r\n[2,] -0.1437625 -0.07774637 0.12881884 0.03798771 -0.2033215\r\n[3,] -0.1472241 -0.08245133 0.04026355 0.04781771 -0.1529378\r\n          [,36]      [,37]      [,38]      [,39]       [,40]\r\n[1,] 0.00124214 0.21495906 0.02378797 0.41673407  0.10881428\r\n[2,] 0.03539575 0.02797152 0.02694622 0.18206938 -0.00243298\r\n[3,] 0.16710928 0.04931019 0.07693321 0.03029568 -0.03445346\r\n           [,41]       [,42]       [,43]       [,44]       [,45]\r\n[1,] -0.02317035 -0.04610297 -0.07775073  0.09374794  0.01631867\r\n[2,]  0.18405415 -0.09677307  0.02205182 -0.05291918 -0.12745722\r\n[3,]  0.04713881 -0.10469180  0.01276998 -0.11525831 -0.12307585\r\n           [,46]        [,47]        [,48]      [,49]       [,50]\r\n[1,] -0.08893959 -0.066087134 -0.033896975 0.03748417 -0.09860551\r\n[2,] -0.05699166  0.026511583  0.005998232 0.04043084  0.09988649\r\n[3,] -0.02619796  0.006607132  0.128890276 0.04518945 -0.02991036\r\n           [,51]        [,52]       [,53]      [,54]       [,55]\r\n[1,]  0.01296319 -0.003461072 -0.02627736 0.01285952 -0.06320502\r\n[2,] -0.03472668  0.126700595  0.05529476 0.04296899  0.01611242\r\n[3,]  0.02033573  0.302788228 -0.01722694 0.13887501 -0.06220450\r\n          [,56]      [,57]       [,58]       [,59]       [,60]\r\n[1,] 0.04111816 0.12291817  0.27477175  0.09285492 -0.05965450\r\n[2,] 0.17848778 0.10158963  0.10257996  0.07757163  0.03783483\r\n[3,] 0.08246204 0.02561423 -0.01797309 -0.01141094 -0.06575820\r\n           [,61]       [,62]       [,63]       [,64]       [,65]\r\n[1,] -0.12136278 -0.05212643  0.01001978 -0.03749729 -0.08881954\r\n[2,] -0.08906478  0.34973344 -0.18619291 -0.24437067  0.02749443\r\n[3,] -0.10261139  0.16414776 -0.19891870 -0.09115140  0.15854540\r\n            [,66]      [,67]      [,68]       [,69]      [,70]\r\n[1,] -0.024980947 0.08160084 0.14402378 -0.02253270 0.18326229\r\n[2,] -0.007333713 0.05363137 0.07141103  0.01306976 0.14572120\r\n[3,] -0.083556786 0.08434479 0.10600642  0.07140820 0.03421079\r\n           [,71]       [,72]       [,73]       [,74]       [,75]\r\n[1,]  0.01987223 -0.10938195  0.05411881  0.02580995  0.05389390\r\n[2,] -0.02115243  0.01261677 -0.01681857  0.05305029 -0.03361567\r\n[3,]  0.04432295  0.18048577 -0.02966901 -0.11892220 -0.02450558\r\n           [,76]       [,77]     [,78]       [,79]       [,80]\r\n[1,]  0.08346283 -0.06182058 -4.744359 -0.18744363 -0.17499693\r\n[2,] -0.02086468  0.13247438 -3.307262  0.13413826  0.10885518\r\n[3,] -0.14510953  0.16692485 -3.742653 -0.01816995 -0.04116233\r\n          [,81]      [,82]     [,83]       [,84]      [,85]\r\n[1,] 0.14713387 -0.1248986 0.9754919 -0.06475981 0.05608290\r\n[2,] 0.07537901 -0.2367227 1.3949689  0.06399761 0.13651498\r\n[3,] 0.02652982 -0.2597603 1.3483980  0.19230933 0.02012401\r\n           [,86]      [,87]        [,88]       [,89]        [,90]\r\n[1,] -0.30756178 0.11022718  0.171431765  0.06668350 -0.003124851\r\n[2,]  0.06031471 0.04747717 -0.005528938  0.02481796  0.014205792\r\n[3,]  0.03678771 0.16980240  0.151860595 -0.04821039 -0.004139557\r\n          [,91]        [,92]       [,93]       [,94]      [,95]\r\n[1,] 0.04622074  0.090181261 -0.04604505  0.06580640 0.14173642\r\n[2,] 0.06329937  0.008838675 -0.07240096 -0.06188769 0.09276354\r\n[3,] 0.01915987 -0.135672256 -0.01942731  0.03034206 0.03634046\r\n           [,96]       [,97]     [,98]       [,99]       [,100]\r\n[1,]  0.18965936 -0.04988672 0.5766389 -0.03957188  0.007752549\r\n[2,] -0.06889861 -0.11589235 0.5682074 -0.07129852 -0.091406807\r\n[3,] -0.07978459 -0.02496983 0.6747148 -0.05331716 -0.107529968\r\n          [,101]      [,102]    [,103]     [,104]      [,105]\r\n[1,] -0.01312273 -0.01181763 0.1199985 0.14943148 -0.10740668\r\n[2,] -0.05813193  0.01646996 0.1975683 0.20346186 -0.05206105\r\n[3,] -0.01322445 -0.06349409 0.2546781 0.09124219 -0.03539053\r\n         [,106]      [,107]      [,108]     [,109]      [,110]\r\n[1,] 0.02258746  0.04812491 -0.06409516 0.04273710 -0.01460021\r\n[2,] 0.11045092 -0.08600713 -0.05832916 0.04531546 -0.05098567\r\n[3,] 0.02597254 -0.03470686 -0.11045704 0.12626635  0.09034445\r\n           [,111]     [,112]      [,113]      [,114]       [,115]\r\n[1,]  0.120807193  0.1195999  0.18925950 -0.09271472 -0.057085268\r\n[2,] -0.001811178 -0.1268563 -0.05092565 -0.05587747  0.096366778\r\n[3,]  0.097552113 -0.2609514  0.02977949  0.08170151 -0.006335007\r\n          [,116]      [,117]     [,118]      [,119]      [,120]\r\n[1,]  0.11585514 -0.05049544 0.07540878  0.01785059  0.06430135\r\n[2,]  0.02522018  0.07276074 0.10370429 -0.07917025  0.01869858\r\n[3,] -0.05168045  0.04032857 0.19675742 -0.01150821 -0.01968286\r\n          [,121]      [,122]      [,123]       [,124]      [,125]\r\n[1,] -0.04298311  0.04079489 -0.06041414 -0.011750890 -0.04550166\r\n[2,] -0.05321378 -0.04124171  0.06694493  0.070510812 -0.21030827\r\n[3,] -0.03093176  0.07614435  0.02510202 -0.008475666 -0.02347328\r\n          [,126]      [,127]      [,128]       [,129]      [,130]\r\n[1,]  0.11097955 -0.24637091 0.046722800 -0.048847396 0.073720053\r\n[2,] -0.10665352 -0.07501517 0.007717835  0.009107843 0.001662996\r\n[3,] -0.05122698  0.01678034 0.023123045  0.031012105 0.063838013\r\n          [,131]     [,132]      [,133]     [,134]      [,135]\r\n[1,] -0.13415056 -0.4292295 -0.02677542 0.14952262 -0.06541855\r\n[2,] -0.03282406 -0.5320446  0.02072983 0.02825750 -0.02982503\r\n[3,] -0.01259000 -0.5687474 -0.15318406 0.07306065  0.05096757\r\n          [,136]      [,137]      [,138]      [,139]      [,140]\r\n[1,]  0.08021230 -0.09341189 -0.02671690 -0.15176196  0.02962985\r\n[2,] -0.07013462  0.01992375 -0.06950717 -0.04408696 -0.09377313\r\n[3,] -0.09520675  0.08959299 -0.04694959 -0.05613213 -0.10142585\r\n          [,141]      [,142]      [,143]       [,144]       [,145]\r\n[1,]  0.16161388  0.07141577  0.06372322  0.005867817 -0.002618913\r\n[2,] -0.01699820 -0.10967018 -0.04513898  0.081730612  0.294857025\r\n[3,]  0.01110352 -0.12602286  0.01570023 -0.051091015  0.124610789\r\n          [,146]      [,147]       [,148]     [,149]     [,150]\r\n[1,] -0.01872567 -0.04991600 -0.027256506 0.05930051 0.03952531\r\n[2,]  0.09415734  0.01551705 -0.115617387 0.06616315 0.12677199\r\n[3,]  0.25578880  0.02818135 -0.007225338 0.01288412 0.15643610\r\n        [,151]      [,152]      [,153]       [,154]      [,155]\r\n[1,] 0.0230126  0.04690588 -0.04385430  0.025889043  0.05686368\r\n[2,] 0.1484294 -0.05647697 -0.05922592  0.063226916 -0.02238526\r\n[3,] 0.1842019 -0.11539790 -0.09148663 -0.006457172  0.01688721\r\n        [,156]      [,157]     [,158]      [,159]     [,160]\r\n[1,] 0.1612850  0.02808929 0.07576993 -0.03871185 0.27184317\r\n[2,] 0.3360443  0.06559707 0.36423922 -0.05806160 0.11002246\r\n[3,] 0.3965186 -0.03016575 0.31495151  0.15312929 0.03696651\r\n         [,161]      [,162]      [,163]      [,164]       [,165]\r\n[1,] 0.29307944 -0.08241996 -0.01799023 -0.11790153 -0.025170870\r\n[2,] 0.09761115  0.05626684  0.03946232  0.03259929 -0.039699692\r\n[3,] 0.17220785 -0.04270339 -0.02529519 -0.03280073 -0.007628142\r\n          [,166]      [,167]     [,168]      [,169]      [,170]\r\n[1,] -0.04706457 -0.03228906 0.01486580  0.03410897  0.08997094\r\n[2,] -0.02353669 -0.12128777 0.05145064  0.01461882 -0.01191414\r\n[3,] -0.02756393 -0.11026121 0.03721116 -0.08949099  0.05856310\r\n          [,171]       [,172]      [,173]      [,174]      [,175]\r\n[1,] -0.04632119  0.135610104 -0.02321601 -0.07550758 -0.16381302\r\n[2,] -0.01514443 -0.038992934 -0.03243395  0.01624453 -0.04762726\r\n[3,] -0.02180461 -0.006510535  0.01781113  0.02623862 -0.04048797\r\n          [,176]     [,177]       [,178]     [,179]     [,180]\r\n[1,] -0.04189295 0.15148470  0.140410155 0.04326902 0.05095452\r\n[2,] -0.11555878 0.08569998 -0.003490022 0.01828323 0.05416968\r\n[3,] -0.16425563 0.17146082  0.068619400 0.13635762 0.09055427\r\n          [,181]       [,182]       [,183]      [,184]      [,185]\r\n[1,] -0.01218278  0.047707740 -0.065614261  0.03087835 0.019091025\r\n[2,] -0.05771073  0.008110725  0.029411763 -0.03026832 0.004953566\r\n[3,] -0.15558219 -0.004117484 -0.008401374 -0.03395857 0.082504667\r\n        [,186]      [,187]      [,188]       [,189]     [,190]\r\n[1,] 0.1010152 0.093787849  0.04519366 -0.149715766 0.02564542\r\n[2,] 0.2768288 0.002896491  0.00288205 -0.002317041 0.20472515\r\n[3,] 0.3093906 0.185895532 -0.05093533  0.102967449 0.16119444\r\n         [,191]      [,192]       [,193]      [,194]       [,195]\r\n[1,] 0.08153410 -0.08363193  0.116248876  0.07510559  0.028269954\r\n[2,] 0.06308736 -0.03121299 -0.009620543 -0.12536697 -0.009901358\r\n[3,] 0.11336213 -0.01572398 -0.039867125 -0.06070736  0.071869873\r\n         [,196]     [,197]      [,198]        [,199]      [,200]\r\n[1,] 0.04374793  0.0306067 -0.01423122 -0.0009407345  0.01116109\r\n[2,] 0.03533378 -0.1049785 -0.04820460 -0.0315684490 -0.05145582\r\n[3,] 0.01109681 -0.1666481 -0.05644229  0.0661051124 -0.07637785\r\n         [,201]      [,202]    [,203]      [,204]      [,205]\r\n[1,] 0.09029138 -0.01473477 0.1183517 -0.06788336 0.002514364\r\n[2,] 0.01361644  0.06719654 0.2151488 -0.06788865 0.020254625\r\n[3,] 0.04770661  0.17415440 0.2358820 -0.01688100 0.109950788\r\n          [,206]      [,207]      [,208]       [,209]      [,210]\r\n[1,] -0.10337599 -0.15819739 -0.10490708 -0.017575335 -0.06240090\r\n[2,]  0.10523003  0.08059917  0.05202673  0.057690587  0.02943062\r\n[3,]  0.02377997 -0.09855437  0.05364608  0.004027244  0.07190191\r\n           [,211]      [,212]      [,213]      [,214]     [,215]\r\n[1,] -0.016532600 -0.07328504  0.08365475  0.34286380 -0.0963589\r\n[2,] -0.008552969 -0.07551909 -0.12199020  0.25189748 -0.0565316\r\n[3,] -0.119144619  0.05820637 -0.08432852 -0.05856914  0.0607432\r\n          [,216]      [,217]     [,218]      [,219]    [,220]\r\n[1,] -0.17888170  0.03668439 -0.7004665  0.04737864 0.1626685\r\n[2,] -0.02578335 -0.05152095 -0.3525909 -0.02983569 0.1501971\r\n[3,] -0.03525455 -0.12300278 -0.2542538  0.16009760 0.1010381\r\n          [,221]      [,222]      [,223]      [,224]      [,225]\r\n[1,]  0.16919723 -0.09332245  0.20004980  0.10661260 0.027264655\r\n[2,]  0.01113881 -0.00366085  0.04691055  0.02603983 0.070807993\r\n[3,] -0.02853836  0.07157905 -0.13754308 -0.03224405 0.009818178\r\n         [,226]     [,227]      [,228]       [,229]      [,230]\r\n[1,] 0.02428964 0.06207667 -0.08166093  0.113831602  0.03132034\r\n[2,] 0.07769687 0.07915758 -0.02231407  0.084806718 -0.15689461\r\n[3,] 0.04967513 0.04366930 -0.13910171 -0.005473904  0.01076712\r\n          [,231]      [,232]        [,233]       [,234]    [,235]\r\n[1,] -0.03456382 -0.07086996 -0.0664970949  0.079126269 0.1576967\r\n[2,] -0.01303482  0.11235482 -0.0472119972 -0.007066698 0.1879414\r\n[3,] -0.06117841  0.13229525 -0.0006693983  0.038082622 0.1642626\r\n          [,236]       [,237]      [,238]      [,239]      [,240]\r\n[1,] -0.07242861 -0.060679540 0.007488072 -0.05117263  0.13693771\r\n[2,] -0.18020511 -0.005515292 0.005323615  0.04557093  0.01427150\r\n[3,] -0.20792490 -0.026204484 0.116029240 -0.00965337 -0.06306633\r\n          [,241]     [,242]     [,243]     [,244]      [,245]\r\n[1,] -0.37703294 0.13546734 0.05247532 0.05501491 0.082340881\r\n[2,] -0.06490818 0.05138187 0.06495947 0.05038793 0.007074311\r\n[3,] -0.41937870 0.14372210 0.04367482 0.12992582 0.131404966\r\n          [,246]      [,247]     [,248]    [,249]      [,250]\r\n[1,] -0.16188540 0.007680578  0.1909865 0.2358013 -0.11898704\r\n[2,]  0.08033013 0.029313516 -0.1486792 0.2157641  0.05247597\r\n[3,] -0.09704037 0.016597759 -0.2120424 0.2251720 -0.04899136\r\n          [,251]     [,252]      [,253]     [,254]      [,255]\r\n[1,] -0.12922350 -0.3348031 -0.01761893 -0.1492761 -0.07322966\r\n[2,] -0.05986992 -0.1188031  0.02589624 -0.1675690  0.01387598\r\n[3,] -0.06133738 -0.1153320  0.13166033 -0.1098961 -0.05030071\r\n          [,256]      [,257]      [,258]       [,259]      [,260]\r\n[1,] -0.09891027 -0.07905637 -0.01703418 -0.002923179  0.02023862\r\n[2,] -0.05357978 -0.16829614  0.07828091 -0.223703027  0.27632055\r\n[3,] -0.17185661 -0.08030650  0.01444841 -0.172713190 -0.07524167\r\n           [,261]      [,262]      [,263]      [,264]     [,265]\r\n[1,] -0.006831818  0.04149275  0.19686872 -0.08984916 -0.1866602\r\n[2,] -0.006285527 -0.09683912 -0.03744879 -0.05834809 -0.1240842\r\n[3,]  0.012716721  0.03523421 -0.02654512 -0.14184952 -0.1396117\r\n        [,266]      [,267]      [,268]     [,269]     [,270]\r\n[1,] 0.1602448 -0.05983455 -0.02839560 0.06783184 0.12312593\r\n[2,] 0.0912225 -0.01839774  0.12244660 0.11336525 0.03489619\r\n[3,] 0.0574608 -0.01124171  0.06315872 0.09125546 0.17926168\r\n          [,271]       [,272]      [,273]     [,274]      [,275]\r\n[1,] -0.01121200  0.031648841 -0.02484243 0.07623786 -0.02644233\r\n[2,] -0.07443111  0.008801795 -0.05950159 0.03754820  0.10479835\r\n[3,]  0.01950239 -0.155382127 -0.19454606 0.11245190  0.02365979\r\n          [,276]     [,277]      [,278]      [,279]       [,280]\r\n[1,]  0.14516488 0.01943621 -0.01519107 -0.01187081 -0.045566306\r\n[2,] -0.07791726 0.25741455  0.05710046 -0.11898427  0.033412673\r\n[3,] -0.07167783 0.06323096  0.05420353 -0.09452247  0.005256354\r\n          [,281]      [,282]      [,283]       [,284]     [,285]\r\n[1,] 0.035979938  0.03678068 -0.11209080 -0.139278978 0.04726018\r\n[2,] 0.008121938  0.01946798 -0.04401416 -0.004070202 0.11919580\r\n[3,] 0.105893590 -0.08386994  0.18485342 -0.035839822 0.15128759\r\n          [,286]      [,287]      [,288]     [,289]      [,290]\r\n[1,] -0.02799588  0.05072744 0.004907768 0.07601693  0.05490066\r\n[2,]  0.01514776 -0.29562968 0.102084860 0.02824203  0.01639785\r\n[3,] -0.05246643 -0.30615622 0.019979175 0.07344486 -0.04757974\r\n         [,291]      [,292]     [,293]      [,294]       [,295]\r\n[1,] 0.09618767 0.151678517 -0.1004529 -0.05030722 -0.116703518\r\n[2,] 0.04637577 0.105601251 -0.1033393  0.02358369  0.032865740\r\n[3,] 0.08255721 0.003930302 -0.1791163 -0.02273582  0.008632692\r\n          [,296]     [,297]      [,298]      [,299]      [,300]\r\n[1,] -0.15197571 0.06356946 -0.06262263  0.05438710 -0.05219115\r\n[2,] -0.01940111 0.15565746 -0.02635029  0.07598012  0.16030227\r\n[3,] -0.00429053 0.17145985  0.02258285 -0.01364348  0.22514419\r\n        [,301]     [,302]      [,303]      [,304]      [,305]\r\n[1,] 0.2050680 0.12845989  0.01921978  0.04048699 -0.02587013\r\n[2,] 0.1675498 0.03746923  0.05011429  0.07070537 -0.04895550\r\n[3,] 0.1897785 0.02442981 -0.05904791 -0.02809922 -0.05312953\r\n          [,306]      [,307]      [,308]      [,309]       [,310]\r\n[1,] -0.06091958  0.22622851 -0.02583806 -0.06351952 -0.001065356\r\n[2,] -0.05661405 -0.01088095 -0.09921473 -0.07185391  0.018533990\r\n[3,] -0.10915315  0.04063140 -0.02099844 -0.16146088  0.019671457\r\n           [,311]      [,312]       [,313]     [,314]       [,315]\r\n[1,] -0.032305688 -0.01437410 -0.053030983 0.02671601  0.162066981\r\n[2,]  0.008852237 -0.05506039 -0.041147631 0.19298290 -0.001432929\r\n[3,]  0.015376883  0.01902910  0.006153993 0.14795077  0.060603369\r\n         [,316]     [,317]      [,318]     [,319]      [,320]\r\n[1,] 0.01248342 0.06690809 -0.01680008 0.03164658 -0.01084370\r\n[2,] 0.04505249 0.16234794 -0.07274230 0.06346707 -0.03178494\r\n[3,] 0.12266167 0.14545971  0.04753947 0.06366099  0.05045570\r\n          [,321]       [,322]       [,323]      [,324]      [,325]\r\n[1,]  0.07335888 -0.007874054 -0.016082630 -0.05392395 -0.06698276\r\n[2,]  0.03584130 -0.144745380 -0.001119758  0.03509476 -0.14086261\r\n[3,] -0.07533805 -0.114872098 -0.048632365  0.10942306 -0.05854078\r\n          [,326]       [,327]       [,328]       [,329]     [,330]\r\n[1,] -0.02696791 -0.017976213  0.025982780  0.036848333 0.01237879\r\n[2,] -0.08170927  0.134990618 -0.008862588 -0.003909879 0.06711595\r\n[3,]  0.06490751 -0.005110458  0.154482007  0.064730100 0.08394097\r\n         [,331]    [,332]     [,333]     [,334]       [,335]\r\n[1,] -0.7021178 0.5374565 0.19869882 0.08592588  0.003259264\r\n[2,] -0.1397800 0.7935132 0.06652986 0.18187265  0.019942252\r\n[3,]  0.6625274 0.6826792 0.03761757 0.13857222 -0.048566770\r\n           [,336]      [,337]    [,338]    [,339]     [,340]\r\n[1,] -0.017559689  0.02492253 0.1249315 0.1226960 0.11163367\r\n[2,] -0.005461998  0.03129506 0.1135423 0.0777054 0.03971887\r\n[3,] -0.087679729 -0.03037414 0.1313429 0.2094652 0.15734860\r\n          [,341]      [,342]      [,343]      [,344]      [,345]\r\n[1,] -0.10634105 -0.10901737 -0.06947979  0.05403144 -0.03136061\r\n[2,] -0.03392104 -0.16822711 -0.16724649  0.06844141  0.04961351\r\n[3,] -0.04125428 -0.09978348 -0.26348612 -0.04369469  0.10635111\r\n            [,346]     [,347]      [,348]      [,349]        [,350]\r\n[1,]  0.0352215320 0.02250233 -0.14609206  0.02603752  0.0008976497\r\n[2,] -0.0002137561 0.10198606 -0.16922621 -0.09600603 -0.1366553754\r\n[3,]  0.0350311548 0.03004424 -0.05039074  0.06706302  0.0474682488\r\n          [,351]      [,352]       [,353]      [,354]      [,355]\r\n[1,] -0.08492935  0.05261917 -0.002712078 -0.04173631 -0.08766926\r\n[2,] -0.08518022 -0.09437098  0.089835979 -0.14466380 -0.05234888\r\n[3,] -0.12240537 -0.04706511  0.197074294 -0.19491069 -0.08034088\r\n          [,356]     [,357]       [,358]      [,359]      [,360]\r\n[1,] -0.01748293 0.08140319 -0.009076188  0.03891879 -0.07069510\r\n[2,]  0.06296342 0.15020922 -0.024204466 -0.01938068  0.26828581\r\n[3,] -0.04289609 0.08904836  0.036470607 -0.06100410  0.03539129\r\n         [,361]    [,362]      [,363]      [,364]       [,365]\r\n[1,] 0.04525856 0.3758443 -0.02190541 0.162575215  0.005307974\r\n[2,] 0.09426238 0.1260188 -0.07778172 0.002552769  0.054705024\r\n[3,] 0.15059383 0.1224859  0.02159095 0.076193169 -0.103104755\r\n          [,366]      [,367]      [,368]      [,369]     [,370]\r\n[1,] -0.01882286 -0.04616634 0.129491597  0.05433006 0.06411678\r\n[2,]  0.04845531  0.09284367 0.110095747 -0.04976790 0.08954338\r\n[3,]  0.13817395  0.10027109 0.005097982 -0.05057735 0.07692035\r\n         [,371]      [,372]       [,373]      [,374]      [,375]\r\n[1,] 0.07350555 -0.09912018  0.034260299 -0.20898157 -0.13316488\r\n[2,] 0.11582971  0.04861606 -0.006673804 -0.07216717 -0.04917652\r\n[3,] 0.07229467  0.08957379 -0.070721515 -0.19933733  0.01460028\r\n          [,376]      [,377]      [,378]      [,379]     [,380]\r\n[1,]  0.07324538  0.06339715 0.008160661  0.02553285 0.08604325\r\n[2,] -0.02821313 -0.04864740 0.126741901 -0.04291559 0.11151189\r\n[3,]  0.03416084  0.09956368 0.033908106  0.15106946 0.04633017\r\n          [,381]      [,382]      [,383]      [,384]     [,385]\r\n[1,]  0.01628662 -0.03175199 -0.04446435  0.03977022 0.00684654\r\n[2,]  0.01081612  0.06446750 -0.06379179  0.07895180 0.21498598\r\n[3,] -0.02309628  0.15620320 -0.16393524 -0.02805457 0.10082961\r\n         [,386]      [,387]        [,388]      [,389]       [,390]\r\n[1,] -0.0878603 -0.02482706  0.0805845782  0.26570842  0.002137563\r\n[2,] -0.1599482  0.05167207 -0.0001966804 -0.03986673 -0.032002259\r\n[3,] -0.1875746 -0.06778778  0.0429068431 -0.01857018  0.001538424\r\n           [,391]     [,392]     [,393]      [,394]      [,395]\r\n[1,] -0.008382283 0.04031271  0.1503319 -0.15207227 0.016287033\r\n[2,] -0.099398069 0.08796118 -0.0818783  0.01025959 0.039604645\r\n[3,]  0.005648589 0.01951103 -0.1348186  0.03933498 0.005619545\r\n          [,396]     [,397]      [,398]      [,399]       [,400]\r\n[1,]  0.05844205 0.04118289 -0.17063127  0.11593033  0.003098754\r\n[2,] -0.01318270 0.10266561 -0.17282510  0.13746029 -0.063525043\r\n[3,] -0.05150577 0.08933778  0.06263008 -0.04378749 -0.056211594\r\n         [,401]      [,402]      [,403]      [,404]      [,405]\r\n[1,] 0.05415037 0.011254997 -0.08021654 -0.09940901  0.01778377\r\n[2,] 0.11264771 0.035782840 -0.01895163  0.09955842 -0.05323046\r\n[3,] 0.03951793 0.004913725 -0.03601250  0.09817193 -0.03813793\r\n          [,406]      [,407]      [,408]      [,409]       [,410]\r\n[1,]  0.11230858 -0.09613845 -0.01305507  0.12205683 -0.037554771\r\n[2,] -0.02192646 -0.21035673 -0.03902277 -0.14837089 -0.102020487\r\n[3,]  0.03404685  0.04706251  0.01461852 -0.01263856  0.008175315\r\n          [,411]      [,412]      [,413]      [,414]      [,415]\r\n[1,] -0.04531926  0.03520882  0.11528349  0.05115390 -0.01917591\r\n[2,] -0.04315428  0.08763956  0.01078005  0.05163160 -0.14459552\r\n[3,]  0.01963619 -0.03415635 -0.05583279 -0.04368478 -0.07436517\r\n           [,416]      [,417]      [,418]     [,419]     [,420]\r\n[1,] -0.014584796 0.003684893 -0.05642702 -0.1454906 -0.1043315\r\n[2,] -0.101387501 0.009382152 -0.11377819 -0.2054397 -0.1259652\r\n[3,] -0.003488383 0.001876917 -0.16105342 -0.1437402 -0.1187559\r\n          [,421]      [,422]      [,423]      [,424]    [,425]\r\n[1,] -0.17035030  0.01960437  0.05055789 -0.18750297 0.1078800\r\n[2,] -0.09877777 -0.03259879 -0.06047024 -0.04328201 0.1905506\r\n[3,] -0.12347282 -0.05843275  0.02038978 -0.12298420 0.1448566\r\n          [,426]      [,427]      [,428]     [,429]       [,430]\r\n[1,] -0.01809759  0.07016796 -0.02551789 -0.0282625 -0.126578495\r\n[2,]  0.06140823 -0.01092621  0.05080711 -0.1517675 -0.095688693\r\n[3,]  0.02115142  0.05863923 -0.03656304 -0.1759150 -0.003724739\r\n           [,431]      [,432]      [,433]      [,434]      [,435]\r\n[1,] -0.132467195  0.02622927 -0.03123109 -0.03247021 -0.08306324\r\n[2,]  0.006821066  0.10227088 -0.08569778 -0.03584000 -0.12596449\r\n[3,] -0.044484649 -0.01938494  0.01647643 -0.07612883 -0.11722742\r\n          [,436]       [,437]      [,438]       [,439]      [,440]\r\n[1,]  0.05341662  0.008125714  0.01058675  0.003382702 -0.04252245\r\n[2,] -0.03659999 -0.019274060 -0.02988921 -0.003417473 -0.07552858\r\n[3,] -0.13190494 -0.101536736 -0.01074645  0.069811776 -0.12404119\r\n          [,441]      [,442]     [,443]      [,444]       [,445]\r\n[1,] -0.07372800 -0.05849045 0.03524964  0.16877052  0.068114609\r\n[2,]  0.08032336  0.01231460 0.06118907 -0.01793814  0.005942287\r\n[3,]  0.06018349  0.02405287 0.12663981 -0.21619299 -0.003959647\r\n          [,446]      [,447]     [,448]       [,449]     [,450]\r\n[1,]  0.07226302 -0.28510937 0.02432721 -0.002935223 0.12678717\r\n[2,] -0.04607016  0.03129927 0.04820821 -0.023311311 0.04368496\r\n[3,] -0.02281588 -0.16155379 0.11851458  0.048239492 0.04805805\r\n         [,451]       [,452]       [,453]    [,454]      [,455]\r\n[1,]  0.1290212  0.045280058  0.009119998 -1.274157  0.08619729\r\n[2,]  0.0715922 -0.005440829 -0.144796416 -1.462837  0.01355328\r\n[3,] -0.1276753  0.029364327 -0.193132773 -1.059171 -0.05841826\r\n          [,456]     [,457]      [,458]      [,459]    [,460]\r\n[1,] -0.02242112 0.12967354 -0.01173547 -0.03486820 0.1541892\r\n[2,] -0.01034332 0.05542339  0.10062410 -0.04629513 0.1058829\r\n[3,]  0.07782142 0.05496720  0.05659538 -0.03782411 0.1209047\r\n          [,461]      [,462]     [,463]       [,464]     [,465]\r\n[1,] -0.02452869 -0.04038483 0.06677143 -0.015628178 0.06511339\r\n[2,]  0.01132475 -0.08882064 0.01159206  0.001231097 0.06531174\r\n[3,] -0.05360979 -0.05398347 0.01270098 -0.074746877 0.01321395\r\n         [,466]     [,467]       [,468]      [,469]      [,470]\r\n[1,] -0.1577305 0.03123928  0.070899189 -0.03319991  0.03282479\r\n[2,] -0.1554452 0.01563417 -0.001428645  0.02413461  0.03101723\r\n[3,] -0.1477797 0.02752027  0.007906511  0.10748497 -0.00635145\r\n         [,471]       [,472]       [,473]     [,474]     [,475]\r\n[1,] 0.16071421 -0.125148043 -0.100893088 -0.1231603 0.14092636\r\n[2,] 0.17233622 -0.008775132  0.053332336 -0.2380183 0.12693952\r\n[3,] 0.08292822  0.026071273  0.002210962 -0.1546290 0.04061038\r\n          [,476]     [,477]     [,478]      [,479]     [,480]\r\n[1,] -0.03535345 0.13054915 0.15377441 -0.01675398 0.08626701\r\n[2,] -0.04771874 0.07086749 0.10664982 -0.10120071 0.06549369\r\n[3,]  0.02360129 0.03611998 0.02511051  0.10376409 0.02557622\r\n         [,481]       [,482]     [,483]       [,484]       [,485]\r\n[1,] 0.05674988  0.001078123 0.04609403 -0.026389971 -0.001271373\r\n[2,] 0.02827371 -0.029098878 0.14548145 -0.004448816  0.029100453\r\n[3,] 0.04148384 -0.008389830 0.01104456  0.070395663  0.001577110\r\n          [,486]      [,487]        [,488]     [,489]      [,490]\r\n[1,]  0.03562701 0.049074814 -0.0618827790 0.20455712 -0.04610657\r\n[2,]  0.04760998 0.005309679  0.0760122985 0.05616186 -0.04608857\r\n[3,] -0.07649758 0.091375560  0.0004809797 0.19519871 -0.03994152\r\n         [,491]       [,492]     [,493]     [,494]    [,495]\r\n[1,] 0.07703701  0.007839035 0.10319084 0.03121312 0.1747248\r\n[2,] 0.18174568 -0.029557465 0.09673726 0.11808877 0.1730784\r\n[3,] 0.04589190 -0.154339537 0.18160686 0.08215430 0.1830094\r\n         [,496]     [,497]      [,498]     [,499]       [,500]\r\n[1,] 0.04601399 -0.3952856  0.13922954 0.02999381  0.053702738\r\n[2,] 0.05933195 -0.2772395 -0.01966263 0.33181602 -0.004030934\r\n[3,] 0.10112993 -0.3435352 -0.02424987 0.25223950  0.046554431\r\n          [,501]      [,502]      [,503]      [,504]      [,505]\r\n[1,] -0.02336188 -0.01022422  0.13448614 -0.01043796  0.05471098\r\n[2,] -0.04218457  0.05461446 -0.04669698 -0.02909032  0.03523045\r\n[3,] -0.05676673  0.17867416  0.07421271  0.05624302 -0.01802966\r\n          [,506]      [,507]      [,508]       [,509]      [,510]\r\n[1,]  0.24928488 -0.06591306 -0.05561115 0.0006714305 0.054474905\r\n[2,]  0.11431139 -0.07981958 -0.02691508 0.0866740122 0.005401139\r\n[3,] -0.00323073 -0.05142399 -0.02943678 0.2296749055 0.023786074\r\n           [,511]      [,512]      [,513]      [,514]       [,515]\r\n[1,]  0.002875101 -0.05484245 0.122998923 -0.03010709 -0.030595267\r\n[2,]  0.053738769  0.11779324 0.007506833  0.03718942  0.122838140\r\n[3,] -0.055321712 -0.02580264 0.081575297  0.02921003  0.007531753\r\n          [,516]       [,517]       [,518]      [,519]     [,520]\r\n[1,] -0.12877816 -0.099791571  0.032153402 -0.01484757 0.01290180\r\n[2,]  0.03527739 -0.092365213  0.043330155 -0.18486373 0.00277519\r\n[3,]  0.09122708 -0.001451677 -0.001876002 -0.10912811 0.02595762\r\n           [,521]     [,522]        [,523]      [,524]      [,525]\r\n[1,] -0.013234006 0.07565649 -0.0802963227  0.09339714 -0.06817224\r\n[2,] -0.005343141 0.02226268 -0.0694348067 -0.01199249 -0.05812830\r\n[3,] -0.041063350 0.03402158 -0.0004286381 -0.10582599 -0.04330410\r\n          [,526]       [,527]      [,528]     [,529]      [,530]\r\n[1,]  0.06233599 -0.058382995 -0.01924860 0.04489377  0.06559096\r\n[2,] -0.03440321  0.007027936  0.09098544 0.12303372 -0.10185946\r\n[3,]  0.00528050 -0.018891983  0.02801421 0.09198729 -0.10712181\r\n         [,531]       [,532]     [,533]        [,534]     [,535]\r\n[1,] 0.14161345 -0.174752995 0.06227533  0.0662514269 -0.2177512\r\n[2,] 0.07809762  0.121065632 0.17180447 -0.0007127949 -0.1000329\r\n[3,] 0.26621985 -0.003991501 0.11837573  0.0913059711 -0.1121633\r\n           [,536]       [,537]      [,538]       [,539]       [,540]\r\n[1,] 0.1329743713  0.203391671 0.005818313 -0.024657749  0.100481018\r\n[2,] 0.0004924695  0.032354366 0.092214897  0.002023025 -0.009617069\r\n[3,] 0.1128094494 -0.004464632 0.065579392  0.203502029  0.011754591\r\n          [,541]     [,542]      [,543]       [,544]     [,545]\r\n[1,]  0.06165432 0.06411295 -0.01389637  0.004815212 0.09737331\r\n[2,] -0.31385234 0.07329942 -0.03446046 -0.010144308 0.16229889\r\n[3,] -0.15213887 0.10936663  0.12071300  0.045249429 0.14473276\r\n         [,546]      [,547]      [,548]     [,549]      [,550]\r\n[1,] 0.10238507 -0.04129021 -0.10708246 0.10174325  0.20909365\r\n[2,] 0.07931236  0.14736558 -0.09683255 0.06213352  0.04103754\r\n[3,] 0.08468400  0.13924018 -0.16481984 0.10367008 -0.10729636\r\n            [,551]      [,552]      [,553]     [,554]        [,555]\r\n[1,]  0.0624070317 -0.61854422  0.04605782 0.14104550  0.0380168185\r\n[2,]  0.0001775564 -0.08443949  0.01284193 0.04988941 -0.0070983134\r\n[3,] -0.0473435894 -0.22047752 -0.10745245 0.03003494 -0.0004632226\r\n           [,556]      [,557]     [,558]       [,559]       [,560]\r\n[1,]  0.081258409 -0.02223173 0.14613253 -0.008433553  0.003069871\r\n[2,] -0.005587024  0.06710502 0.04002861 -0.115116470 -0.009286086\r\n[3,] -0.016599791  0.07533991 0.01688313 -0.062957704  0.125670001\r\n           [,561]      [,562]       [,563]     [,564]      [,565]\r\n[1,] -0.077496365  0.07959890 -0.044659276 0.09971232  0.19913223\r\n[2,] -0.064988703 -0.26414856  0.012861216 0.04657340 -0.06462403\r\n[3,] -0.005218811  0.05580022 -0.006904415 0.02663442 -0.01509167\r\n          [,566]      [,567]      [,568]    [,569]     [,570]\r\n[1,] 0.114193216 -0.01769557 -0.05894598 0.0364887 0.06750645\r\n[2,] 0.004687837  0.16266641  0.12134925 0.1508602 0.09792332\r\n[3,] 0.026070904  0.06955954  0.09051076 0.2009770 0.06939902\r\n        [,571]      [,572]     [,573]      [,574]      [,575]\r\n[1,] 0.1354541  0.03108129 -0.1254772  0.09168441 -0.03282591\r\n[2,] 0.3937585  0.03401157 -0.1821007 -0.09378675  0.06853189\r\n[3,] 0.2023968 -0.01192953 -0.1219219  0.04055867  0.08656785\r\n         [,576]     [,577]     [,578]       [,579]      [,580]\r\n[1,] 0.02814430 0.02949820 0.08399966  0.002397518 -0.01157259\r\n[2,] 0.03997610 0.04344099 0.05165761 -0.106884822  0.14118411\r\n[3,] 0.04757469 0.08710670 0.12816823 -0.140741035  0.09758914\r\n          [,581]       [,582]      [,583]     [,584]     [,585]\r\n[1,] -0.02753787  0.001775346 -0.03254072 0.07128654 0.01661594\r\n[2,]  0.04450496 -0.064209178 -0.03106757 0.10829566 0.04823763\r\n[3,] -0.01151269  0.017413810  0.01344837 0.16895512 0.06729863\r\n        [,586]       [,587]       [,588]   [,589]       [,590]\r\n[1,] 0.1588211 -0.057526991  0.164113939 10.90037  0.006117357\r\n[2,] 0.3630488  0.006175421 -0.022398140 11.46694 -0.026343763\r\n[3,] 0.1494202 -0.146993756 -0.004960514 10.94750  0.002871577\r\n          [,591]     [,592]     [,593]      [,594]     [,595]\r\n[1,] -0.03501413 0.09018756 0.09292749  0.06452341 0.09570796\r\n[2,] -0.01308394 0.12172861 0.12719628 -0.04230946 0.01630062\r\n[3,]  0.02746754 0.05904661 0.10432360  0.01748843 0.06194986\r\n         [,596]       [,597]     [,598]      [,599]      [,600]\r\n[1,] -0.1245510  0.126792595 0.08956464  0.04358978  0.06552735\r\n[2,] -0.1412971  0.068248354 0.10850821 -0.05155482 -0.03458041\r\n[3,] -0.1030471 -0.008065663 0.16562979 -0.04389632  0.01321192\r\n          [,601]      [,602]       [,603]       [,604]     [,605]\r\n[1,]  0.07334094 -0.14853717 -0.049167056 -0.025061384 -0.2480717\r\n[2,] -0.10552808 -0.01617761  0.003674608 -0.039574828 -0.2337332\r\n[3,] -0.05090923 -0.18733242  0.033544432 -0.002467056 -0.2309394\r\n          [,606]      [,607]     [,608]      [,609]       [,610]\r\n[1,]  0.05167240 -0.06212182 0.03001972 -0.00619802  0.007160423\r\n[2,] -0.01576734  0.06934274 0.05146546  0.10603949 -0.086451992\r\n[3,] -0.03427850 -0.03544147 0.01841952  0.02314705  0.022775982\r\n          [,611]    [,612]      [,613]     [,614]    [,615]\r\n[1,] -0.05672626 0.4887470 -0.09061193 0.02761512 0.1753153\r\n[2,] -0.03457374 0.3132913 -0.06518495 0.07075723 0.1365166\r\n[3,]  0.05754312 0.2122859 -0.16194762 0.09791545 0.1323497\r\n           [,616]      [,617]     [,618]       [,619]       [,620]\r\n[1,] -0.073208764 -0.04377925 -0.2182470  0.086536877 -0.003400031\r\n[2,] -0.040997539 -0.09075450  0.1293170  0.057046954 -0.022099968\r\n[3,]  0.007723839 -0.08163230  0.1922674 -0.003527197  0.028169820\r\n          [,621]       [,622]    [,623]    [,624]      [,625]\r\n[1,] -0.08855677 -0.039544635 0.2913368 0.1970724  0.13446003\r\n[2,] -0.02975615 -0.002721837 0.1388374 0.1549672 -0.11485711\r\n[3,]  0.02392290 -0.021772724 0.2180016 0.2168985 -0.08745814\r\n          [,626]     [,627]      [,628]      [,629]      [,630]\r\n[1,] 0.025623247 0.02731249  0.09582750 0.165652469  0.09727748\r\n[2,] 0.039845135 0.02735314 -0.03477661 0.004500133 -0.01256608\r\n[3,] 0.003514442 0.01346861  0.08918107 0.032330044 -0.02128103\r\n         [,631]     [,632]       [,633]         [,634]    [,635]\r\n[1,] 0.09574559 0.05824444  0.026450677 0.000007026363 0.1319726\r\n[2,] 0.05082534 0.03188763 -0.044073578 0.045933425426 0.1515071\r\n[3,] 0.05104866 0.02088815 -0.009409678 0.131420865655 0.1152052\r\n         [,636]      [,637]      [,638]       [,639]      [,640]\r\n[1,] 0.11872996  0.06740426 -0.03295327  0.005091864  0.07032682\r\n[2,] 0.06416409  0.03777796  0.02751510  0.095414653 -0.02035147\r\n[3,] 0.17874897 -0.01981001  0.06767552 -0.010958292  0.01296261\r\n         [,641]     [,642]       [,643]     [,644]         [,645]\r\n[1,] 0.06485382 0.10023060 -0.037353795 0.05855997 -0.04702448472\r\n[2,] 0.05736234 0.02852193  0.005037601 0.02233046  0.03281811997\r\n[3,] 0.18294042 0.28169319  0.029283861 0.04694387  0.00003481098\r\n          [,646]      [,647]     [,648]      [,649]      [,650]\r\n[1,] 0.014599899  0.05295621 -0.2318424 -0.03862790  0.06915803\r\n[2,] 0.002245907 -0.07289909 -0.2497116 -0.05421761 -0.05715271\r\n[3,] 0.017213577 -0.04228573 -0.2032288 -0.02221883  0.09396903\r\n           [,651]      [,652]      [,653]      [,654]     [,655]\r\n[1,] -0.008573882  0.13817659  0.11774102 0.007180711 0.01153503\r\n[2,]  0.029099595 -0.07085797 -0.39698678 0.020039564 0.02389313\r\n[3,]  0.048816685  0.11434027  0.09909724 0.011828766 0.06005841\r\n         [,656]     [,657]     [,658]       [,659]       [,660]\r\n[1,] 0.16323003 -0.2821815 0.08282419  0.005385381 -0.080821373\r\n[2,] 0.02120099 -0.1751777 0.01142891 -0.157813326 -0.007341859\r\n[3,] 0.02138077 -0.2424365 0.07629174 -0.090664104 -0.010929093\r\n          [,661]      [,662]      [,663]       [,664]     [,665]\r\n[1,] -0.05973516 -0.05402818  0.05043638 -0.053119019 -0.4382347\r\n[2,] -0.03607860  0.09529447  0.12802537 -0.027307253 -0.1644347\r\n[3,] -0.09080776  0.07575031 -0.03847428 -0.002830364 -0.2158440\r\n           [,666]      [,667]      [,668]      [,669]      [,670]\r\n[1,]  0.127558082 -0.10169803 -0.06219704 -0.02747761 -0.01329563\r\n[2,] -0.009646388  0.07948887 -0.23120938 -0.03643456  0.04320972\r\n[3,] -0.021814600 -0.07878051 -0.23047507  0.03719737  0.03053882\r\n          [,671]      [,672]      [,673]       [,674]      [,675]\r\n[1,]  0.01891686 -0.12061667  0.32487297 -0.005448162 0.006524894\r\n[2,] -0.06934149  0.01744718  0.09623934  0.009026065 0.225343466\r\n[3,]  0.14336102  0.07816807 -0.10652222  0.047874715 0.168353990\r\n        [,676]       [,677]      [,678]       [,679]     [,680]\r\n[1,] 0.0451108  0.124926098 -0.06043627  0.003482156 0.13969089\r\n[2,] 0.1218993 -0.030079177 -0.03301956 -0.047115240 0.10056076\r\n[3,] 0.1264632  0.004649751 -0.02930072  0.053336959 0.02692399\r\n         [,681]       [,682]   [,683]      [,684]     [,685]\r\n[1,] 0.04883071  0.006217855 0.105628 -0.03514947  0.1576692\r\n[2,] 0.15232715 -0.054866929 0.146859 -0.07151469  0.1043521\r\n[3,] 0.20380823 -0.171182945 0.048924 -0.06596526 -0.1094459\r\n          [,686]    [,687]      [,688]     [,689]      [,690]\r\n[1,] -0.13343440 0.1150668  0.04277812 -0.1700584 -0.08152786\r\n[2,] -0.02560283 0.1137769 -0.13468525 -0.0432230  0.09222019\r\n[3,] -0.11482394 0.1435495  0.07105525 -0.0952179 -0.02150658\r\n          [,691]      [,692]       [,693]      [,694]      [,695]\r\n[1,] 0.001460104 -0.02684199  0.159551024  0.01701519 -0.17243104\r\n[2,] 0.076190174  0.04345487  0.122259133  0.04309671 -0.04582918\r\n[3,] 0.076813765  0.07290456 -0.009004088 -0.02147820 -0.05113938\r\n          [,696]      [,697]      [,698]      [,699]      [,700]\r\n[1,] 0.071569681  0.01079237 -0.06372253  0.03304843 -0.02441823\r\n[2,] 0.061498780 -0.07271672  0.08297615 -0.09365286 -0.09535123\r\n[3,] 0.008571736  0.06767070  0.04084178  0.01795634  0.01589342\r\n          [,701]       [,702]      [,703]      [,704]     [,705]\r\n[1,] 0.051343724  0.017095907  0.08447219  0.01810732 0.03884545\r\n[2,] 0.003084684  0.002061225 -0.04824588 -0.06848171 0.02627856\r\n[3,] 0.019820197 -0.030093983 -0.02302835 -0.07308745 0.01341599\r\n          [,706]      [,707]       [,708]      [,709]     [,710]\r\n[1,]  0.02292803 -0.02241533 -0.005699741 -0.14785856 0.06368943\r\n[2,]  0.02831715 -0.02251743  0.113158464 -0.09536365 0.12465273\r\n[3,] -0.02921605 -0.03969850  0.140876904 -0.07692195 0.03233643\r\n          [,711]      [,712]       [,713]      [,714]      [,715]\r\n[1,] -0.03690190 0.093571730 -0.004948105  0.04304661 -0.01409842\r\n[2,] -0.11779904 0.008528029  0.011699425 -0.04191633  0.13354866\r\n[3,] -0.06761794 0.038781248 -0.031239903 -0.03373789  0.05055318\r\n           [,716]      [,717]      [,718]      [,719]     [,720]\r\n[1,] -0.044394851 -0.01228573 0.004501287  0.03997793 0.11182661\r\n[2,] -0.005906425 -0.07271305 0.073141493 -0.12115751 0.06879991\r\n[3,] -0.086893030 -0.07782221 0.092925116 -0.09501198 0.03865187\r\n          [,721]     [,722]     [,723]       [,724]      [,725]\r\n[1,]  0.16200098 -0.1963276 0.07506953  0.108284950  0.06242521\r\n[2,]  0.07843442 -0.0272484 0.09158851 -0.045756642 -0.03038417\r\n[3,] -0.16206633 -0.0492104 0.11310261 -0.001868449 -0.05736248\r\n         [,726]      [,727]       [,728]      [,729]       [,730]\r\n[1,] 0.06534697  0.02324181  0.010514254  0.05806167 -0.002928009\r\n[2,] 0.04052048 -0.23091494 -0.006549641 -0.03614023 -0.163491592\r\n[3,] 0.04237001 -0.14198439  0.101073012  0.03866592 -0.154501140\r\n          [,731]     [,732]       [,733]      [,734]       [,735]\r\n[1,] -0.07826689 -0.4306147  0.034221351 -0.06769373  0.023430072\r\n[2,]  0.01333705 -0.4634736  0.003700569 -0.04821631  0.003874643\r\n[3,]  0.07550879 -0.5461201 -0.050262697 -0.24380949 -0.072438218\r\n         [,736]      [,737]       [,738]        [,739]      [,740]\r\n[1,] -0.1128417 -0.01578417 -0.099718347 -0.0455011204 -0.10483529\r\n[2,]  0.2115270  0.04394335  0.007121652  0.0004530831 -0.08130268\r\n[3,]  0.1422799  0.06445334  0.084635690  0.0257632788 -0.05460403\r\n         [,741]      [,742]     [,743]      [,744]       [,745]\r\n[1,] 0.14446807  0.03796916 0.06176548 -0.02199821  0.007535934\r\n[2,] 0.13209458 -0.11759082 0.11235521  0.03373785 -0.042034440\r\n[3,] 0.09626281 -0.29080799 0.11512416  0.07427175 -0.164961562\r\n        [,746]      [,747]      [,748]      [,749]     [,750]\r\n[1,] 0.1538259 -0.02231905  0.08385108 -0.09496891 -0.3132910\r\n[2,] 0.1086871  0.02552897 -0.07375989 -0.10328012 -0.2329280\r\n[3,] 0.1080883  0.02716528 -0.08429061  0.13082039 -0.1498961\r\n         [,751]      [,752]      [,753]     [,754]      [,755]\r\n[1,] 0.13544297 -0.09229933 0.006370207 0.10247675 -0.01745539\r\n[2,] 0.07169295 -0.11198869 0.277260184 0.07255528  0.01768901\r\n[3,] 0.09499805 -0.12241976 0.367802501 0.06684625  0.10543489\r\n         [,756]      [,757]      [,758]      [,759]     [,760]\r\n[1,] 0.05288321  0.09733097  0.06987759 -0.19151534 0.06136052\r\n[2,] 0.02264082 -0.05838012 -0.06376933 -0.10843939 0.06910763\r\n[3,] 0.22303495 -0.04375997 -0.15094276 -0.06138091 0.03490181\r\n          [,761]      [,762]      [,763]     [,764]    [,765]\r\n[1,] -0.03410539 -0.01715597  0.08264564 0.16676839 0.1246100\r\n[2,] -0.11565023 -0.06192952  0.02285620 0.02905226 0.3959301\r\n[3,]  0.04694730 -0.07533912 -0.08235338 0.10327009 0.2405042\r\n        [,766]      [,767]      [,768]\r\n[1,] 0.1230820 -0.01121444  0.03752019\r\n[2,] 0.1823843  0.05410477 -0.13935684\r\n[3,] 0.1092856  0.06907135 -0.12155339\r\n\r\n3.\r\nGenerating sentence embeddings for the CommonLit Readability\r\ndataset\r\nIn summary, NLP models may provide meaningful contextual numerical\r\nrepresentations of words or sentences. These numerical representations\r\ncan be used as input features for predictive models to predict a\r\nparticular outcome. In our case, we can generate the embeddings for each\r\nreading passage. In the coming weeks, we will use them to predict the\r\ntarget score using various prediction algorithms.\r\nFirst, we will check the length of the reading excerpts in the\r\ndataset.\r\n\r\n\r\nsentence_length <- nchar(readability$excerpt)\r\n\r\nsummary(sentence_length)\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  669.0   886.0   972.0   972.6  1059.0  1343.0 \r\n\r\nThe number of characters ranges from 669 to 1343. In this case, I\r\nwant to use a model that can handle long texts. For instance, RoBERTa\r\nwould ignore any text after the first 512 characters in a reading\r\nexcerpt, and we may lose some vital information regarding the\r\noutcome.\r\nAn alternative model to process longer texts is the Longformer model.\r\nThe Hugging Face page for this model is https://huggingface.co/allenai/longformer-base-4096.\r\nLet’s load this model as we did for RoBERTa.\r\n\r\n\r\nmodel.name <- 'allenai/longformer-base-4096'\r\n\r\nlongformer      <- st$models$Transformer(model.name)\r\npooling_model   <- st$models$Pooling(longformer$get_word_embedding_dimension())\r\nLFmodel         <- st$SentenceTransformer(modules = list(longformer,pooling_model))\r\n\r\n\r\nThis model can handle texts up to 4096 characters, and returns a\r\nvector of length 768.\r\n\r\n\r\nLFmodel$get_max_seq_length()\r\n\r\n[1] 4096\r\n\r\nLFmodel$get_sentence_embedding_dimension()\r\n\r\n[1] 768\r\n\r\nNow, we can submit the reading excerpts from our dataset all at once,\r\nand get the sentence embeddings for each one. Since our dataset has 2834\r\nobservations, this should return a matrix with 2834 rows and 768\r\ncolumns.\r\n\r\n\r\nread.embeddings <- LFmodel$encode(readability$excerpt,\r\n                                  show_progress_bar = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# Check the embedding matrix\r\n\r\ndim(read.embeddings)\r\n\r\n[1] 2834  768\r\n\r\nhead(read.embeddings)\r\n\r\n           [,1]        [,2]          [,3]        [,4]       [,5]\r\n[1,] 0.06878642  0.02737294  0.0353682041 -0.17769572 0.17525391\r\n[2,] 0.06200736  0.18268286  0.0024652195 -0.10744876 0.51925117\r\n[3,] 0.02648670  0.09453232  0.0296205040 -0.06081667 0.25797406\r\n[4,] 0.04218233  0.03943034 -0.0004737412 -0.22149815 0.16781905\r\n[5,] 0.01741268  0.11062079  0.0012940745 -0.30953035 0.27908468\r\n[6,] 0.03088607 -0.01201622  0.1024196073 -0.04266621 0.08677045\r\n             [,6]         [,7]        [,8]        [,9]        [,10]\r\n[1,] -0.062003829 -0.009241561  0.18663073 -0.08267820  0.050396595\r\n[2,] -0.009631317  0.021055585  0.10391098 -0.12528235  0.120555066\r\n[3,] -0.026006516  0.010462754  0.07656582 -0.05405279  0.043316741\r\n[4,] -0.130812421 -0.001381062  0.17187740  0.02726705  0.100796096\r\n[5,]  0.201777592 -0.169042930  0.17893136  0.07310306 -0.053442664\r\n[6,] -0.099492773 -0.007720584 -0.03680542 -0.08005710 -0.002916175\r\n           [,11]       [,12]      [,13]       [,14]      [,15]\r\n[1,] -0.08924759 -0.12584847 0.07532627 -0.09043550 0.16977578\r\n[2,] -0.12325408 -0.03490974 0.05919705 -0.07151967 0.06679756\r\n[3,] -0.12445029  0.01071901 0.05318572  0.01152111 0.06358113\r\n[4,] -0.01702231 -0.05677414 0.08501364  0.08951008 0.02121747\r\n[5,] -0.08641744 -0.05077145 0.16863522  0.06391545 0.09241463\r\n[6,] -0.10258079 -0.08562840 0.05569165  0.10128744 0.10752373\r\n          [,16]     [,17]        [,18]       [,19]       [,20]\r\n[1,] 0.06627524 0.1254534  0.149795040 -0.09539226 -0.05386250\r\n[2,] 0.06315146 0.1181628 -0.172043473 -0.10171498  0.08964073\r\n[3,] 0.08248881 0.1091467 -0.076039344 -0.12442163  0.13084817\r\n[4,] 0.21985857 0.2758147  0.066664048 -0.01095178  0.11738628\r\n[5,] 0.06473929 0.1145013  0.026287500 -0.16021247 -0.03594755\r\n[6,] 0.03598172 0.3445779  0.008862211 -0.03162296  0.06223473\r\n            [,21]      [,22]        [,23]       [,24]         [,25]\r\n[1,]  0.019800426 0.05689066 -0.066817492 -0.01374022 -0.0005434537\r\n[2,] -0.033529568 0.09369566  0.005841169 -0.12309865 -0.0396644436\r\n[3,] -0.003348924 0.08551493  0.096154064 -0.04318059 -0.0134544317\r\n[4,]  0.047499601 0.01420696 -0.059058707 -0.06542822  0.0618643127\r\n[5,]  0.011399466 0.15386474 -0.157574072  0.12298665 -0.0167587493\r\n[6,]  0.068682052 0.07682541  0.009790933 -0.09392697  0.0859657153\r\n             [,26]      [,27]     [,28]       [,29]      [,30]\r\n[1,] -0.0398280062 0.06324669 0.1239742  0.05714065 -0.0784037\r\n[2,]  0.0007338378 0.16816032 0.1769400  0.14561471 -0.1236172\r\n[3,]  0.0233391654 0.09676986 0.1174871  0.16839454 -0.1228397\r\n[4,]  0.0026793315 0.05301249 0.1669814  0.03192727 -0.2018328\r\n[5,]  0.0843952000 0.24310870 0.2146871  0.04003200 -0.1260741\r\n[6,] -0.0813994482 0.17552009 0.1065100 -0.05466254 -0.1454486\r\n           [,31]        [,32]        [,33]      [,34]       [,35]\r\n[1,] -0.05841080  0.009110089  0.078652218 0.10221330 -0.10459986\r\n[2,] -0.04242602  0.069912344  0.057975896 0.03614431 -0.18198293\r\n[3,] -0.05578281  0.075226575  0.023002410 0.04269278 -0.05390055\r\n[4,] -0.00404502 -0.018618550  0.001835824 0.06220116 -0.03155796\r\n[5,] -0.20792694 -0.103156604 -0.026916955 0.07625656  0.05396373\r\n[6,] -0.07710957  0.064506762  0.057588067 0.05781943  0.01271113\r\n           [,36]        [,37]       [,38]     [,39]       [,40]\r\n[1,]  0.02391223  0.110306241 0.043328114 0.4521784  0.03735125\r\n[2,]  0.04250664 -0.009082338 0.007062219 0.4343244  0.06531474\r\n[3,]  0.01181644  0.005166513 0.011582398 0.3327737  0.09386559\r\n[4,] -0.05737976  0.074416302 0.015505768 0.5607200 -0.03983847\r\n[5,] -0.01515435  0.076285161 0.131026924 0.4974277  0.03435208\r\n[6,] -0.01635659  0.006289245 0.023226904 0.3060058  0.05415952\r\n           [,41]      [,42]       [,43]        [,44]       [,45]\r\n[1,]  0.01160207 -0.1441056 -0.06634481  0.013188138 -0.03034872\r\n[2,] -0.09629171 -0.2475861 -0.06511805  0.029367110 -0.02183991\r\n[3,] -0.08046697 -0.1241342 -0.07358038  0.041240882 -0.01687782\r\n[4,]  0.02450111 -0.1278276 -0.04889721 -0.058667518 -0.04264880\r\n[5,] -0.04384707 -0.1287246 -0.06949089  0.075815417 -0.04147740\r\n[6,] -0.03843844 -0.2790852 -0.07422113  0.003265372  0.01405668\r\n           [,46]       [,47]       [,48]         [,49]       [,50]\r\n[1,]  0.05563493 -0.12030786  0.17755996 -0.0009431239  0.02273739\r\n[2,] -0.00349074 -0.25071853  0.11311000  0.0546880662  0.04223915\r\n[3,] -0.03969015 -0.16227439  0.17778057  0.0135517670 -0.02265750\r\n[4,]  0.01296046 -0.01586921  0.20198743 -0.0287195258 -0.02992338\r\n[5,]  0.10140187 -0.21261519 -0.04230785 -0.0213186536  0.01532555\r\n[6,]  0.05705843 -0.21435523  0.14221516  0.0081882812  0.06091512\r\n            [,51]       [,52]         [,53]       [,54]       [,55]\r\n[1,] -0.055851076 0.045075454  0.0457919426  0.01860519 -0.04628744\r\n[2,]  0.028229486 0.145248696  0.0022057290  0.02396804 -0.08741326\r\n[3,]  0.013225293 0.138259456  0.0169651713  0.01428844 -0.09812585\r\n[4,]  0.009362207 0.009168806 -0.0189137459 -0.07263523 -0.05096374\r\n[5,] -0.054614712 0.016204031  0.0061255246  0.01164986 -0.12076854\r\n[6,] -0.046974260 0.004813397  0.0006372099 -0.02829496 -0.07158546\r\n          [,56]      [,57]       [,58]      [,59]       [,60]\r\n[1,] 0.11621416 0.19029057 -0.18323594 0.15596573 -0.16865835\r\n[2,] 0.15194535 0.07216028  0.19703192 0.03920581 -0.05542378\r\n[3,] 0.05171975 0.07081036  0.16018121 0.07730552 -0.12761781\r\n[4,] 0.06082216 0.14776467  0.06345271 0.16414967 -0.11517348\r\n[5,] 0.06762488 0.09770873 -0.06482673 0.05881982 -0.18440580\r\n[6,] 0.18844305 0.15123229  0.03043088 0.07150017 -0.06692398\r\n           [,61]       [,62]       [,63]      [,64]        [,65]\r\n[1,] -0.01584120 -0.30836877 -0.08627234 0.05220238 -0.073660202\r\n[2,]  0.09358547 -0.12590311 -0.01988886 0.15112427 -0.057417575\r\n[3,]  0.00353132 -0.02997617 -0.06488663 0.10850967  0.000176775\r\n[4,]  0.07951759  0.01796308 -0.08190001 0.09804805 -0.032816716\r\n[5,]  0.01957706  0.39047232 -0.16133040 0.22548598 -0.071758203\r\n[6,]  0.05250159 -0.21743841 -0.03778880 0.11767841 -0.063424781\r\n           [,66]      [,67]       [,68]        [,69]       [,70]\r\n[1,] -0.09121702 0.06205500 -0.00664423  0.044466395  0.10872079\r\n[2,] -0.06620028 0.02835573 -0.07774094  0.025311865  0.10251271\r\n[3,] -0.08376483 0.08219867 -0.15987641  0.002131245  0.04943869\r\n[4,] -0.10609961 0.01014323 -0.20266037  0.042984761  0.04320575\r\n[5,] -0.12487545 0.05126919 -0.21430783 -0.032729916 -0.02177408\r\n[6,] -0.12166830 0.08317646 -0.05596460  0.031021968  0.12048520\r\n          [,71]      [,72]      [,73]       [,74]         [,75]\r\n[1,] 0.05152616 -0.1286170 0.07124258 -0.13667069  0.0436399579\r\n[2,] 0.11669674 -0.1081983 0.02155845  0.05324470  0.0256285314\r\n[3,] 0.07750056 -0.1166162 0.03136687  0.08986921 -0.0002351542\r\n[4,] 0.17135516 -0.1582801 0.02830557 -0.13921450  0.0152892377\r\n[5,] 0.02453504 -0.2918184 0.01494434 -0.25516808  0.1191712022\r\n[6,] 0.08055479 -0.1039434 0.03267877  0.08227553 -0.0112367412\r\n           [,76]       [,77]     [,78]       [,79]        [,80]\r\n[1,]  0.09222718 -0.02197541 -5.946264  0.14185232 -0.002517788\r\n[2,]  0.02680033  0.01777905 -5.769702  0.13710310 -0.039606657\r\n[3,]  0.12901822 -0.04005376 -5.613939  0.19400115 -0.007314286\r\n[4,] -0.03612557  0.07990569 -5.738643  0.11591304 -0.004460549\r\n[5,]  0.00174457 -0.01451725 -6.881078 -0.05151975 -0.024183193\r\n[6,]  0.03638640 -0.05377566 -6.141560  0.12704419 -0.014760924\r\n            [,81]      [,82]    [,83]       [,84]        [,85]\r\n[1,]  0.041527856 0.09985719 1.121162  0.03830210 -0.063193858\r\n[2,] -0.011150517 0.01501055 1.177877  0.01126669 -0.113686204\r\n[3,] -0.007199855 0.02349413 1.150699  0.02726431 -0.095392525\r\n[4,]  0.067316115 0.04642459 0.721678 -0.19776239  0.007204633\r\n[5,]  0.026808724 0.08641411 1.250171  0.10575949 -0.082243785\r\n[6,] -0.017409449 0.05579916 1.169361  0.05016227 -0.159839779\r\n           [,86]      [,87]       [,88]       [,89]        [,90]\r\n[1,] -0.26729175 0.07502843  0.13967815 -0.02483355  0.007648359\r\n[2,] -0.03492378 0.01044415 -0.01767602  0.02817056 -0.012143429\r\n[3,] -0.19305614 0.04591532  0.11847871  0.06890325  0.066408977\r\n[4,] -0.24397978 0.12207813 -0.01864471 -0.09496449  0.026372869\r\n[5,] -0.08974808 0.03191085  0.03928379  0.08986512  0.036878455\r\n[6,] -0.11673717 0.01388938  0.02112357 -0.02177202  0.040249281\r\n            [,91]      [,92]       [,93]     [,94]        [,95]\r\n[1,]  0.085554458 0.07065610 -0.02443473 0.1472436  0.053917125\r\n[2,]  0.005007802 0.07478565  0.01366426 0.1178765  0.090566173\r\n[3,] -0.082261726 0.09630763  0.06820065 0.1347773 -0.003685075\r\n[4,]  0.048539050 0.10452553 -0.03157165 0.1828313  0.032530654\r\n[5,] -0.062374547 0.08127025 -0.05123742 0.3114559  0.083208121\r\n[6,]  0.050125770 0.05533622  0.02127386 0.1706575  0.015914422\r\n          [,96]        [,97]       [,98]       [,99]      [,100]\r\n[1,] 0.20212734 -0.116203837  0.23486662 -0.02284844 -0.04338972\r\n[2,] 0.06854723 -0.059805643  0.74454349  0.03837124  0.01315809\r\n[3,] 0.16587275 -0.040862981  0.60500520  0.01118777 -0.08465243\r\n[4,] 0.21602593 -0.042178612 -0.03365597 -0.01383332  0.02952261\r\n[5,] 0.10416410 -0.110708982  0.45391500  0.07056663 -0.02640560\r\n[6,] 0.05282875 -0.001121965  0.56108010  0.01365058 -0.03908147\r\n          [,101]       [,102]     [,103]     [,104]      [,105]\r\n[1,]  0.23872551  0.020933937 -0.2689151 0.15147276 -0.07147629\r\n[2,]  0.15307951 -0.001722996 -0.2714304 0.03367482 -0.06458593\r\n[3,] -0.04544293 -0.019447301 -0.1242545 0.19996431 -0.08332866\r\n[4,]  0.09374914 -0.014098025 -0.2161348 0.16988960 -0.01689096\r\n[5,]  0.46665555  0.075723723 -0.0992410 0.27997312 -0.11320364\r\n[6,]  0.11137514  0.028680129 -0.2897870 0.04343583 -0.02774085\r\n          [,106]       [,107]      [,108]      [,109]     [,110]\r\n[1,] -0.10091496  0.007719000  0.14613576 -0.06393286 0.08753217\r\n[2,] -0.09945138 -0.130645871 -0.04965333 -0.03176723 0.06711566\r\n[3,] -0.06551234 -0.116697073  0.05235539 -0.05145445 0.02444218\r\n[4,] -0.17506407  0.036558561  0.01593233 -0.06472807 0.13053261\r\n[5,] -0.06535351 -0.009498350  0.08395107 -0.05001440 0.04134391\r\n[6,] -0.14649561 -0.008850493  0.06866036 -0.06751941 0.05398743\r\n         [,111]      [,112]      [,113]      [,114]      [,115]\r\n[1,] 0.05948964  0.19408694 0.125716850  0.01159422 -0.07214574\r\n[2,] 0.16257027 -0.05577020 0.044497672 -0.01349812 -0.07794671\r\n[3,] 0.12493593  0.12674727 0.039884962 -0.02161937 -0.06567856\r\n[4,] 0.02918692  0.12518449 0.006082227  0.06100188 -0.03282348\r\n[5,] 0.09491364 -0.09426759 0.193379387  0.08677589 -0.13170332\r\n[6,] 0.08426497  0.24839157 0.110648654  0.05496157 -0.13149224\r\n          [,116]      [,117]       [,118]       [,119]      [,120]\r\n[1,] -0.04729865 -0.01458118  0.007416536  0.015140262 -0.01364085\r\n[2,]  0.09857436  0.05374681 -0.029883321 -0.006696742  0.11462778\r\n[3,]  0.08839864 -0.01325786 -0.036843956  0.028005935 -0.02517148\r\n[4,] -0.03490236  0.01637201 -0.026882786  0.002716305 -0.02008895\r\n[5,]  0.01232459 -0.03626100  0.051883530 -0.022906721 -0.08221104\r\n[6,] -0.06458402 -0.02047255 -0.006263990  0.064767666 -0.14237389\r\n           [,121]     [,122]      [,123]      [,124]      [,125]\r\n[1,] -0.029898381 0.13351487  0.03838781  0.12505446 0.277584553\r\n[2,]  0.042145960 0.02854462 -0.03316170 -0.11165556 0.011946644\r\n[3,]  0.052602947 0.06012842 -0.02576030  0.04977613 0.038049541\r\n[4,] -0.032401364 0.11898023  0.11506625  0.04655128 0.011379799\r\n[5,]  0.003823568 0.10826161  0.10774323  0.18962641 0.001085924\r\n[6,] -0.001228545 0.12465001  0.02570309  0.06945270 0.124052927\r\n          [,126]       [,127]      [,128]       [,129]       [,130]\r\n[1,]  0.16553748 -0.045734782 -0.02095836 -0.028291635 -0.031091623\r\n[2,]  0.07167397  0.007441618 -0.15562278 -0.063137099 -0.016863035\r\n[3,]  0.03975507 -0.010778110 -0.09103153  0.004879553 -0.007059486\r\n[4,]  0.05562349 -0.062935844  0.02984938 -0.104437672 -0.044301443\r\n[5,]  0.06157386  0.007056880 -0.01511185 -0.088381991  0.057884976\r\n[6,] -0.03435799 -0.035998534 -0.05777951 -0.017548744 -0.040689226\r\n           [,131]      [,132]       [,133]      [,134]       [,135]\r\n[1,] -0.068024397 -0.19685575 -0.057357501 -0.02150770  0.008344368\r\n[2,] -0.127200842 -0.16465619 -0.030997569 -0.11230204 -0.059650008\r\n[3,] -0.081564896 -0.02827076  0.002269247 -0.10395408  0.009244090\r\n[4,] -0.107820205  0.22287221 -0.001476477 -0.06372286  0.015938897\r\n[5,] -0.008750514 -0.12222122 -0.003253136  0.02914541  0.068772882\r\n[6,] -0.103431627 -0.23775043  0.019290864 -0.02134318 -0.001622586\r\n           [,136]      [,137]        [,138]      [,139]      [,140]\r\n[1,] -0.102784194 -0.13495629  0.0142317209  0.02313250 -0.02105902\r\n[2,] -0.075864702 -0.03264857  0.0236496422 -0.06827071  0.10808191\r\n[3,]  0.001411123 -0.03584372 -0.0004037199 -0.05033645  0.08863413\r\n[4,] -0.063902400 -0.11118698  0.0589230135  0.06679199  0.02210390\r\n[5,] -0.093556628 -0.22081396  0.0001657894  0.01393972  0.12405545\r\n[6,] -0.020176252 -0.13743046  0.0230124258  0.03991869  0.04353123\r\n           [,141]       [,142]      [,143]       [,144]      [,145]\r\n[1,]  0.088557333  0.040845133 -0.02909195 -0.024868179 -0.10013279\r\n[2,]  0.009596834  0.078423120 -0.08343355  0.081874341 -0.13825487\r\n[3,]  0.097992301  0.080675490  0.03624254  0.096607886 -0.05899038\r\n[4,]  0.032477561  0.008287883  0.01552685 -0.093810037 -0.12003399\r\n[5,] -0.045573782 -0.080180943  0.09660947 -0.003410187 -0.07613166\r\n[6,]  0.052745692  0.069129847 -0.03849388 -0.013348532  0.05511277\r\n          [,146]       [,147]      [,148]      [,149]     [,150]\r\n[1,]  0.01643182  0.040502507 -0.05167877  0.02774313 0.09360757\r\n[2,]  0.04458265 -0.047292855  0.02723607  0.03013752 0.16895996\r\n[3,]  0.01492312  0.006588188 -0.04234581  0.03453827 0.03027742\r\n[4,] -0.06132180  0.020900434 -0.03471314  0.01671150 0.07657420\r\n[5,]  0.08212277 -0.040125243 -0.06929307 -0.00432986 0.05462028\r\n[6,] -0.04039430  0.055167176 -0.15788952  0.02860752 0.15503193\r\n           [,151]       [,152]      [,153]       [,154]       [,155]\r\n[1,] -0.073861621  0.006610246 -0.07322597  0.004320241  0.079006694\r\n[2,] -0.002897343 -0.032812975 -0.05331478  0.036128804  0.098519921\r\n[3,] -0.061893009 -0.046659250 -0.06112369  0.034307890 -0.002737739\r\n[4,] -0.004555190 -0.061745219 -0.05912172  0.062221162 -0.084972933\r\n[5,] -0.023094578 -0.040132854  0.03313675 -0.009959953  0.055616483\r\n[6,] -0.080483474 -0.080875732 -0.03731335  0.051242720 -0.007341798\r\n        [,156]       [,157]    [,158]      [,159]    [,160]    [,161]\r\n[1,] 0.1483632  0.008235849 0.1733116 -0.01954455 0.3793607 0.1087773\r\n[2,] 0.2014366  0.067571804 0.3155718 -0.10236399 0.2850931 0.1659556\r\n[3,] 0.1542511  0.003424891 0.2529557  0.03088392 0.2083622 0.1295714\r\n[4,] 0.2362703 -0.027450442 0.2345958 -0.11995528 0.4174938 0.1058028\r\n[5,] 0.1877714  0.063925140 0.2155312  0.08399600 0.5725406 0.1434436\r\n[6,] 0.1657421  0.029531976 0.5287693 -0.01082900 0.2701406 0.1110887\r\n          [,162]     [,163]      [,164]       [,165]     [,166]\r\n[1,]  0.02382109 0.03934348 -0.07816513 -0.063273966 0.08881398\r\n[2,]  0.05152238 0.05214351 -0.12706640 -0.132280082 0.16892013\r\n[3,] -0.01791148 0.02596789 -0.12161267 -0.018316647 0.05279658\r\n[4,] -0.09078024 0.06221739 -0.04423966  0.020219367 0.07977139\r\n[5,]  0.01971423 0.07503922 -0.16571607 -0.160742879 0.23236096\r\n[6,]  0.07012735 0.01299729 -0.14214301 -0.008776547 0.07046251\r\n          [,167]      [,168]       [,169]       [,170]      [,171]\r\n[1,]  0.11146063 -0.02742520  0.010782450  0.004686479 -0.04409403\r\n[2,] -0.03822517  0.10706264 -0.017904045  0.010008985  0.04308533\r\n[3,]  0.03960572  0.01006290  0.037345715 -0.020681502  0.02676369\r\n[4,]  0.02204281  0.04992779  0.024536621  0.065638922 -0.06172321\r\n[5,] -0.04216772 -0.05271583  0.015409810  0.044527601 -0.03895360\r\n[6,]  0.15301870  0.09301525 -0.002569517  0.079733737  0.11045213\r\n           [,172]       [,173]       [,174]       [,175]      [,176]\r\n[1,] -0.030827977 -0.016552867 -0.037396442 -0.110546499  0.02091328\r\n[2,] -0.118163362 -0.101622179  0.041260589  0.065143712 -0.00664208\r\n[3,] -0.133316681 -0.062520601  0.000882378 -0.001258559 -0.04469740\r\n[4,] -0.108924128 -0.029740350  0.044388141  0.034460146 -0.03467989\r\n[5,]  0.009975387  0.009619499  0.066007525 -0.016155105  0.03704870\r\n[6,] -0.122878514 -0.031314857  0.041082900 -0.014921487  0.04582291\r\n          [,177]     [,178]      [,179]       [,180]      [,181]\r\n[1,]  0.16406690 0.11637163 0.102757700 -0.065808818 -0.03085301\r\n[2,]  0.23942780 0.12110980 0.192517295 -0.008136311 -0.11009263\r\n[3,]  0.16348210 0.11496253 0.116396785 -0.053040501 -0.05610459\r\n[4,] -0.01567582 0.13300121 0.143559024 -0.046972897 -0.06362554\r\n[5,]  0.17504741 0.18193628 0.006118873  0.022498267  0.17850330\r\n[6,]  0.21360956 0.02295497 0.114575908 -0.123433337 -0.03238666\r\n           [,182]     [,183]       [,184]        [,185]      [,186]\r\n[1,]  0.047487181 0.08365419 -0.006170089 -0.0099381404  0.02653007\r\n[2,]  0.047892727 0.05988264 -0.101161323 -0.0003986875  0.08727922\r\n[3,]  0.022679541 0.05163966  0.004005735  0.0242591817  0.08526003\r\n[4,] -0.005437774 0.05755933 -0.108031176 -0.0605965853 -0.06346862\r\n[5,]  0.024581207 0.04812210 -0.115264393 -0.0225114767  0.05047017\r\n[6,]  0.102973312 0.10902948 -0.045517776  0.0117747318 -0.03072424\r\n         [,187]      [,188]      [,189]       [,190]       [,191]\r\n[1,] 0.15526831  0.07208288 -0.14248841  0.014994285 -0.011817173\r\n[2,] 0.04445880 -0.02742394 -0.04661029  0.185127467  0.004467144\r\n[3,] 0.07624952 -0.05811815 -0.06095228  0.085832350  0.029420679\r\n[4,] 0.11060012  0.09253567 -0.11716639  0.112309471 -0.029765543\r\n[5,] 0.14934652  0.03446966 -0.14601848  0.114642330 -0.031648837\r\n[6,] 0.09348606 -0.02047507 -0.09198567 -0.005882796  0.044917762\r\n        [,192]      [,193]    [,194]       [,195]       [,196]\r\n[1,] 0.2376663  0.06471327 0.2869302  0.006595411 -0.002131236\r\n[2,] 0.1286241  0.01402647 0.2060701  0.057511937  0.080612928\r\n[3,] 0.1429217 -0.03414715 0.2847818  0.063547119  0.014086470\r\n[4,] 0.1777786  0.06618749 0.2010641 -0.062344369  0.014133361\r\n[5,] 0.2387890 -0.03502944 0.4510851  0.118920438  0.100134738\r\n[6,] 0.2079677 -0.00836664 0.2252501  0.042308558 -0.035618603\r\n           [,197]      [,198]      [,199]      [,200]      [,201]\r\n[1,]  0.111284904  0.06065015  0.14765435 -0.09968079  0.05590762\r\n[2,] -0.006094806 -0.04312481  0.07411973 -0.02219011  0.09446417\r\n[3,] -0.026928667  0.03070987  0.10312929 -0.01123741  0.08575176\r\n[4,]  0.064696468 -0.03691681  0.18689941 -0.02606682 -0.06948781\r\n[5,]  0.116514213 -0.03425939 -0.10715778 -0.14326958 -0.04532622\r\n[6,]  0.027877506  0.02917253  0.02128681 -0.02145256  0.09128784\r\n           [,202]      [,203]      [,204]      [,205]       [,206]\r\n[1,]  0.146735489  0.11358202 -0.08193325  0.25428578  0.065701984\r\n[2,]  0.027812298  0.08971079 -0.02867646  0.00942719 -0.054258276\r\n[3,] -0.046032969  0.02980055  0.01649122  0.17686845 -0.050942793\r\n[4,]  0.123839982  0.02067737 -0.07756262  0.21948744  0.007775099\r\n[5,] -0.008841063 -0.07333382 -0.08066885  0.35297471  0.064149112\r\n[6,] -0.053674281 -0.09016324  0.03218077 -0.02127722  0.046199832\r\n         [,207]       [,208]      [,209]      [,210]       [,211]\r\n[1,] 0.01931344  0.055372115  0.01453709  0.02544891 -0.073285304\r\n[2,] 0.06116709  0.009637616  0.03871655 -0.01346140  0.003739743\r\n[3,] 0.12515076 -0.015982902  0.04348699  0.02505557 -0.025655473\r\n[4,] 0.19800356  0.078355394 -0.02394519  0.01757303 -0.092073761\r\n[5,] 0.18612207  0.150131032  0.03098740 -0.03294157 -0.097650550\r\n[6,] 0.09481613  0.032243874  0.01760928  0.12533936 -0.021152314\r\n         [,212]      [,213]      [,214]       [,215]        [,216]\r\n[1,] 0.07963207 0.054734237 -0.01597461 -0.054342546 -0.0002604671\r\n[2,] 0.17093231 0.108724132  0.22238894  0.014393425  0.0134764072\r\n[3,] 0.15664001 0.028745873  0.18153188 -0.102613285  0.0280863140\r\n[4,] 0.05071726 0.009211972  0.05590099 -0.109988436 -0.0181687865\r\n[5,] 0.12925413 0.067303337  0.27411485 -0.153481498  0.0342940465\r\n[6,] 0.14717685 0.034810364  0.18813257  0.009363505  0.0130624538\r\n           [,217]      [,218]      [,219]     [,220]      [,221]\r\n[1,] -0.058852341  0.12951709 -0.11006151 0.13824514 0.072320662\r\n[2,]  0.044101015  0.01253397 -0.14474536 0.04638780 0.053958572\r\n[3,]  0.056321442 -0.05187616 -0.03446175 0.16116869 0.083157308\r\n[4,] -0.007440881 -0.08260800  0.10185594 0.04197086 0.116478071\r\n[5,] -0.066293940 -0.26113343 -0.01156739 0.31020588 0.092619948\r\n[6,]  0.075965993 -0.09815512 -0.08419202 0.27215043 0.001470058\r\n          [,222]      [,223]       [,224]      [,225]      [,226]\r\n[1,]  0.07041030  0.03555779  0.072285376  0.05357500 -0.07106956\r\n[2,] -0.01374738  0.17003758 -0.019520387  0.03457224 -0.03713996\r\n[3,]  0.04476531  0.13863727  0.004889852  0.10775344 -0.10282592\r\n[4,] -0.02810579  0.01348006  0.009996367 -0.02153545 -0.11958762\r\n[5,] -0.02968163 -0.01568093  0.022583006 -0.11505753  0.06802285\r\n[6,]  0.04070271  0.14868467  0.062011078  0.05736085 -0.13058169\r\n         [,227]      [,228]      [,229]      [,230]     [,231]\r\n[1,] 0.16905032  0.03836681 -0.09137645  0.09602909 0.04561829\r\n[2,] 0.06799433  0.05174431 -0.10136008 -0.01144294 0.05436248\r\n[3,] 0.10437881  0.01218759 -0.05382705 -0.04475870 0.02737739\r\n[4,] 0.20471486 -0.01401014 -0.14690535  0.07012411 0.07716142\r\n[5,] 0.13378915 -0.01853155 -0.16943839  0.16641875 0.03436823\r\n[6,] 0.13722783  0.03344594 -0.04833374  0.03120663 0.04564085\r\n          [,232]      [,233]       [,234]    [,235]      [,236]\r\n[1,] -0.04100875 -0.09729417 -0.015945243 0.2084298 -0.32908994\r\n[2,] -0.09437405 -0.09850014 -0.015326987 0.1066173  0.06720883\r\n[3,] -0.08152922 -0.04496773 -0.020547934 0.1454203 -0.18244785\r\n[4,]  0.04185037  0.01151387 -0.006661795 0.1374446 -0.41618568\r\n[5,] -0.01318259 -0.05339691 -0.204763591 0.1586502 -0.31462738\r\n[6,] -0.02640618 -0.05233365  0.049708031 0.1770238 -0.44506806\r\n           [,237]     [,238]     [,239]        [,240]     [,241]\r\n[1,] -0.057311643 0.05173610 0.12793347  0.0748575628 -0.4823146\r\n[2,] -0.113685295 0.12084391 0.06927693 -0.0003902306 -0.6902798\r\n[3,] -0.088810235 0.06109840 0.03226926  0.0239857566 -0.4093242\r\n[4,]  0.004091115 0.13506255 0.08105611  0.0400430895 -0.5522546\r\n[5,] -0.093278319 0.09151746 0.08942989  0.0366468728 -0.6477040\r\n[6,] -0.049687769 0.08295970 0.03150887 -0.0254437830 -0.3423410\r\n           [,242]     [,243]     [,244]       [,245]      [,246]\r\n[1,] 0.0491617545 0.08810919 0.16544563 -0.029332409  0.04919896\r\n[2,] 0.0671593398 0.12259185 0.20401637  0.018133275  0.03448596\r\n[3,] 0.0335012637 0.10995636 0.08732561 -0.021502355  0.03198661\r\n[4,] 0.0002215922 0.12138499 0.14508900 -0.086681329  0.12600115\r\n[5,] 0.0538666286 0.03495837 0.30469996 -0.012076542 -0.01498211\r\n[6,] 0.0276439749 0.11181737 0.15175976 -0.001726018  0.13548543\r\n          [,247]      [,248]    [,249]      [,250]      [,251]\r\n[1,]  0.04697078 -0.04890043 0.1589750 -0.03094804  0.02603056\r\n[2,]  0.19406812  0.14153977 0.1780439 -0.07918438  0.01485757\r\n[3,]  0.05676338  0.05711843 0.1806150  0.02305853 -0.02673498\r\n[4,] -0.04791526  0.09965809 0.1092957 -0.04397365 -0.01028533\r\n[5,] -0.01059569  0.19260283 0.2623497  0.08115722 -0.01139705\r\n[6,]  0.04929274  0.11131403 0.1806237 -0.02468466 -0.02996133\r\n         [,252]       [,253]      [,254]      [,255]      [,256]\r\n[1,] -0.1724897 -0.002401687  0.01534870  0.13890797 -0.11863050\r\n[2,] -0.1395971  0.031402815  0.06491155 -0.02091154 -0.16273288\r\n[3,] -0.1389630  0.088268481 -0.01522258 -0.01420320 -0.16057290\r\n[4,] -0.1291551  0.046002023  0.05335227  0.03016902 -0.09637893\r\n[5,] -0.1920401 -0.042774189 -0.08610818  0.23634340 -0.13199009\r\n[6,] -0.1981733  0.067647107  0.09951241  0.02313998 -0.08580471\r\n            [,257]       [,258]        [,259]     [,260]      [,261]\r\n[1,] -0.0225894023  0.002869749  0.0007577947 -0.1680808 -0.01088620\r\n[2,] -0.0189757459  0.013451111 -0.1751664877 -0.3328351  0.05819667\r\n[3,] -0.0008530638 -0.031288225 -0.1339446753 -0.2323560  0.02520966\r\n[4,] -0.0292648710  0.020038901 -0.0299417879 -0.1354145  0.04481651\r\n[5,] -0.0285401363  0.079823844 -0.0336319245 -0.2776764  0.01303334\r\n[6,]  0.0409367606  0.002441470 -0.2192038000 -0.1395366  0.02692296\r\n           [,262]     [,263]      [,264]      [,265]       [,266]\r\n[1,]  0.015875859 0.10763116 -0.05167066  0.03018143  0.042046655\r\n[2,]  0.016516799 0.02993468 -0.07149611  0.04834108  0.030737203\r\n[3,] -0.020461027 0.14536050 -0.03549042  0.11245230  0.044793285\r\n[4,]  0.004487541 0.05348589 -0.09096584  0.01046701 -0.073750377\r\n[5,] -0.015841294 0.12982632 -0.09173639 -0.16630651  0.018952580\r\n[6,] -0.020832295 0.08133712 -0.02231704  0.01190614  0.007591155\r\n           [,267]      [,268]       [,269]       [,270]        [,271]\r\n[1,] -0.011430854 -0.02523194 -0.008423136 -0.012671799 -0.0837400183\r\n[2,]  0.004502268 -0.03668711 -0.017272692  0.024523588 -0.0125292251\r\n[3,] -0.003664760 -0.01398616 -0.037800763  0.038294815  0.0008871728\r\n[4,] -0.022474337 -0.06638593 -0.031185303 -0.075725012  0.0010475843\r\n[5,] -0.021464296 -0.07377630 -0.083129086 -0.036009334 -0.0625632778\r\n[6,] -0.081773162  0.02574211 -0.016636886  0.002660608  0.0052323635\r\n          [,272]        [,273]       [,274]       [,275]       [,276]\r\n[1,] -0.05445842 -0.0093964469  0.007517941 -0.011246663 -0.050030861\r\n[2,] -0.11006894  0.0164988823 -0.094942071  0.041801900 -0.018264698\r\n[3,] -0.01204425 -0.0008054683  0.044470005  0.007643088 -0.007325893\r\n[4,] -0.04889615  0.0439554788 -0.097014301  0.012413584 -0.069276668\r\n[5,] -0.15815380 -0.0682735518 -0.078197189 -0.049895536 -0.007100256\r\n[6,] -0.05562257  0.0194925591 -0.037227783  0.018591374 -0.016361907\r\n           [,277]     [,278]      [,279]       [,280]       [,281]\r\n[1,]  0.002760567 0.11517621 -0.06390390  0.071153320 -0.007525825\r\n[2,] -0.065950707 0.12664177 -0.19320154  0.056531884  0.081739336\r\n[3,] -0.068363689 0.10404641 -0.09738331  0.024798499  0.032022521\r\n[4,]  0.191381738 0.07087343 -0.22808014  0.007383201  0.047871239\r\n[5,] -0.167920634 0.06478498 -0.02706838  0.105121240  0.050797608\r\n[6,] -0.134912848 0.05239017 -0.25800148 -0.030304926  0.061357789\r\n          [,282]       [,283]      [,284]      [,285]      [,286]\r\n[1,] -0.08069538  0.021202818 -0.09088831 0.003913802  0.17268385\r\n[2,] -0.18115973  0.007516647  0.04322838 0.066528082  0.08921006\r\n[3,] -0.05470851 -0.030388746  0.01764945 0.005210739  0.07403997\r\n[4,] -0.03100875  0.030917926 -0.05289397 0.049072228  0.12345744\r\n[5,] -0.14827476 -0.034416664 -0.12222683 0.077855960 -0.01282306\r\n[6,] -0.06022647 -0.054439630  0.00454731 0.010294611  0.06895743\r\n          [,287]      [,288]     [,289]     [,290]       [,291]\r\n[1,]  0.03366879 -0.10051147 0.07276155 0.12489814 -0.029919706\r\n[2,] -0.05449703 -0.13379160 0.05222794 0.09763101 -0.011916515\r\n[3,] -0.03373038 -0.05401787 0.06726157 0.05924873 -0.003839437\r\n[4,] -0.03131104 -0.12301756 0.14005129 0.12710060 -0.017647358\r\n[5,] -0.64840341 -0.23175310 0.13674939 0.12690376 -0.084012493\r\n[6,]  0.06048229 -0.13205385 0.02206899 0.06544769 -0.033752106\r\n        [,292]        [,293]      [,294]      [,295]      [,296]\r\n[1,] 0.1965406 -0.0471164025 -0.09477758 -0.09592266 -0.14934851\r\n[2,] 0.1406389 -0.1141664833 -0.11793546 -0.03406483  0.03635797\r\n[3,] 0.2234888 -0.0002842388 -0.14569619 -0.10272630 -0.06068196\r\n[4,] 0.1935525  0.0183725003 -0.07485901 -0.10831285 -0.14020436\r\n[5,] 0.1750222 -0.0604986213 -0.04264855 -0.07688938 -0.09810477\r\n[6,] 0.1693811 -0.0750083998 -0.10084461 -0.08070657 -0.18682951\r\n           [,297]      [,298]       [,299]       [,300]      [,301]\r\n[1,]  0.005727845 -0.12093994 -0.003552702  0.020256335 0.016346853\r\n[2,]  0.015381114 -0.02306485 -0.015180913  0.009877626 0.141062707\r\n[3,]  0.015630685 -0.14868614 -0.007421072 -0.047970403 0.209519491\r\n[4,] -0.049769286 -0.13912010  0.035268210 -0.045016538 0.171082839\r\n[5,]  0.060936205 -0.17971252  0.027693467  0.137620136 0.006965111\r\n[6,]  0.020025980 -0.08880578  0.018619476  0.059553828 0.181092486\r\n          [,302]       [,303]       [,304]      [,305]       [,306]\r\n[1,] -0.11119895  0.077715434  0.055607371 0.123822115 -0.059588451\r\n[2,] -0.03967703 -0.057833292 -0.060716528 0.030593542  0.047125291\r\n[3,] -0.01229836 -0.035303757 -0.023960592 0.009966978  0.012475309\r\n[4,] -0.13119549 -0.068845138  0.002193379 0.147963077 -0.109287247\r\n[5,] -0.11175846 -0.002594928 -0.062189352 0.033291709 -0.072154455\r\n[6,] -0.03947608  0.094287872  0.019838080 0.029311704 -0.007268577\r\n          [,307]      [,308]      [,309]      [,310]       [,311]\r\n[1,]  0.05673822  0.05476711 -0.20486258  0.07061189 -0.062737666\r\n[2,]  0.15709876 -0.02061976 -0.16822445  0.01269852 -0.004345024\r\n[3,]  0.11021169  0.05043468 -0.07344368 -0.05323145  0.003844622\r\n[4,]  0.19097759  0.03540033 -0.14295101  0.02988683 -0.021163179\r\n[5,]  0.25572175  0.02911083 -0.12724619  0.06165682 -0.003929416\r\n[6,] -0.01906445  0.13942367 -0.13242154  0.07677628 -0.046496250\r\n          [,312]       [,313]       [,314]       [,315]        [,316]\r\n[1,] -0.08939619 -0.062958047  0.002350205 -0.028255599  0.0001992079\r\n[2,] -0.21793611 -0.065695688  0.139431939 -0.057130031  0.0188816339\r\n[3,] -0.14733313 -0.054874431  0.075538740  0.003273665  0.0071602282\r\n[4,] -0.13412905 -0.012387750  0.120887332  0.002740759 -0.0330684409\r\n[5,] -0.28124347 -0.009996795  0.111794926  0.031146541 -0.1252306700\r\n[6,] -0.05679907 -0.093764365 -0.010528004  0.028798489  0.0264520179\r\n        [,317]      [,318]     [,319]       [,320]       [,321]\r\n[1,] 0.2366612 -0.11066158 0.10403828 -0.093767919  0.015524159\r\n[2,] 0.1956885 -0.10604562 0.16890952 -0.016343875 -0.001231328\r\n[3,] 0.1087591  0.02513982 0.08264175 -0.071640648  0.017064070\r\n[4,] 0.2386470 -0.14696006 0.13099055 -0.006232007 -0.001926191\r\n[5,] 0.2298327  0.05288503 0.11605821 -0.037210070  0.103908330\r\n[6,] 0.2072268 -0.02443120 0.07308445 -0.073828861  0.048229817\r\n           [,322]     [,323]        [,324]      [,325]       [,326]\r\n[1,]  0.015304311 0.06769788  0.0001059437  0.13685572 -0.007125911\r\n[2,] -0.008312246 0.07099401  0.0118308347  0.11766487 -0.052791048\r\n[3,] -0.018496867 0.02761492  0.0046130028  0.16656889 -0.009691294\r\n[4,] -0.108083107 0.10613571 -0.0353088155 -0.03805166 -0.025139719\r\n[5,] -0.039033186 0.16623124  0.0668120831  0.03538605 -0.161977589\r\n[6,] -0.059934128 0.13409884  0.0132886814  0.06696119 -0.010411450\r\n          [,327]      [,328]       [,329]     [,330]      [,331]\r\n[1,] -0.09181453  0.16432312 -0.009687589 0.17562778  0.07574756\r\n[2,] -0.06574848  0.01219540 -0.081525952 0.09776805 -0.61099488\r\n[3,] -0.22979331 -0.00453141 -0.068012930 0.08546595 -0.64116883\r\n[4,] -0.08695555  0.03141475 -0.049017664 0.07748068 -0.32160136\r\n[5,] -0.17806914 -0.10213739 -0.101243451 0.16242576 -0.14651221\r\n[6,] -0.01661999  0.07961593 -0.058121491 0.11321305  0.18859792\r\n       [,332]        [,333]      [,334]       [,335]      [,336]\r\n[1,] 1.193441  0.0129416380 -0.04349080  0.023731416 -0.01097963\r\n[2,] 1.284796 -0.0026454674 -0.05538445  0.048212808 -0.02144463\r\n[3,] 1.263941 -0.0068049454 -0.02919309  0.054390196 -0.03931105\r\n[4,] 1.251522  0.0002867049 -0.12403233 -0.007573304 -0.04308565\r\n[5,] 1.432551  0.0599770546 -0.14562739  0.028240087 -0.01404148\r\n[6,] 1.122777  0.0230755098 -0.16138813  0.047617856 -0.04634143\r\n            [,337]     [,338]     [,339]    [,340]      [,341]\r\n[1,] -0.0005624792 0.07864352 0.04369727 0.2572507 -0.13316961\r\n[2,] -0.0054396740 0.05523947 0.15087622 0.1561821 -0.16100292\r\n[3,]  0.0424030162 0.10528552 0.11152666 0.1685743 -0.11688816\r\n[4,]  0.0157117005 0.06848166 0.05481624 0.1754922 -0.19357994\r\n[5,] -0.0011950094 0.06173320 0.07855321 0.1942654 -0.14314735\r\n[6,]  0.0254576858 0.10283647 0.07377444 0.1628036 -0.05531729\r\n           [,342]      [,343]      [,344]       [,345]       [,346]\r\n[1,]  0.084040277 -0.20822357 -0.01748156  0.036807824  0.006929139\r\n[2,]  0.022380980 -0.24127041 -0.01108753  0.003058314 -0.040675364\r\n[3,]  0.028627515 -0.14913662 -0.02476752  0.031754505 -0.023920799\r\n[4,]  0.064644255 -0.08234471 -0.10837119  0.067176044  0.017211705\r\n[5,]  0.095448487 -0.23304875  0.01558274  0.068599999  0.039748468\r\n[6,] -0.008355043 -0.17206128 -0.02875835 -0.000583343  0.013256798\r\n           [,347]      [,348]     [,349]     [,350]      [,351]\r\n[1,] -0.005286428 -0.05701319 0.13193549 0.08130930  0.04443504\r\n[2,] -0.086272962 -0.02544612 0.10772895 0.13471921  0.15701103\r\n[3,]  0.002710936 -0.10538142 0.07507202 0.07743213  0.12704344\r\n[4,] -0.040683534 -0.05692405 0.13225992 0.18849103 -0.02326316\r\n[5,]  0.039327800 -0.01303095 0.11355264 0.07589500  0.13782470\r\n[6,] -0.079469174 -0.02516594 0.06308875 0.05220953  0.07971393\r\n          [,352]       [,353]     [,354]     [,355]      [,356]\r\n[1,] -0.02041185  0.024838975 0.15259443 0.03433982  0.01410498\r\n[2,] -0.15240444 -0.043450933 0.08405434 0.05179580 -0.02859024\r\n[3,] -0.13477083 -0.027553072 0.08378947 0.04356062 -0.05607921\r\n[4,]  0.00702041 -0.009490191 0.10871423 0.01426539  0.13265060\r\n[5,]  0.04278595  0.179218471 0.21891491 0.02857704  0.13487510\r\n[6,] -0.01153280  0.004404217 0.07570871 0.07068371  0.11111432\r\n            [,357]      [,358]         [,359]     [,360]    [,361]\r\n[1,] -0.0292523298  0.03229756  0.06580498815 -0.3292757 0.1424101\r\n[2,]  0.0261140857  0.07876643  0.07768756896 -0.2670496 0.2002343\r\n[3,]  0.0005733054  0.03968107 -0.03592544794 -0.2605197 0.1676459\r\n[4,] -0.0405364856  0.05002245  0.05217316374 -0.1848912 0.1879823\r\n[5,]  0.0207061786  0.03774624  0.11750797927 -0.6159015 0.2709713\r\n[6,] -0.0419533104 -0.01498811 -0.00003440182 -0.1675033 0.1870018\r\n            [,362]     [,363]       [,364]      [,365]      [,366]\r\n[1,]  0.0003190466 0.10675317 -0.111781068 -0.26239395 -0.05190451\r\n[2,]  0.0764605403 0.06744294 -0.010644180 -0.08685648 -0.06909190\r\n[3,]  0.0122582344 0.06609353 -0.046548788 -0.04289630 -0.16739215\r\n[4,] -0.0306625608 0.13241743 -0.007170338 -0.22639199 -0.08798391\r\n[5,] -0.0633786842 0.06781193 -0.142221734 -0.24863669 -0.08552787\r\n[6,]  0.0074897874 0.13996646 -0.026384473 -0.11121219 -0.05444014\r\n         [,367]    [,368]      [,369]       [,370]     [,371]\r\n[1,] -0.2834142 0.1727780 -0.07177187 -0.024423050 0.11998454\r\n[2,] -0.2122738 0.1857841 -0.08979926 -0.061952963 0.18627794\r\n[3,] -0.2145126 0.1474682 -0.06742267  0.003836003 0.16774292\r\n[4,] -0.3423468 0.2046937 -0.09281652 -0.062675677 0.07005385\r\n[5,] -0.4036809 0.1320605 -0.07927746  0.047409087 0.14555788\r\n[6,] -0.2898973 0.1320122 -0.10214323 -0.049238041 0.14702497\r\n           [,372]       [,373]      [,374]      [,375]       [,376]\r\n[1,] -0.041142341 -0.018378917  0.01463352 -0.08249739  0.071360566\r\n[2,]  0.004512242  0.056799449 -0.10813290 -0.12739737  0.108280510\r\n[3,] -0.006871527  0.012354914 -0.05375824 -0.09662658  0.045653116\r\n[4,]  0.067509219 -0.007863864 -0.09351169  0.03765030 -0.020628667\r\n[5,]  0.089546248  0.028547799 -0.16447680 -0.04109525  0.001788142\r\n[6,] -0.043425713  0.015164633 -0.11218750 -0.05097958  0.068636134\r\n          [,377]       [,378]      [,379]       [,380]      [,381]\r\n[1,]  0.12089308 -0.013052741  0.16159718 -0.033959396 -0.05752983\r\n[2,] -0.01441357  0.005857999 -0.02121415 -0.036436468 -0.05078482\r\n[3,]  0.02916666  0.013266120  0.13223200 -0.067180336 -0.04920297\r\n[4,]  0.08411981 -0.018478211  0.22633652 -0.132887974 -0.00440924\r\n[5,] -0.01408248 -0.024376320  0.23904698 -0.001653882 -0.07003044\r\n[6,]  0.02938977  0.006751281  0.12645385 -0.053276427 -0.10031450\r\n           [,382]       [,383]     [,384]     [,385]       [,386]\r\n[1,] -0.022969488 -0.080281503 0.06668742 0.10190790  0.129292399\r\n[2,]  0.121824607 -0.015189585 0.08290621 0.06280330  0.016867727\r\n[3,]  0.006504176 -0.002793557 0.08314306 0.12468025 -0.008007731\r\n[4,] -0.073884942 -0.102000907 0.03791762 0.13350716  0.163697481\r\n[5,]  0.017336383 -0.135498956 0.08327901 0.24546270  0.137031600\r\n[6,]  0.013923938 -0.113184415 0.04481167 0.04153857 -0.015584327\r\n          [,387]      [,388]    [,389]       [,390]      [,391]\r\n[1,]  0.04810674 -0.07212174 0.3981770  0.000651068 -0.01844281\r\n[2,]  0.08981262 -0.05153822 0.2162744 -0.028560745 -0.05627009\r\n[3,]  0.03696474 -0.01001888 0.1683803 -0.063800417 -0.06998126\r\n[4,]  0.04972491 -0.02204287 0.4260711 -0.051906057 -0.01058496\r\n[5,]  0.15990907 -0.04117636 0.5371452 -0.097876579 -0.09795716\r\n[6,] -0.01717817 -0.02943450 0.2676919 -0.051112194 -0.09012391\r\n          [,392]      [,393]       [,394]        [,395]      [,396]\r\n[1,]  0.09665750 -0.21984693  0.077912547 -0.0058517088  0.01657313\r\n[2,] -0.03973968 -0.19702394 -0.096619479 -0.0071436288 -0.05720001\r\n[3,] -0.01701060 -0.11564402 -0.038870413  0.0449757017 -0.04972860\r\n[4,]  0.03982517 -0.18398654 -0.002435916  0.0043237559 -0.01682329\r\n[5,]  0.06203847 -0.05672979  0.094466582  0.0115374159  0.02329841\r\n[6,]  0.09998129 -0.06618091  0.092190057  0.0006072707 -0.01348944\r\n          [,397]     [,398]    [,399]       [,400]      [,401]\r\n[1,] 0.029640796 -0.2341008 0.2731010 -0.031091580 -0.01752099\r\n[2,] 0.008583851  0.1481631 0.2792898 -0.064082794 -0.07462133\r\n[3,] 0.040935591  0.1765531 0.2668295 -0.038621537 -0.06084965\r\n[4,] 0.081413433 -0.2189223 0.2725447 -0.195925117 -0.03351154\r\n[5,] 0.186902285  0.1041174 0.2916219 -0.148684010 -0.07247491\r\n[6,] 0.037978858 -0.2098601 0.2650369 -0.003034409 -0.08609407\r\n           [,402]      [,403]      [,404]      [,405]       [,406]\r\n[1,] -0.055302404 -0.08445628 0.124047980 -0.07431474  0.033396285\r\n[2,]  0.026117302 -0.06102932 0.003592577  0.07306582 -0.087093271\r\n[3,] -0.026919138 -0.02823371 0.013763811  0.06260777  0.003319245\r\n[4,] -0.084522232  0.01085360 0.148553595  0.14148732  0.085137658\r\n[5,]  0.004054369 -0.05274140 0.276448041 -0.02868071 -0.083662651\r\n[6,]  0.085933879 -0.04678488 0.110170588  0.11049432 -0.065488786\r\n            [,407]       [,408]      [,409]     [,410]        [,411]\r\n[1,] -0.0833341256 -0.014363504 -0.08923447 0.18580405  0.0527944863\r\n[2,]  0.0011571615 -0.042233806 -0.05868121 0.10632948 -0.0005507554\r\n[3,] -0.0839891359  0.001657259 -0.11240651 0.09383041  0.0061236657\r\n[4,] -0.0770618767 -0.014581834 -0.18196091 0.16608985  0.0234143566\r\n[5,] -0.0693483502 -0.138323084 -0.20459747 0.23080853  0.0352744423\r\n[6,] -0.0009951121  0.019755036 -0.15968242 0.11442276 -0.0185750797\r\n         [,412]     [,413]      [,414]    [,415]      [,416]\r\n[1,] 0.07303324 0.10113084 -0.06256250 0.1384585 -0.09067871\r\n[2,] 0.02094452 0.04001701 -0.03109857 0.1248479 -0.06825428\r\n[3,] 0.03956146 0.01913995 -0.01180687 0.1212969 -0.06751077\r\n[4,] 0.03233004 0.07872116  0.06508625 0.0848489 -0.03301696\r\n[5,] 0.07095757 0.02153363  0.14255515 0.1556596 -0.03317551\r\n[6,] 0.06180367 0.02782013  0.06218156 0.1112180 -0.04977389\r\n           [,417]      [,418]       [,419]      [,420]      [,421]\r\n[1,]  0.019350978  0.01523919 -0.034700625 -0.15207638 -0.09571344\r\n[2,]  0.015089027 -0.13927549  0.059285089 -0.07940488 -0.13693495\r\n[3,] -0.001048695 -0.03808055 -0.063612774 -0.05192776 -0.18541250\r\n[4,]  0.031780388 -0.08320169  0.066831119  0.02576646 -0.08961902\r\n[5,]  0.039614756 -0.10220528 -0.120935448 -0.17789137 -0.10625569\r\n[6,]  0.058109745 -0.01198777 -0.004356108  0.08624344 -0.10441324\r\n          [,422]     [,423]      [,424]    [,425]       [,426]\r\n[1,] -0.02452135 0.14253086 -0.05657317 0.3280779 -0.050315730\r\n[2,] -0.06415525 0.07602257 -0.06367315 0.1944746 -0.065398335\r\n[3,] -0.06640281 0.02816646 -0.09902421 0.1656896 -0.024696238\r\n[4,] -0.04900699 0.19998491 -0.10840013 0.3802281  0.003306185\r\n[5,] -0.01606719 0.20564817  0.00140973 0.6608974  0.056467004\r\n[6,] -0.03569328 0.09580938 -0.06109742 0.1208945 -0.076438911\r\n           [,427]      [,428]       [,429]      [,430]      [,431]\r\n[1,] -0.057930082 -0.05862350 -0.095415689 -0.10481034 -0.06387607\r\n[2,]  0.010639274 -0.12303247 -0.051223289 -0.09339171 -0.05981595\r\n[3,] -0.001297983 -0.03304556 -0.122825339 -0.12358673 -0.06911961\r\n[4,] -0.123188123 -0.16720073  0.003969586 -0.10472445  0.06382620\r\n[5,] -0.115927018 -0.08907121 -0.051757324 -0.17229636 -0.06435966\r\n[6,] -0.064655416 -0.06646870 -0.047626436 -0.13625301  0.03774286\r\n         [,432]       [,433]     [,434]      [,435]       [,436]\r\n[1,] 0.13167702 -0.043118063 0.06146807  0.01869973 -0.035224576\r\n[2,] 0.16845942 -0.192152798 0.13094904  0.03596283  0.033534888\r\n[3,] 0.14747572 -0.076833479 0.13047643  0.02428866  0.014121264\r\n[4,] 0.25472251 -0.007195167 0.15603444 -0.00596704 -0.059642036\r\n[5,] 0.07380281 -0.071108758 0.15276530  0.05286176 -0.119509064\r\n[6,] 0.11972743 -0.148522720 0.09822256 -0.08528683 -0.005617861\r\n           [,437]      [,438]      [,439]      [,440]      [,441]\r\n[1,]  0.002619761 -0.08160212  0.08646131 -0.07696088  0.06323978\r\n[2,] -0.009239418 -0.10202175 -0.02077171 -0.06891251  0.05190685\r\n[3,] -0.009320967 -0.04484579  0.10739184 -0.02527408 -0.00380435\r\n[4,]  0.054793835 -0.07226864  0.01493895 -0.08142722  0.07248068\r\n[5,] -0.043097962 -0.18096198 -0.06011949 -0.18862140  0.21504816\r\n[6,]  0.023491399 -0.01777480  0.07863464 -0.12527528  0.16570453\r\n         [,442]      [,443]      [,444]      [,445]       [,446]\r\n[1,] 0.01898709 0.092833392  0.06426620  0.10507441  0.084170453\r\n[2,] 0.05463684 0.003707611 -0.09394057  0.05733651  0.106665961\r\n[3,] 0.04206577 0.029764373  0.01227091  0.08231951  0.069943115\r\n[4,] 0.09810630 0.079904787  0.07489314 -0.01253995 -0.008394581\r\n[5,] 0.06945176 0.109472521  0.04685316  0.04640696  0.102879412\r\n[6,] 0.03247901 0.101068415  0.01057376  0.11562008  0.069222540\r\n           [,447]     [,448]       [,449]      [,450]    [,451]\r\n[1,] -0.024374206 0.04312007  0.107861504  0.13711369 0.1291108\r\n[2,]  0.001252827 0.08850355  0.049432795  0.06416221 0.1807139\r\n[3,] -0.038332362 0.08287523  0.023291139  0.06984615 0.1361608\r\n[4,] -0.021328092 0.12580599  0.139093950  0.06507008 0.1231573\r\n[5,] -0.129619345 0.10041834  0.199645579 -0.01893841 0.1511120\r\n[6,] -0.030451402 0.09080670 -0.002246229  0.09229445 0.1914775\r\n          [,452]     [,453]    [,454]       [,455]       [,456]\r\n[1,] -0.06324755 -0.2063144 -1.571300  0.011344370  0.010410558\r\n[2,] -0.08418371 -0.2678480 -2.114250 -0.065529682 -0.081989832\r\n[3,] -0.04842710 -0.1767584 -2.138216  0.027580163 -0.015867468\r\n[4,]  0.02564547 -0.1488806 -1.496716 -0.007744787 -0.008718793\r\n[5,] -0.05997337 -0.4564939 -1.645690  0.049410906  0.040771157\r\n[6,] -0.11389697 -0.2701066 -1.500608 -0.003843458  0.063257314\r\n         [,457]      [,458]      [,459]      [,460]      [,461]\r\n[1,] 0.03868202 -0.05845854 -0.12579459 0.005873961 -0.04595002\r\n[2,] 0.10147323 -0.04141930 -0.09119720 0.114982814 -0.09992974\r\n[3,] 0.04151445 -0.04142060 -0.09097617 0.015678143 -0.05815847\r\n[4,] 0.03656706 -0.14261545 -0.21980679 0.045785569 -0.01823764\r\n[5,] 0.11710306 -0.07848915 -0.34025028 0.061278366  0.03995755\r\n[6,] 0.01473739 -0.09901690 -0.18111326 0.036432479 -0.08962186\r\n           [,462]       [,463]       [,464]      [,465]      [,466]\r\n[1,] -0.022218920 -0.009022172  0.064793751 -0.01880163 -0.09919238\r\n[2,] -0.148305222 -0.019372806 -0.015265632  0.08394215 -0.10314022\r\n[3,] -0.073457286  0.009041996 -0.009008986  0.06632591 -0.02867668\r\n[4,]  0.006836804 -0.142976373  0.013609613  0.10924794 -0.05234744\r\n[5,] -0.121134020 -0.058553871 -0.002999091  0.01065715 -0.04240579\r\n[6,] -0.073510975 -0.042291988  0.015074087  0.04148116 -0.08730940\r\n         [,467]     [,468]      [,469]        [,470]      [,471]\r\n[1,] -0.1562882 0.09386369  0.01467662  0.0904685780 -0.14649619\r\n[2,] -0.1320574 0.11428429 -0.06193297  0.0262522604 -0.01595671\r\n[3,] -0.1121669 0.13802610  0.14548612  0.0356551185  0.04380416\r\n[4,] -0.1064183 0.10055429  0.02231788  0.0873961374 -0.01238519\r\n[5,] -0.1398273 0.35697773 -0.11256426  0.0611174740 -0.14872175\r\n[6,] -0.2030301 0.02190426  0.00917128 -0.0002893528 -0.02008463\r\n          [,472]       [,473]      [,474]      [,475]      [,476]\r\n[1,]  0.06360921  0.002835296 -0.14112200  0.01089384 -0.09374716\r\n[2,] -0.09267101 -0.011794514 -0.09564537  0.09496267 -0.09849050\r\n[3,] -0.06956573 -0.032383204 -0.06577902  0.05314546 -0.08813422\r\n[4,]  0.06355684 -0.006422742 -0.14174026  0.01064497 -0.12374763\r\n[5,]  0.07341044 -0.028724032 -0.13385014 -0.01298049 -0.06281004\r\n[6,]  0.02312492 -0.038785104 -0.03216275  0.07268793 -0.11918852\r\n           [,477]    [,478]      [,479]      [,480]      [,481]\r\n[1,]  0.085721515 0.2369088 -0.03163733 -0.04961759  0.07567289\r\n[2,]  0.212540090 0.4052520 -0.06378204  0.06538963  0.06334462\r\n[3,]  0.035656735 0.3395200 -0.12373400  0.06951213  0.03967689\r\n[4,]  0.069634311 0.3115078 -0.04387229  0.01914976  0.02651158\r\n[5,]  0.093301833 0.1765930 -0.06029833  0.04276315 -0.01899371\r\n[6,] -0.007837979 0.4033882  0.01032726  0.09456421  0.03680502\r\n           [,482]      [,483]       [,484]       [,485]       [,486]\r\n[1,]  0.014224278 -0.04090451  0.028643118  0.005485986  0.024289938\r\n[2,]  0.028082833  0.12406152  0.050737426  0.004382323  0.031838104\r\n[3,] -0.011032891  0.11820289  0.030434420  0.024688704  0.056926142\r\n[4,]  0.058519281  0.02351317  0.002203047  0.014882185  0.109045506\r\n[5,]  0.030299021 -0.03670148 -0.004659429  0.038361084 -0.009311764\r\n[6,] -0.008477365  0.01284167  0.008294596 -0.044905756  0.010071323\r\n           [,487]       [,488]     [,489]      [,490]      [,491]\r\n[1,]  0.027732164 -0.054697767 0.08244185 -0.01940191 -0.18509653\r\n[2,] -0.079287864  0.041016314 0.07844529 -0.07530592 -0.10736346\r\n[3,] -0.002462003  0.028426407 0.04889518 -0.06274561 -0.01069009\r\n[4,] -0.104643159  0.001387747 0.01470165 -0.03739887 -0.08979299\r\n[5,] -0.052101936  0.134826988 0.06035344  0.02556883 -0.08897312\r\n[6,] -0.078772254 -0.022792758 0.03378720 -0.09432817 -0.05770224\r\n          [,492]      [,493]    [,494]     [,495]      [,496]\r\n[1,] -0.03261138  0.01393211 0.1644626 0.10143952 -0.01441867\r\n[2,] -0.19953111  0.05215713 0.1580814 0.10161492 -0.05557086\r\n[3,] -0.11720665  0.09111667 0.1566399 0.11439383 -0.04178641\r\n[4,] -0.02815072 -0.02023082 0.2387220 0.11720078  0.06616344\r\n[5,] -0.11564896 -0.05522026 0.1704042 0.08389510  0.07512923\r\n[6,] -0.09523348  0.08016193 0.2092725 0.09535258 -0.05332264\r\n          [,497]      [,498]      [,499]       [,500]       [,501]\r\n[1,]  0.13447063 0.176436096 -0.15089904  0.008123890 -0.062278148\r\n[2,] -0.14183965 0.101238340  0.16178432 -0.021955615 -0.004715705\r\n[3,] -0.11346148 0.155305624  0.08963346  0.015225065 -0.032031965\r\n[4,]  0.07938032 0.187938437 -0.41815472 -0.008774522  0.033738963\r\n[5,]  0.18068925 0.002046947 -0.19629756 -0.005373829  0.010599384\r\n[6,] -0.17102486 0.110839099  0.01813341  0.027815249 -0.043837361\r\n          [,502]     [,503]       [,504]     [,505]     [,506]\r\n[1,] -0.06082680 0.16952525  0.006600899 0.10919487 0.03599295\r\n[2,] -0.01016839 0.11328282  0.014151102 0.12925433 0.03840022\r\n[3,] -0.01715080 0.12347469  0.032013722 0.14595447 0.03473345\r\n[4,] -0.02428205 0.11775508 -0.134463638 0.11429659 0.08892217\r\n[5,] -0.08576907 0.08981242 -0.034434706 0.03607088 0.17811798\r\n[6,]  0.01329549 0.08677683  0.127937108 0.19148800 0.04910553\r\n           [,507]       [,508]      [,509]      [,510]      [,511]\r\n[1,] -0.081771679 -0.109140441 -0.07335753 -0.01502807 -0.01548101\r\n[2,]  0.022215756 -0.021342721 -0.08955568  0.04025942 -0.02196560\r\n[3,] -0.009848425 -0.045117740 -0.05692595  0.01472343 -0.02373696\r\n[4,] -0.019649435 -0.007875388 -0.07495666  0.03273108 -0.04050305\r\n[5,] -0.024837051 -0.084895268  0.02489420  0.10396164  0.08499401\r\n[6,] -0.058013108 -0.079431497 -0.02372424  0.03251337 -0.06706781\r\n         [,512]     [,513]       [,514]     [,515]     [,516]\r\n[1,] 0.20747209 0.10412335  0.030921176 0.04140276 0.13099532\r\n[2,] 0.24077047 0.07284018  0.027469020 0.10394826 0.11812397\r\n[3,] 0.07832344 0.05361049 -0.003711764 0.01981778 0.14238910\r\n[4,] 0.22885244 0.09878208  0.029369308 0.01469264 0.08092756\r\n[5,] 0.09952058 0.11782581 -0.058057539 0.01267564 0.13141654\r\n[6,] 0.21342711 0.05211563 -0.037015975 0.08124081 0.14627922\r\n           [,517]      [,518]    [,519]    [,520]      [,521]\r\n[1,] -0.096544601 -0.02722531 0.1704053 0.6414233 -0.10801749\r\n[2,] -0.068766735 -0.03247895 0.2348349 0.4556105 -0.04150981\r\n[3,] -0.074356921 -0.05799698 0.2257889 0.3981168 -0.05121266\r\n[4,] -0.007750449  0.01547960 0.1302545 0.4229303 -0.05532151\r\n[5,] -0.035133030 -0.05410383 0.3415779 0.2393515  0.01129965\r\n[6,]  0.055697769 -0.07908421 0.3142245 0.2538634 -0.13136469\r\n          [,522]      [,523]       [,524]         [,525]       [,526]\r\n[1,] 0.028488843  0.01229791  0.006769815 -0.02220854349  0.008598752\r\n[2,] 0.090912819 -0.04902017 -0.038836576 -0.05532822385 -0.013462212\r\n[3,] 0.041311838  0.05117850 -0.030412136  0.00006190916  0.024389319\r\n[4,] 0.089615382  0.05270386  0.011018316  0.00578867877  0.048756767\r\n[5,] 0.005162219 -0.01371515  0.028771298 -0.07557101548  0.118265748\r\n[6,] 0.070394009  0.04985644 -0.056496751 -0.04647116736  0.046551116\r\n           [,527]      [,528]     [,529]       [,530]      [,531]\r\n[1,] -0.080438398 -0.05049169 0.06124162 -0.037969735 -0.10010156\r\n[2,] -0.052286796  0.05558256 0.04355393 -0.065679953  0.28628990\r\n[3,] -0.053335365  0.05906692 0.07683131  0.002506749  0.15025698\r\n[4,]  0.050195921  0.04279092 0.01739311 -0.035167418  0.07004783\r\n[5,]  0.004074886  0.08705671 0.13898785 -0.020913765  0.01851276\r\n[6,] -0.033718653  0.01462394 0.15657954  0.006698030  0.16144638\r\n         [,532]     [,533]       [,534]      [,535]      [,536]\r\n[1,] -0.1132798 0.05667033  0.024161274 -0.03244631 -0.02328633\r\n[2,] -0.1149235 0.14032076  0.022697564 -0.08682737 -0.01391359\r\n[3,] -0.1466271 0.07502884  0.012020401 -0.04629718  0.03811403\r\n[4,] -0.1270624 0.04202068 -0.072816454 -0.04554997 -0.06863053\r\n[5,] -0.1100831 0.17043321 -0.002551615 -0.16728048  0.06786533\r\n[6,] -0.1331794 0.06237527  0.017379900 -0.07297575  0.03169990\r\n           [,537]      [,538]     [,539]       [,540]      [,541]\r\n[1,] -0.049195871 -0.05195329 0.10871506  0.004009936  0.04403062\r\n[2,]  0.079532564  0.10515817 0.20094171  0.148961648 -0.09903095\r\n[3,] -0.007620021  0.04351531 0.04169322  0.079800084  0.05986871\r\n[4,] -0.082801260 -0.01348173 0.09745122  0.073346667  0.11053089\r\n[5,] -0.010112019  0.02520639 0.14831597 -0.052286886 -0.07703506\r\n[6,] -0.040300995  0.02824445 0.13761625  0.132395297  0.06715680\r\n          [,542]       [,543]     [,544]      [,545]        [,546]\r\n[1,] -0.09582030  0.061663870 0.10744517 0.060571976 0.02810172178\r\n[2,] -0.09636804  0.151209295 0.13660660 0.107330613 0.05540509522\r\n[3,] -0.01025726  0.078234747 0.10478605 0.050891187 0.03553915024\r\n[4,] -0.05504660  0.001779500 0.10987019 0.024541641 0.15697939694\r\n[5,] -0.02101547  0.126134664 0.09540144 0.086240515 0.10588035733\r\n[6,] -0.02692613 -0.007448914 0.05709149 0.003051612 0.00007900961\r\n         [,547]     [,548]       [,549]       [,550]      [,551]\r\n[1,] 0.02419191 0.08803953 -0.080621943 -0.074648671 -0.03054844\r\n[2,] 0.05273153 0.07319796  0.049742538 -0.011444277 -0.04701106\r\n[3,] 0.06160079 0.09243686  0.001928369  0.070776083  0.05945547\r\n[4,] 0.09536791 0.07475567 -0.086712070  0.001008996  0.13148466\r\n[5,] 0.03845917 0.03740457 -0.089416340  0.134222209 -0.03225615\r\n[6,] 0.03995414 0.01827375 -0.035682827  0.096883036  0.09749158\r\n       [,552]     [,553]       [,554]     [,555]      [,556]\r\n[1,] 3.490957 0.09625969  0.059705943 -0.2869728 -0.07438415\r\n[2,] 3.009406 0.06977017  0.016659489 -0.2880733  0.00384901\r\n[3,] 2.826385 0.09348783  0.016458966 -0.2091832 -0.02817199\r\n[4,] 2.873447 0.06551965  0.091334976 -0.3344169 -0.03123012\r\n[5,] 2.630254 0.01768534 -0.009353253 -0.1667103  0.03629993\r\n[6,] 2.829683 0.14530991  0.045351252 -0.3220121 -0.02746979\r\n           [,557]       [,558]      [,559]      [,560]       [,561]\r\n[1,] -0.040171362  0.002730965 -0.04682355 -0.07253763 -0.007843177\r\n[2,]  0.003828358  0.015141261 -0.02566535  0.01270058  0.016795876\r\n[3,]  0.013223354  0.004305525 -0.02875073 -0.03749710  0.011652901\r\n[4,]  0.006316597  0.009411959 -0.02258588 -0.11649830  0.015000288\r\n[5,] -0.050341222 -0.014990398 -0.06293204 -0.13567844  0.025790883\r\n[6,] -0.016510585  0.100776270 -0.02727570 -0.01096613  0.043969691\r\n          [,562]       [,563]      [,564]        [,565]       [,566]\r\n[1,] -0.10107207 -0.090766735  0.05772920  0.0357586853 -0.007650903\r\n[2,]  0.25792950 -0.004117329 -0.04113457 -0.0826064125  0.031086469\r\n[3,]  0.14507194 -0.033674099 -0.00137327 -0.0002648491  0.002827710\r\n[4,] -0.24326281 -0.060469653  0.04351293 -0.0448633507  0.019339940\r\n[5,] -0.33636457 -0.015884975  0.11548383  0.0600102246  0.033983886\r\n[6,] -0.03306252 -0.088506110 -0.01203969 -0.1086494625 -0.042369910\r\n         [,567]     [,568]      [,569]      [,570]       [,571]\r\n[1,] 0.15585314 0.13051005 0.108799525 0.064547881 -0.007139327\r\n[2,] 0.01048216 0.13346034 0.059202123 0.051440962 -0.097843759\r\n[3,] 0.03364506 0.08602677 0.066494778 0.020454615 -0.141242966\r\n[4,] 0.16724968 0.18270221 0.004842736 0.019932335 -0.105267487\r\n[5,] 0.19563849 0.05251458 0.017202612 0.179404736  0.375534475\r\n[6,] 0.07881544 0.13055730 0.071095765 0.004675159 -0.218478486\r\n            [,572]        [,573]      [,574]      [,575]       [,576]\r\n[1,]  0.0058474680 -0.0531055890  0.11780847 -0.03810208  0.019046675\r\n[2,] -0.0034160153 -0.0007439902  0.19959642  0.04476420 -0.036860768\r\n[3,] -0.0167324562 -0.0843516439  0.29047588  0.04091063 -0.002043449\r\n[4,] -0.0376024060  0.0235952139  0.06060461 -0.06478819  0.069602810\r\n[5,]  0.0257282034 -0.0984239876 -0.03339000  0.03665282  0.051747896\r\n[6,] -0.0002246065  0.0356406122  0.26451844 -0.04111731  0.015969321\r\n          [,577]    [,578]      [,579]        [,580]      [,581]\r\n[1,] 0.051728375 0.2837152 -0.12573980 0.04450747371 -0.20007625\r\n[2,] 0.006162872 0.2887946 -0.14360166 0.12143808603 -0.07812190\r\n[3,] 0.032963041 0.2407141 -0.13535056 0.05022520199 -0.03726773\r\n[4,] 0.007787528 0.3056432 -0.10764431 0.00005984147  0.01136403\r\n[5,] 0.043765217 0.4460579 -0.18649001 0.03132902831 -0.18883580\r\n[6,] 0.013649425 0.2142384 -0.00264157 0.05455103889 -0.15630220\r\n          [,582]        [,583]     [,584]     [,585]     [,586]\r\n[1,]  0.04763975 -0.1027267799 0.08992080 0.12758245 0.13977332\r\n[2,]  0.03859643  0.0130787063 0.19769299 0.10191844 0.11003129\r\n[3,]  0.05335652  0.0005924911 0.20755954 0.15173234 0.08798728\r\n[4,]  0.07987957 -0.0494213626 0.12524505 0.08972426 0.07025620\r\n[5,]  0.10612362 -0.0147182839 0.10312622 0.20463093 0.27740616\r\n[6,] -0.03431785  0.0195534118 0.06446878 0.16805252 0.12031044\r\n           [,587]       [,588]   [,589]       [,590]      [,591]\r\n[1,] -0.196053058 -0.146547332 10.55038  0.079190038 -0.04186701\r\n[2,] -0.009361898 -0.084520370 10.62850 -0.006851233 -0.01928166\r\n[3,] -0.027103726 -0.092572533 10.67282 -0.018484965 -0.01760916\r\n[4,] -0.157154799  0.041249577 10.60373 -0.035592694 -0.05346956\r\n[5,] -0.276923925  0.007194216 10.01591  0.077082850 -0.11309950\r\n[6,]  0.087701470 -0.180597544 11.05808 -0.048482083 -0.05305341\r\n         [,592]     [,593]      [,594]        [,595]      [,596]\r\n[1,] 0.11649291 0.02831738  0.04861200  0.0386349931 -0.01457212\r\n[2,] 0.13541912 0.04426802  0.05433436 -0.0221196022 -0.10279615\r\n[3,] 0.12529573 0.07380189  0.03893021  0.0117186746 -0.06627520\r\n[4,] 0.09005984 0.10297865  0.06968775  0.0643280819 -0.11483820\r\n[5,] 0.06878667 0.08176273 -0.01724512  0.0463816263 -0.08895335\r\n[6,] 0.13597111 0.11060708 -0.03129032 -0.0007398911 -0.11685614\r\n          [,597]       [,598]      [,599]        [,600]      [,601]\r\n[1,]  0.01919353  0.030889636 -0.02790228  0.0506280921  0.09110180\r\n[2,] -0.05993731  0.039520446 -0.07885469 -0.0756680071  0.03081270\r\n[3,]  0.10984821  0.007104009 -0.09006495 -0.0352274142 -0.01103595\r\n[4,]  0.08354216 -0.054511044 -0.04526887 -0.0006344669  0.07211627\r\n[5,] -0.05610860  0.090620331 -0.03970441  0.1431293339  0.08085971\r\n[6,] -0.07293376  0.055972077 -0.07523298  0.0279906746  0.04177547\r\n          [,602]       [,603]       [,604]     [,605]     [,606]\r\n[1,] -0.01676150 -0.003624233  0.008825682 -0.2234787 0.18158303\r\n[2,] -0.11423547 -0.006869983 -0.095324494 -0.1961642 0.14314942\r\n[3,] -0.10119756 -0.005852762 -0.058466714 -0.2928435 0.11609706\r\n[4,]  0.03140411 -0.021237850  0.026722971 -0.1151845 0.16916385\r\n[5,] -0.01638347 -0.096810423  0.053151648 -0.2580744 0.07612113\r\n[6,] -0.09530030  0.032139529 -0.049519286 -0.2045097 0.12998132\r\n         [,607]       [,608]       [,609]      [,610]       [,611]\r\n[1,] 0.07298457 -0.002651701 -0.158432558 -0.04743477 -0.005269919\r\n[2,] 0.11046232  0.057270877  0.016978169 -0.10646354  0.004269409\r\n[3,] 0.18984833  0.023391020  0.003230169 -0.14901696  0.011862932\r\n[4,] 0.02340865  0.045395304  0.039700259 -0.02079147  0.003488605\r\n[5,] 0.06108408  0.064282469  0.080747120  0.16635144 -0.024059182\r\n[6,] 0.09323003 -0.007702209  0.046257153 -0.05669680 -0.016939459\r\n        [,612]     [,613]       [,614]     [,615]      [,616]\r\n[1,] 0.4457970 0.05577384  0.067969643 0.11045261 -0.13252786\r\n[2,] 0.1118504 0.01980666  0.163218647 0.23445050 -0.05967879\r\n[3,] 0.1646561 0.02292584  0.041179903 0.23108661 -0.09604835\r\n[4,] 0.2329158 0.07637633  0.008157513 0.23833963  0.04642060\r\n[5,] 0.9287996 0.01388288 -0.051213335 0.28340751 -0.15695475\r\n[6,] 0.2541634 0.01503851  0.088469163 0.03579002 -0.12264512\r\n          [,617]      [,618]      [,619]      [,620]     [,621]\r\n[1,] -0.03382699 -0.06885208 -0.10973601  0.18532184 0.04200668\r\n[2,]  0.01342961 -0.18169832 -0.16695282 -0.02456107 0.05904873\r\n[3,] -0.02139417 -0.13064028 -0.09058248  0.02769518 0.03524665\r\n[4,] -0.06634750 -0.21438241 -0.05371312  0.04360333 0.03625013\r\n[5,] -0.15594494 -0.12468186 -0.04595389 -0.13771467 0.09732056\r\n[6,] -0.10214241 -0.10415285 -0.10364988  0.03797656 0.01805913\r\n          [,622]      [,623]      [,624]     [,625]       [,626]\r\n[1,] -0.02282365  0.08494413 -0.02972709 -0.2462529 -0.055069294\r\n[2,] -0.02308360  0.07176681  0.02247033 -0.2373391 -0.015013164\r\n[3,] -0.02676908  0.09027924  0.09314279 -0.1643709 -0.012452682\r\n[4,] -0.01139543  0.05307195 -0.02017500 -0.3225598 -0.016039358\r\n[5,] -0.06631549 -0.01779379 -0.13958761  0.3482752 -0.000444417\r\n[6,]  0.08783573  0.02368183  0.20505987 -0.4261086  0.038747206\r\n           [,627]      [,628]     [,629]     [,630]      [,631]\r\n[1,]  0.024370451 -0.04531929 0.02464755 0.06753983 -0.02750900\r\n[2,]  0.040073313  0.16513161 0.03286229 0.11217613 -0.01320007\r\n[3,]  0.048136637 -0.03035239 0.06777806 0.07820796 -0.02618234\r\n[4,] -0.004290697  0.11767638 0.09995991 0.17675555 -0.06916409\r\n[5,] -0.067067407  0.06463806 0.06637219 0.02278540 -0.12282576\r\n[6,]  0.081883602  0.06337563 0.05944666 0.11099052 -0.05610881\r\n         [,632]        [,633]      [,634]      [,635]       [,636]\r\n[1,] 0.06387967  0.0353682823  0.01161192 -0.09307051 -0.032301307\r\n[2,] 0.04192519 -0.0545239002  0.04249949  0.04873569  0.057895780\r\n[3,] 0.04624534 -0.0129249850  0.06176883  0.01341312  0.042152233\r\n[4,] 0.10888313  0.0351713225  0.07008977 -0.03464171  0.001583954\r\n[5,] 0.01548626  0.0615639053 -0.03843283 -0.03582926  0.063118577\r\n[6,] 0.01118421 -0.0005861023  0.07010639  0.01689513  0.081836693\r\n          [,637]      [,638]       [,639]     [,640]     [,641]\r\n[1,]  0.03606339 -0.13619940 -0.051367376 0.10353312 0.07392192\r\n[2,]  0.09408715 -0.08630372 -0.080650181 0.17458060 0.07599048\r\n[3,]  0.04858595 -0.04637498 -0.080615312 0.07917575 0.01036839\r\n[4,]  0.12021314 -0.15209834 -0.119823240 0.04993320 0.03411581\r\n[5,]  0.02005794 -0.13674355  0.002104346 0.07433884 0.12297715\r\n[6,] -0.03113093  0.04155017 -0.085316509 0.05384269 0.09085373\r\n          [,642]      [,643]     [,644]      [,645]       [,646]\r\n[1,]  0.01333701 -0.00582011 0.15713017 -0.05514756  0.044768147\r\n[2,]  0.06772912 -0.04244962 0.11102190 -0.13224754  0.026957383\r\n[3,]  0.02008614 -0.02806790 0.10794488 -0.14916168 -0.005188774\r\n[4,]  0.04652314 -0.05789090 0.08634548 -0.07793872  0.094190098\r\n[5,]  0.01489882 -0.12299080 0.10937995 -0.01534617  0.117422812\r\n[6,] -0.05196442 -0.05412933 0.09008405 -0.04924094  0.063689373\r\n          [,647]      [,648]       [,649]      [,650]       [,651]\r\n[1,]  0.08581818  0.14838016 -0.067709140  0.02948107  0.085206084\r\n[2,] -0.01263564  0.16454530 -0.053092696 -0.03030418 -0.033222977\r\n[3,] -0.01956559  0.04567408  0.006813697 -0.03798756  0.004434918\r\n[4,]  0.13683076 -0.11759794  0.035414111  0.06610941  0.062410619\r\n[5,]  0.16712075 -0.05554906  0.101892747  0.07900701  0.012045181\r\n[6,]  0.02136588  0.25598088 -0.050038930  0.04889823  0.038526274\r\n         [,652]      [,653]      [,654]     [,655]      [,656]\r\n[1,] 0.33459133 -0.19480552 -0.10066294 0.03892896 -0.08909143\r\n[2,] 0.19124396  0.07472620  0.07023271 0.03579077 -0.03493149\r\n[3,] 0.09335737 -0.07692355  0.07608291 0.03562872  0.06468541\r\n[4,] 0.14920172 -0.32679296  0.04796868 0.02999353 -0.08332895\r\n[5,] 0.09121822 -0.18201549  0.01287561 0.15056956  0.03620177\r\n[6,] 0.08416826 -0.10758856  0.08227087 0.01508947 -0.18300064\r\n          [,657]       [,658]     [,659]       [,660]       [,661]\r\n[1,] -0.05931152 -0.172557011 0.04345829 -0.092010327  0.009216376\r\n[2,]  0.11495795  0.022532800 0.04255837  0.003888385 -0.017729213\r\n[3,]  0.10427144  0.003396329 0.08324926 -0.074082427 -0.050252728\r\n[4,]  0.26004377 -0.111642517 0.06373667 -0.034114234  0.046861809\r\n[5,]  0.17605624 -0.184327334 0.10711108 -0.005935872  0.078089602\r\n[6,] -0.01360951 -0.066098414 0.06643615 -0.034024406 -0.035054497\r\n           [,662]     [,663]      [,664]    [,665]      [,666]\r\n[1,]  0.026112964 0.04457451 -0.08195589 0.3652456  0.06814873\r\n[2,] -0.084683232 0.20700213 -0.08401885 0.2779030  0.07222094\r\n[3,] -0.041739639 0.16441514 -0.13062321 0.1978280  0.08606069\r\n[4,]  0.006917433 0.01766723  0.12383654 0.3698117  0.14708796\r\n[5,]  0.139725775 0.01032029 -0.12477722 0.8141871 -0.04375590\r\n[6,] -0.119117558 0.09434055  0.01547983 0.5833461  0.07563126\r\n          [,667]      [,668]     [,669]      [,670]       [,671]\r\n[1,] -0.18481112 -0.10735333 0.05807753 0.149984196 -0.023577670\r\n[2,] -0.05562754 -0.05690396 0.05784097 0.006915372  0.020710021\r\n[3,] -0.10250952 -0.09087744 0.01515261 0.026762316  0.006017368\r\n[4,] -0.14188011 -0.02832006 0.08038715 0.124325506 -0.021963054\r\n[5,] -0.15972200  0.05010274 0.12669383 0.088621408  0.195333421\r\n[6,] -0.06781326 -0.15493165 0.03824890 0.026009392  0.147379860\r\n          [,672]     [,673]     [,674]      [,675]      [,676]\r\n[1,] -0.09994625 0.05075474 0.13770406 -0.05091312 -0.04950824\r\n[2,] -0.09429576 0.31762967 0.09407511  0.03314918  0.04867224\r\n[3,] -0.10170484 0.17788664 0.01901637  0.07297327  0.03511119\r\n[4,] -0.10722709 0.13335045 0.16321246 -0.11654978  0.04190560\r\n[5,] -0.06281777 0.10059397 0.23034173 -0.08462723  0.04426360\r\n[6,] -0.19650432 0.24314374 0.04436253  0.06214135  0.04953858\r\n         [,677]      [,678]       [,679]     [,680]    [,681]\r\n[1,] 0.16101447  0.03261756  0.005240528 0.17729074 0.1418883\r\n[2,] 0.08751366 -0.07091376  0.044283517 0.04682508 0.1179714\r\n[3,] 0.05827320 -0.01815493  0.062092572 0.07742473 0.1240227\r\n[4,] 0.17231475 -0.07104249  0.001609321 0.15648434 0.1138151\r\n[5,] 0.16983619 -0.10041435 -0.032318529 0.12371835 0.1463693\r\n[6,] 0.17711435 -0.02248377  0.033749245 0.10109564 0.1119698\r\n           [,682]       [,683]      [,684]      [,685]      [,686]\r\n[1,] -0.049148161 -0.102843538 -0.06435385  0.03377604 -0.12738264\r\n[2,] -0.110841155  0.005716325 -0.10358587 -0.14668216 -0.06658430\r\n[3,] -0.062345993  0.025146997 -0.05186344 -0.11892558 -0.08773232\r\n[4,] -0.006465571 -0.008823811  0.01966340 -0.04686633 -0.08963613\r\n[5,] -0.018102204 -0.011661214 -0.05419477 -0.01541439 -0.02234121\r\n[6,] -0.046921309 -0.093011886 -0.03803139 -0.08654918 -0.10063978\r\n         [,687]      [,688]       [,689]      [,690]      [,691]\r\n[1,] 0.01920767 -0.01419010 -0.036548067  0.10385793 -0.11925840\r\n[2,] 0.04427815 -0.02267865 -0.003029384  0.27559093 -0.07172565\r\n[3,] 0.04829218 -0.10402574 -0.074470729  0.21307336 -0.04934324\r\n[4,] 0.12816328  0.01964043 -0.015710143 -0.05150509 -0.11027411\r\n[5,] 0.16518584 -0.06116838 -0.023354217  0.12244792 -0.17023247\r\n[6,] 0.05208472 -0.14492629 -0.004217244  0.20259228 -0.14674774\r\n          [,692]     [,693]      [,694]       [,695]       [,696]\r\n[1,]  0.12935558 0.08585732 -0.15259826 -0.064426281  0.009427316\r\n[2,]  0.03327186 0.10031641 -0.12604305 -0.060223628 -0.059971269\r\n[3,] -0.03249951 0.09358674 -0.02737276 -0.052089132 -0.034287207\r\n[4,]  0.10765537 0.04947363 -0.18106559 -0.174612015  0.017920753\r\n[5,]  0.05951141 0.13333648 -0.16659611 -0.220832214 -0.016421426\r\n[6,] -0.01870032 0.08445355 -0.05548840 -0.009819379  0.009299650\r\n         [,697]       [,698]      [,699]     [,700]      [,701]\r\n[1,] 0.07498431  0.008924695 -0.03596460 0.02562026  0.05723343\r\n[2,] 0.08413466 -0.003248119 -0.02388706 0.06124064  0.09032298\r\n[3,] 0.02059310  0.011218891 -0.03507020 0.03977913  0.02634757\r\n[4,] 0.08082996 -0.004628158 -0.07882575 0.19233127 -0.02707683\r\n[5,] 0.04945038  0.018265825  0.06390458 0.17153908  0.04357209\r\n[6,] 0.15937333  0.014907548 -0.03504714 0.01345458  0.01027399\r\n          [,702]     [,703]       [,704]        [,705]     [,706]\r\n[1,]  0.05042209 0.17513010  0.078270368  0.0009879635 0.07609536\r\n[2,]  0.01899258 0.10236545 -0.003082412  0.0433276631 0.03979125\r\n[3,] -0.01377380 0.09460923  0.039820436  0.0470335819 0.03629850\r\n[4,] -0.07509898 0.09751321  0.005451262 -0.0174586307 0.06780927\r\n[5,]  0.04036519 0.05327782 -0.020448197 -0.0222254954 0.04950605\r\n[6,]  0.00286444 0.20329627  0.061935090  0.0600249246 0.01128719\r\n         [,707]       [,708]      [,709]       [,710]       [,711]\r\n[1,] 0.08480427 -0.051488783 -0.15092826  0.040600438  0.021364078\r\n[2,] 0.11551305 -0.089709833 -0.09235039  0.054773681 -0.051684495\r\n[3,] 0.04133570 -0.007567647 -0.13295510  0.086913794 -0.010189838\r\n[4,] 0.13612410 -0.022304114 -0.14579950  0.052552368  0.006682473\r\n[5,] 0.13363947 -0.084428973 -0.14787699 -0.033316150 -0.034028500\r\n[6,] 0.10874654 -0.010607310 -0.07875548 -0.002926263  0.017746029\r\n           [,712]     [,713]      [,714]      [,715]       [,716]\r\n[1,]  0.055909742 0.13373113 0.010450879 -0.07233977  0.049532827\r\n[2,]  0.044699516 0.05596512 0.017389840 -0.01976865 -0.060050644\r\n[3,] -0.003019668 0.09866080 0.008203694 -0.04194739  0.033643007\r\n[4,]  0.106271528 0.10256605 0.050474018 -0.10010678 -0.096137635\r\n[5,]  0.134674683 0.03579661 0.262081206 -0.02019673 -0.058704950\r\n[6,]  0.018991491 0.07984170 0.046879679 -0.13486291 -0.004231149\r\n          [,717]      [,718]      [,719]     [,720]    [,721]\r\n[1,] -0.15207241  0.01589682 -0.07201874 0.12655738 0.2714218\r\n[2,] -0.14243512 -0.06132271 -0.08397345 0.05954942 0.2686446\r\n[3,] -0.11367873  0.01625160 -0.06947573 0.13009270 0.2074144\r\n[4,] -0.01131313  0.01650861 -0.16232024 0.12199844 0.3521203\r\n[5,] -0.20922038  0.07140674  0.07200902 0.14064504 0.2851988\r\n[6,] -0.18501478  0.01336365 -0.18002975 0.08586299 0.2361627\r\n          [,722]    [,723]       [,724]     [,725]     [,726]\r\n[1,] -0.04793752 0.1239760 -0.042619105 0.07675026 0.04989492\r\n[2,] -0.03580271 0.2204790  0.028894657 0.07819990 0.03738726\r\n[3,] -0.04341270 0.1997167  0.009197655 0.10368587 0.06880984\r\n[4,] -0.01400930 0.1125405 -0.013918034 0.10096098 0.07105603\r\n[5,] -0.03376498 0.2289448 -0.006609058 0.08865908 0.02157804\r\n[6,] -0.09421291 0.2373826 -0.087017901 0.13768604 0.07877830\r\n         [,727]      [,728]      [,729]        [,730]       [,731]\r\n[1,] 0.11111774  0.12066169 -0.08823191 -0.0968144462 -0.003828065\r\n[2,] 0.09153498  0.01680492  0.05834011 -0.1283439696 -0.086855493\r\n[3,] 0.02735950  0.01842253  0.05487081 -0.0598209202 -0.041060273\r\n[4,] 0.15378579  0.08465433 -0.09540128 -0.0794036761 -0.066965066\r\n[5,] 0.22088622  0.12059090 -0.18659893 -0.0002434805  0.124148265\r\n[6,] 0.01092780 -0.05189246 -0.05732826 -0.0068929754 -0.059818584\r\n         [,732]       [,733]      [,734]      [,735]     [,736]\r\n[1,] -1.0205674 -0.012650539 -0.07955996 -0.19061120 0.24889600\r\n[2,] -1.6376132 -0.027073890 -0.11495793  0.02188260 0.27936694\r\n[3,] -1.3168830  0.028417423 -0.15257415  0.01262438 0.11415002\r\n[4,] -0.8893182 -0.003889868 -0.03194398 -0.10278581 0.14121774\r\n[5,] -1.3300040 -0.001954583 -0.11487593 -0.01644935 0.06424145\r\n[6,] -1.0931637  0.005661166 -0.04133619 -0.03086518 0.11152110\r\n           [,737]      [,738]      [,739]       [,740]       [,741]\r\n[1,] -0.003787148 -0.02626314 -0.05684900 -0.030116322  0.019574083\r\n[2,]  0.031762563 -0.19462261  0.03391609 -0.065266207  0.010448777\r\n[3,] -0.028514821 -0.09818638  0.05229101 -0.056107655 -0.002290886\r\n[4,] -0.013125353 -0.09966719 -0.01316734 -0.062725045  0.047559965\r\n[5,]  0.006487907 -0.21648905 -0.03054102 -0.003993882 -0.035927922\r\n[6,]  0.143368512 -0.06219042  0.01643150  0.027760662  0.167845950\r\n           [,742]      [,743]     [,744]      [,745]     [,746]\r\n[1,] -0.006300187  0.11072528 0.06599118 0.104969658 0.11591630\r\n[2,]  0.039824650  0.11417527 0.07130589 0.086033270 0.07507633\r\n[3,] -0.046479836  0.07696974 0.11749076 0.069948621 0.07018101\r\n[4,]  0.001496303  0.11861439 0.06500278 0.093097188 0.08963138\r\n[5,] -0.009119453 -0.01157343 0.12341224 0.069648638 0.06475202\r\n[6,] -0.055967491  0.04665795 0.05940204 0.004410517 0.11607914\r\n           [,747]      [,748]      [,749]    [,750]     [,751]\r\n[1,] -0.005869865 -0.09371068 -0.07998058 0.5072689 0.15181084\r\n[2,] -0.013162416 -0.08033861 -0.08842883 0.4089560 0.18010509\r\n[3,] -0.030671582 -0.08326717 -0.20276427 0.3482460 0.08906674\r\n[4,] -0.036494512 -0.02704215  0.01600152 0.2208505 0.13019434\r\n[5,]  0.059290923 -0.02641463 -0.26967326 0.1779452 0.25794142\r\n[6,]  0.017716359 -0.08491317 -0.08338597 0.4885524 0.10065948\r\n         [,752]     [,753]       [,754]       [,755]       [,756]\r\n[1,] -0.2158261 0.05548894  0.006289285 -0.021548092 -0.012992766\r\n[2,] -0.1794356 0.16941381  0.060302734  0.026785025  0.017900588\r\n[3,] -0.2917365 0.14918669  0.006558592  0.059029546  0.060162686\r\n[4,] -0.2923160 0.05454070  0.049739074 -0.055772901  0.020266797\r\n[5,] -0.3163047 0.04446334 -0.182113379 -0.017461553 -0.096357614\r\n[6,] -0.2533574 0.23800616  0.053148732  0.007075642  0.005899848\r\n           [,757]     [,758]      [,759]      [,760]       [,761]\r\n[1,] -0.004315879 0.08018567 -0.16251971 -0.12237902 -0.099664904\r\n[2,]  0.021114955 0.01345702 -0.12080519 -0.06235048  0.159044981\r\n[3,]  0.024573060 0.07193569 -0.06111015 -0.04808743 -0.002424658\r\n[4,]  0.051165286 0.05130168 -0.16208030 -0.12310728 -0.090935059\r\n[5,] -0.124562979 0.13982485 -0.18622497 -0.12187486 -0.046656270\r\n[6,]  0.037395142 0.01761835 -0.05774543 -0.06053047  0.041841812\r\n          [,762]    [,763]    [,764]      [,765]        [,766]\r\n[1,] -0.06233726 0.1447253 0.1683183 -0.19668962  0.0165074896\r\n[2,] -0.04395547 0.2787576 0.1744189 -0.01972667  0.1150709465\r\n[3,] -0.01668818 0.1851185 0.1324145  0.05224041 -0.0654691830\r\n[4,]  0.02574161 0.1451475 0.1177684  0.06272639 -0.0085238395\r\n[5,]  0.03204844 0.1277810 0.1642708 -0.14526126  0.0274618156\r\n[6,] -0.07035222 0.1358655 0.1458258 -0.19299366  0.0009176398\r\n           [,767]      [,768]\r\n[1,]  0.067714356 -0.02958016\r\n[2,] -0.040427078  0.03495659\r\n[3,]  0.001104798  0.03656437\r\n[4,]  0.010757907 -0.03644707\r\n[5,]  0.004185256  0.02862171\r\n[6,] -0.015431712  0.02789298\r\n\r\nIt took about 51 minutes on my computer. So, you may want to export\r\nthese embeddings as an external file to save time. It is not a good idea\r\nto run this repeatedly every time we need these embeddings. So, you run\r\nit first and then export this embedding matrix as a .csv file. Whenever\r\nwe need this matrix, we can just read them into our R environment\r\ninstead of re-doing this analysis. Below, I am appending the target\r\noutcome to the embedding matrix, and then exporting them all as a .csv\r\nfile.\r\n\r\n\r\nread.embeddings <- as.data.frame(read.embeddings)\r\nread.embeddings$target <- readability$target\r\n\r\n\r\nwrite.csv(read.embeddings, \r\n          here('data/readability_features.csv'),\r\n          row.names = FALSE)\r\n\r\n\r\nNote that it takes significantly less time to compute sentence\r\nembeddings if you have access to computational resources with GPU. For\r\ninstance, if you run the same analysis above on a Kaggle notebook with\r\nGPU turned on, it only takes 70 seconds. So, it is important to use GPU\r\nresources when using these models if you have access. You can check the\r\nassociated Kaggle notebook for the analysis in this post.\r\nhttps://www.kaggle.com/code/uocoeeds/lecture-2b-data-preprocessing-ii\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:04:50-08:00"
    },
    {
      "path": "lecture-3a.html",
      "title": "An Overview of the Linear Regression",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "10/04/2022",
      "contents": "\r\n\r\nContents\r\nModel\r\nDescription\r\nModel\r\nEstimation\r\nMatrix\r\nSolution\r\nlm()\r\nfunction\r\n\r\nPerformance\r\nEvaluation for Models Predicting a Continuous Outcome\r\nAccuracy\r\nReduction in Total Amount of\r\nError\r\n\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:04:50 ]\r\nThe prediction algorithms are classified into two main categories in\r\nthe machine learning literature: supervised and\r\nunsupervised. Supervised algorithms are used when the dataset\r\nhas an actual outcome of interest to predict (labels), and the goal is\r\nto build the “best” model predicting the outcome of interest. On the\r\nother side, unsupervised algorithms are used when the dataset doesn’t\r\nhave an outcome of interest. The goal is typically to identify similar\r\ngroups of observations (rows of data) or similar groups of variables\r\n(columns of data) in data. In this course, we plan to cover several\r\nsupervised algorithms. Linear regression is one of the most\r\nstraightforward approaches among supervised algorithms and the easiest\r\nto interpret.\r\nModel Description\r\nIn most general terms, the linear regression model with \\(P\\) predictors (\\(X_1\\),\\(X_2\\),\\(X_3\\),…,\\(X_p\\)) to predict an outcome (\\(Y\\)) can be written as the following:\r\n\\[ Y = \\beta_0  + \\sum_{p=1}^{P}\r\n\\beta_pX_{p} + \\epsilon.\\] In this model, \\(Y\\) represents the observed value for the\r\noutcome of an observation, \\(X_{p}\\)\r\nrepresents the observed value of the \\(p^{th}\\) variable for the same observation,\r\nand \\(\\beta_p\\) is the associated model\r\nparameter for the \\(p^{th}\\) variable.\r\n\\(\\epsilon\\) is the model error\r\n(residual) for the observation.\r\nThis model includes only the main effects of each predictor and can\r\nbe easily extended by including quadratic or higher-order polynomial\r\nterms for all (or a specific subset of) predictors. For instance, the\r\nmodel below includes all first-order, second-order, and third-order\r\npolynomial terms for all predictors.\r\n\\[ Y = \\beta_0  + \\sum_{p=1}^{P}\r\n\\beta_pX_{p} + \\sum_{k=1}^{P} \\beta_{k+P}X_{k}^2 + \\sum_{m=1}^{P}\r\n\\beta_{m+2P}X_{m}^3 + \\epsilon.\\] Sometimes, the effect of\r\npredictor variables on the outcome variable is not additive, and the\r\neffect of one predictor on the response variable can depend on the\r\nlevels of another predictor. These non-additive effects are also called\r\ninteraction effects. The interaction effects can also be a first-order\r\ninteraction (interaction between two variables, e.g., \\(X_1*X_2\\)), second-order interaction (\\(X_1*X_2*X_3\\)), or higher orders. It is\r\nalso possible to add the interaction effects to the model. For instance,\r\nthe model below also adds the first-order interactions.\r\n\\[ Y = \\beta_0  + \\sum_{p=1}^{P}\r\n\\beta_pX_{p} + \\sum_{k=1}^{P} \\beta_{k+P}X_{k}^2 + \\sum_{m=1}^{P}\r\n\\beta_{m+2P}X_{m}^3 + \\sum_{i=1}^{P}\\sum_{j=i+1}^{P}\\beta_{i,j}X_iX_j +\r\n\\epsilon.\\] If you are uncomfortable or confused with notational\r\nrepresentation, below is an example of different models you can write\r\nwith three predictors (\\(X_1,X_2,X_3\\)).\r\nA model with only main effects:\r\n\\[ Y = \\beta_0  + \\beta_1X_{1} +\r\n\\beta_2X_{2} + \\beta_3X_{3}+ \\epsilon.\\]\r\nA model with polynomial terms up to the 3rd degree was added:\r\n\\[Y = \\beta_0  + \\beta_1X_1 + \\beta_2X_2 +\r\n\\beta_3X_3 + \\\\ \\beta_4X_1^2 + \\beta_5X_2^2 + \\beta_6X_3^2+ \\\\\r\n\\beta_{7}X_1^3 + \\beta_{8}X_2^3 + \\beta_{9}X_3^3\\]\r\nA model with both interaction terms and polynomial terms up to the\r\n3rd degree was added:\r\n\\[Y = \\beta_0  + \\beta_1X_1 + \\beta_2X_2 +\r\n\\beta_3X_3 + \\\\ \\beta_4X_1^2 + \\beta_5X_2^2 + \\beta_6X_3^2+ \\\\\r\n\\beta_{7}X_1^3 + \\beta_{8}X_2^3 + \\beta_{9}X_3^3+ \\\\ \\beta_{1,2}X_1X_2+\r\n\\beta_{1,3}X_1X_3 + \\beta_{2,3}X_2X_3 + \\epsilon\\]\r\nModel Estimation\r\nSuppose that we would like to predict the target readability score\r\nfor a given text from the Feature 220 (there are 768 features extracted\r\nfrom the NLP model as numerical embeddings). Below is a scatterplot to\r\nshow the relationship between these two variables for a random sample of\r\n20 observations. There seems to be a moderate positive correlation. So,\r\nwe can tell that the higher the score for Feature 220 is for a given\r\ntext, the higher the readability score (more challenging to read).\r\n\r\n\r\nreadability_sub <- read.csv('./data/readability_sub.csv',header=TRUE)\r\n\r\n\r\n\r\n\r\n\r\nLet’s consider a simple linear regression model: the readability\r\nscore is the outcome (\\(Y\\)), and\r\nFeature 220 is the predictor(\\(X\\)).\r\nOur regression model would be \\[Y =\r\n\\beta_0  + \\beta_1X + \\epsilon.\\]\r\nIn this case, the set of coefficients, {\\(\\beta_0,\\beta_1\\)}, represents a linear\r\nline. We can write any set of {\\(\\beta_0,\\beta_1\\)} coefficients and use it\r\nas our model. For instance, suppose I guesstimate that these\r\ncoefficients are {\\(\\beta_0,\\beta_1\\)}\r\n= {-1.5,2}. Then, my model would be\r\n\\[Y = -1.5  + 2X + \\epsilon.\\]\r\n\r\n\r\n\r\nUsing this model, I can predict the target readability score for any\r\nobservation in my dataset. For instance, Feature 220 is -.139 for the\r\nfirst reading passage. Then, my prediction of the readability score\r\nbased on this model would be -1.778. On the other side, the observed\r\nvalue of the readability score for this observation is -2.062. This\r\ndiscrepancy between the observed value and the model prediction is the\r\nmodel error (residual) for the first observation and captured in the\r\n\\(\\epsilon\\) term in the model.\r\n\\[Y_{(1)} = -1.5  + 2X_{(1)} +\r\n\\epsilon_{(1)}.\\] \\[\\hat{Y}_{(1)}\r\n=  -1.5 + 2*(-0.139) = -1.778\\] \\[\\hat{\\epsilon}_{(1)} = -2.062 - (-1.778)\r\n=  -0.284 \\] We can visualize this in the plot. The black dot\r\nrepresents the observed data point, and the blue dot on the line\r\nrepresents the model prediction for a given \\(X\\) value. The vertical distance between\r\nthese two data points is this observation’s model error.\r\n\r\n\r\n\r\nWe can do the same thing for the second observation. Feature 220 is\r\nequal to 0.218 for the second reading passage. The model predicts a\r\nreadability score of -1.065. The observed value of the readability score\r\nfor this observation is 0.583. Therefore the model error for the second\r\nobservation would be 1.648.\r\n\\[Y_{(2)} = -1.5  + 2X_{(2)} +\r\n\\epsilon_{(2)}.\\] \\[\\hat{Y}_{(2)}\r\n=  -1.5 + 2*(0.218) = -1.065\\] \\[\\hat{\\epsilon}_{(2)} = 0.583 - (-1.065) =  1.648\r\n\\]\r\n\r\n\r\n\r\nUsing a similar approach, we can calculate the model error for every\r\nobservation.\r\n\r\n\r\nd <-  readability_sub[,c('V220','target')]\r\n\r\nd$predicted <- -1.5 + 2*d$V220\r\nd$error     <- d$target - d$predicted\r\n\r\nd\r\n\r\n          V220      target  predicted       error\r\n1  -0.13908258 -2.06282395 -1.7781652 -0.28465879\r\n2   0.21764143  0.58258607 -1.0647171  1.64730321\r\n3   0.05812133 -1.65313060 -1.3837573 -0.26937327\r\n4   0.02526429 -0.87390681 -1.4494714  0.57556460\r\n5   0.22430885 -1.74049148 -1.0513823 -0.68910918\r\n6  -0.07795373 -3.63993555 -1.6559075 -1.98402809\r\n7   0.43400714 -0.62284268 -0.6319857  0.00914304\r\n8  -0.24364550 -0.34426981 -1.9872910  1.64302120\r\n9   0.15893717 -1.12298826 -1.1821257  0.05913740\r\n10  0.14496475 -0.99857142 -1.2100705  0.21149908\r\n11  0.34222975 -0.87656742 -0.8155405 -0.06102693\r\n12  0.25219145 -0.03304643 -0.9956171  0.96257066\r\n13  0.03532625 -0.49529863 -1.4293475  0.93404886\r\n14  0.36410633  0.12453660 -0.7717873  0.89632394\r\n15  0.29988593  0.09678258 -0.9002281  0.99701073\r\n16  0.19837037  0.38422270 -1.1032593  1.48748196\r\n17  0.07807041 -0.58143038 -1.3438592  0.76242880\r\n18  0.07935690 -0.34324576 -1.3412862  0.99804044\r\n19  0.57000953 -0.39054205 -0.3599809 -0.03056111\r\n20  0.34523284 -0.67548411 -0.8095343  0.13405021\r\n\r\n\r\n\r\n\r\nWhile it is helpful to see the model error for every observation, we\r\nwill need to aggregate them in some way to form an overall measure of\r\nthe total amount of error for this model. Some alternatives for\r\naggregating these individual errors could be using\r\nthe sum of the residuals (SR),\r\nthe sum of the absolute value of residuals (SAR), or\r\nthe sum of squared residuals (SSR)\r\nAmong these alternatives, (a) is not a helpful aggregation as the\r\npositive and negative residuals will cancel each other, and (a) may\r\nmisrepresent the total amount of error for all observations. Both (b)\r\nand (c) are plausible alternatives and can be used. On the other hand,\r\n(b) is less desirable because the absolute values are mathematically\r\nmore challenging to deal with (ask a calculus professor!). So, (c) seems\r\nto be a good way of aggregating the total amount of error, and it is\r\nmathematically easier to work with. We can show (c) in a mathematical\r\nnotation as the following.\r\n\\[SSR = \\sum_{i=1}^{N}(Y_{(i)} -\r\n(\\beta_0+\\beta_1X_{(i)}))^2\\] \\[SSR =\r\n\\sum_{i=1}^{N}(Y_{(i)} - \\hat{Y_{(i)}})^2\\] \\[SSR = \\sum_{i=1}^{N}\\epsilon_{(i)}^2\\]\r\nFor our model, the sum of squared residuals would be 17.767.\r\n\r\n\r\nsum(d$error^2)\r\n\r\n[1] 17.76657\r\n\r\nNow, how do we know that the set of coefficients we guesstimate,\r\n{\\(\\beta_0,\\beta_1\\)} = {-1.5,2}, is a\r\ngood model? Is there any other set of coefficients that would provide\r\nless error than this model? The only way of knowing this is to try a\r\nbunch of different models and see if we can find a better one that gives\r\nus better predictions (smaller residuals). But, there are infinite pairs\r\nof {\\(\\beta_0,\\beta_1\\)} coefficients,\r\nso which ones should we try?\r\nBelow, I will do a quick exploration. For instance, suppose the\r\npotential range for my intercept (\\(\\beta_0\\)) is from -10 to 10. I will\r\nconsider every single possible value from -10 t 10 with increments of\r\n.1. Also, suppose the potential range for my slope (\\(\\beta_1\\)) is from -5 to 5. I will consider\r\nevery single possible value from -5 to 5 with increments of .01. Given\r\nthat every single combination of \\(\\beta_0\\) and \\(\\beta_1\\) indicates a different model,\r\nthese settings suggest a total of 201,201 models to explore. If you are\r\ncrazy enough, you can try every single model and compute the SSR. Then,\r\nwe can plot them in a 3D by putting \\(\\beta_0\\) on the X-axis, \\(\\beta_1\\) on the Y-axis, and SSR on the\r\nZ-axis. Check the plot below and tell me if you can explore and find the\r\nminimum of this surface.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFinding the best set of {\\(\\beta_0,\\beta_1\\)} coefficients that\r\nminimize the sum of squared residuals is an optimization problem. For\r\nany optimization problem, there is a loss function we\r\neither try to minimize or maximize. In this case, our loss function is\r\nthe sum of squared residuals.\r\n\\[Loss = \\sum_{i=1}^{N}(Y_{(i)} -\r\n(\\beta_0+\\beta_1X_{(i)}))^2\\] In this loss function, \\(X\\) and \\(Y\\) values are observed data, and {\\(\\beta_0,\\beta_1\\)} are unknown parameters.\r\nOptimization aims to find the set {\\(\\beta_0,\\beta_1\\)} coefficients that\r\nprovide the minimum value of this function. Once the minimum of this\r\nfunction is found, we can argue that the corresponding coefficients are\r\nour best solution for the regression model.\r\nIn this case, this is a good-looking surface with a single global\r\nminimum, and it is not difficult to find the minimum of this loss\r\nfunction. We also have an analytical solution to find its minima because\r\nof its simplicity. Most of the time, the optimization problems are more\r\ncomplex, and we solve them using numerical techniques such as steepest\r\nascent (or descent), Newton-Raphson, Quasi-Newton, Genetic Algorithm,\r\nand many more.\r\nMatrix Solution\r\nWe can find the best set of coefficients for most regression problems\r\nwith a simple matrix operation. Let’s first see how we can represent the\r\nregression problem in matrix form. Suppose I wrote the regression model\r\npresented in the earlier section for every observation in a dataset with\r\na sample size of N.\r\n\\[Y_{(1)} = \\beta_0  + \\beta_1X_{(1)} +\r\n\\epsilon_{(1)}.\\]\r\n\\[Y_{(2)} = \\beta_0  + \\beta_1X_{(2)} +\r\n\\epsilon_{(2)}.\\] \\[Y_{(3)} =\r\n\\beta_0  + \\beta_1X_{(3)} + \\epsilon_{(3)}.\\] \\[...\\] \\[...\\] \\[...\\] \\[Y_{(20)} = \\beta_0  + \\beta_1X_{(20)} +\r\n\\epsilon_{(20)}.\\]\r\nWe can write all of these equations in a much simpler format as\r\n\\[ \\mathbf{Y} = \\mathbf{X}\r\n\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\]\r\nsuch that \\(\\mathbf{Y}\\) is an N x 1\r\ncolumn vector of observed values for the outcome variable, \\(\\mathbf{X}\\) is an N x (P+1) design\r\nmatrix for the set of predictor variables, including an\r\nintercept term, and \\(\\boldsymbol{\\beta}\\) is a (P+1) x 1 column\r\nvector of regression coefficients, and \\(\\boldsymbol{\\epsilon}\\) is an N x 1 column\r\nvector of residuals. These matrix elements would look like the following\r\nfor the problem above with our small dataset.\r\n\r\n\r\n\r\nOr, more specifically, we can replace the observed values of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) with the corresponding\r\nelements.\r\n\r\n\r\n\r\nIt can be shown that the set of {\\(\\beta_0,\\beta_1\\)} coefficients that yields\r\nthe minimum sum of squared residuals for this model can be analytically\r\nfound using the following matrix operation.\r\n\\[\\hat{\\boldsymbol{\\beta}} =\r\n(\\mathbf{X^T}\\mathbf{X})^{-1}\\mathbf{X^T}\\mathbf{Y}\\]\r\nSuppose we apply this matrix operation to our small datasets. In that\r\ncase, we will find that the best set of {\\(\\beta_0,\\beta_1\\)} coefficients to predict\r\nthe readability score with the least amount of error using Feature 220\r\nas a predictor is {\\(\\beta_0,\\beta_1\\)}\r\n= {-1.108,2.049}. These estimates are also known as the least\r\nsquare estimates, and the best linear unbiased estimators\r\n(BLUE) for the given regression model.\r\n\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(1,readability_sub$V220))\r\n\r\nY\r\n\r\n             [,1]\r\n [1,] -2.06282395\r\n [2,]  0.58258607\r\n [3,] -1.65313060\r\n [4,] -0.87390681\r\n [5,] -1.74049148\r\n [6,] -3.63993555\r\n [7,] -0.62284268\r\n [8,] -0.34426981\r\n [9,] -1.12298826\r\n[10,] -0.99857142\r\n[11,] -0.87656742\r\n[12,] -0.03304643\r\n[13,] -0.49529863\r\n[14,]  0.12453660\r\n[15,]  0.09678258\r\n[16,]  0.38422270\r\n[17,] -0.58143038\r\n[18,] -0.34324576\r\n[19,] -0.39054205\r\n[20,] -0.67548411\r\n\r\nX\r\n\r\n      [,1]        [,2]\r\n [1,]    1 -0.13908258\r\n [2,]    1  0.21764143\r\n [3,]    1  0.05812133\r\n [4,]    1  0.02526429\r\n [5,]    1  0.22430885\r\n [6,]    1 -0.07795373\r\n [7,]    1  0.43400714\r\n [8,]    1 -0.24364550\r\n [9,]    1  0.15893717\r\n[10,]    1  0.14496475\r\n[11,]    1  0.34222975\r\n[12,]    1  0.25219145\r\n[13,]    1  0.03532625\r\n[14,]    1  0.36410633\r\n[15,]    1  0.29988593\r\n[16,]    1  0.19837037\r\n[17,]    1  0.07807041\r\n[18,]    1  0.07935690\r\n[19,]    1  0.57000953\r\n[20,]    1  0.34523284\r\n\r\nbeta <- solve(t(X)%*%X)%*%t(X)%*%Y\r\n\r\nbeta \r\n\r\n          [,1]\r\n[1,] -1.108295\r\n[2,]  2.048931\r\n\r\nOnce we find the best estimates for the model coefficients, we can\r\nalso calculate the model predicted values and residual sum of squares\r\nfor the given model and dataset.\r\n\\[\\boldsymbol{\\hat{Y}} = \\mathbf{X}\r\n\\hat{\\boldsymbol{\\beta}} \\]\r\n\\[ \\boldsymbol{\\hat{\\epsilon}} =\r\n\\boldsymbol{Y} - \\hat{\\boldsymbol{Y}} \\] \\[ SSR = \\boldsymbol{\\hat{\\epsilon}^T}\r\n\\boldsymbol{\\hat{\\epsilon}} \\]\r\n\r\n\r\nY_hat <-  X%*%beta\r\n\r\nY_hat\r\n\r\n             [,1]\r\n [1,] -1.39326572\r\n [2,] -0.66236275\r\n [3,] -0.98920846\r\n [4,] -1.05653027\r\n [5,] -0.64870167\r\n [6,] -1.26801692\r\n [7,] -0.21904429\r\n [8,] -1.60750797\r\n [9,] -0.78264374\r\n[10,] -0.81127226\r\n[11,] -0.40708984\r\n[12,] -0.59157213\r\n[13,] -1.03591401\r\n[14,] -0.36226624\r\n[15,] -0.49384943\r\n[16,] -0.70184783\r\n[17,] -0.94833418\r\n[18,] -0.94569824\r\n[19,]  0.05961526\r\n[20,] -0.40093671\r\n\r\nE <- Y - Y_hat\r\nE\r\n\r\n            [,1]\r\n [1,] -0.6695582\r\n [2,]  1.2449488\r\n [3,] -0.6639221\r\n [4,]  0.1826235\r\n [5,] -1.0917898\r\n [6,] -2.3719186\r\n [7,] -0.4037984\r\n [8,]  1.2632382\r\n [9,] -0.3403445\r\n[10,] -0.1872992\r\n[11,] -0.4694776\r\n[12,]  0.5585257\r\n[13,]  0.5406154\r\n[14,]  0.4868028\r\n[15,]  0.5906320\r\n[16,]  1.0860705\r\n[17,]  0.3669038\r\n[18,]  0.6024525\r\n[19,] -0.4501573\r\n[20,] -0.2745474\r\n\r\nSRR <- t(E)%*%E\r\n\r\nSRR\r\n\r\n         [,1]\r\n[1,] 14.56567\r\n\r\nNote that the matrix formulation is generalized to a regression model\r\nfor more than one predictor. When there are more predictors in the\r\nmodel, the dimensions of the design matrix (\\(\\mathbf{X}\\)) and regression coefficient\r\nmatrix (\\(\\boldsymbol{\\beta}\\)) will be\r\ndifferent, but the matrix calculations will be identical. It isn’t easy\r\nto visualize the surface we are trying to minimize beyond two\r\ncoefficients. Still, we know that the matrix solution will always\r\nprovide us with the set of coefficients that yields the least amount of\r\nerror in our predictions.\r\nLet’s assume that we would like to expand our model by adding another\r\npredictor, Feature 166 as the second predictor. Our new model will\r\nbe\r\n\\[Y_{(i)} = \\beta_0  + \\beta_1X_{1(i)} +\r\n\\beta_2X_{2(i)} + \\epsilon_{(i)}.\\] Note that I added a subscript\r\nfor \\(X\\) to differentiate different\r\npredictors. Let’s say \\(X_1\\)\r\nrepresents Feature 220 and \\(X_2\\)\r\nrepresents Feature 166. Now, we are looking for the best set of three\r\ncoefficients, {\\(\\beta_0,\\beta_1,\r\n\\beta_2\\)} that would yield the least error in predicting the\r\nreadability. Now, our matrix elements will look like the following:\r\n\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(1,readability_sub[,c('V220','V166')]))\r\n\r\nY\r\n\r\n             [,1]\r\n [1,] -2.06282395\r\n [2,]  0.58258607\r\n [3,] -1.65313060\r\n [4,] -0.87390681\r\n [5,] -1.74049148\r\n [6,] -3.63993555\r\n [7,] -0.62284268\r\n [8,] -0.34426981\r\n [9,] -1.12298826\r\n[10,] -0.99857142\r\n[11,] -0.87656742\r\n[12,] -0.03304643\r\n[13,] -0.49529863\r\n[14,]  0.12453660\r\n[15,]  0.09678258\r\n[16,]  0.38422270\r\n[17,] -0.58143038\r\n[18,] -0.34324576\r\n[19,] -0.39054205\r\n[20,] -0.67548411\r\n\r\nX\r\n\r\n      1        V220        V166\r\n [1,] 1 -0.13908258  0.19028091\r\n [2,] 1  0.21764143  0.07101288\r\n [3,] 1  0.05812133  0.03993277\r\n [4,] 1  0.02526429  0.18845809\r\n [5,] 1  0.22430885  0.06200715\r\n [6,] 1 -0.07795373  0.10754109\r\n [7,] 1  0.43400714  0.12202360\r\n [8,] 1 -0.24364550  0.02454670\r\n [9,] 1  0.15893717  0.10422343\r\n[10,] 1  0.14496475  0.02339597\r\n[11,] 1  0.34222975  0.22065343\r\n[12,] 1  0.25219145  0.10865010\r\n[13,] 1  0.03532625  0.07549474\r\n[14,] 1  0.36410633  0.18675801\r\n[15,] 1  0.29988593  0.11618323\r\n[16,] 1  0.19837037  0.08272671\r\n[17,] 1  0.07807041  0.10235218\r\n[18,] 1  0.07935690  0.11618605\r\n[19,] 1  0.57000953 -0.02385423\r\n[20,] 1  0.34523284  0.09299514\r\n\r\nWe will get the following estimates for {\\(\\beta_0,\\beta_1, \\beta_2\\)} =\r\n{-1.007,2.036,-0.988} yielding a value of 14.495 for the residual sum of\r\nsquares.\r\n\r\n\r\nbeta <- solve(t(X)%*%X)%*%t(X)%*%Y\r\n\r\nbeta \r\n\r\n           [,1]\r\n1    -1.0068344\r\nV220  2.0363683\r\nV166 -0.9877414\r\n\r\nY_hat <-  X%*%beta\r\n\r\nE <- Y - Y_hat\r\n\r\nSSR <- t(E)%*%E\r\n\r\nSSR\r\n\r\n         [,1]\r\n[1,] 14.49461\r\n\r\nlm() function\r\nWhile learning the inner mechanics of how numbers work behind the\r\nscenes is always exciting, it is handy to use already existing packages\r\nand tools to do all these computations. A simple go-to function for\r\nfitting a linear regression to predict a continuous outcome is the\r\nlm() function.\r\nLet’s fit the models we discussed in the earlier section using the\r\nlm() function and see if we get the same regression\r\ncoefficients.\r\nModel 1: Predicting readability scores from Feature 220\r\n\r\n\r\n\r\nmod <- lm(target ~ 1 + V220,data=readability_sub)\r\n\r\nsummary(mod)\r\n\r\n\r\nCall:\r\nlm(formula = target ~ 1 + V220, data = readability_sub)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-2.37192 -0.45499 -0.00234  0.56655  1.26324 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)  -1.1083     0.2662  -4.163 0.000584 ***\r\nV220          2.0489     1.0356   1.978 0.063390 .  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.8996 on 18 degrees of freedom\r\nMultiple R-squared:  0.1786,    Adjusted R-squared:  0.133 \r\nF-statistic: 3.914 on 1 and 18 DF,  p-value: 0.06339\r\n\r\nIn the Coefficients table, the numbers under the\r\nEstimate column are the estimated regression\r\ncoefficients, identical to the numbers we obtained before using matrix\r\ncalculations. We ignore the other numbers in this table since our focus\r\nin this class is not significance testing. Another number in this table\r\nis Residual Standard Error (RSE), and this number is\r\ndirectly related to the Sum of Squared Residuals (SSR) for this model.\r\nNote that we obtained a value of 14.566 for SSR when we fitted the\r\nmodel. The relationship between SSR and RSE is\r\n\\[RSE = \\sqrt{\\frac{SSR}{df_{regression}}}\r\n= \\sqrt{\\frac{SSR}{N-k}}, \\]\r\nwhere the degrees of freedom for the regression model, in this case,\r\nare equal to the difference between the number of observations (\\(N\\)) and the number of coefficients in the\r\nmodel (\\(k\\)).\r\n\\[RSE = \\sqrt{\\frac{14.566}{20-2}} =\r\n0.8996.\\]\r\nRSE is a measure that summarizes the amount of uncertainty for\r\nindividual predictions. Another relevant number reported is the\r\nR-squared (0.2596), which is the square of the correlation between\r\npredicted and observed values.\r\nModel 2: Predicting readability scores from Feature 220 and\r\nFeature 166\r\n\r\n\r\nmod <- lm(target ~ 1 + V220 + V166,data=readability_sub)\r\n\r\nsummary(mod)\r\n\r\n\r\nCall:\r\nlm(formula = target ~ 1 + V220 + V166, data = readability_sub)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2.3681 -0.4265  0.0019  0.5827  1.2164 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept)  -1.0068     0.4452  -2.262   0.0371 *\r\nV220          2.0364     1.0639   1.914   0.0726 .\r\nV166         -0.9877     3.4214  -0.289   0.7763  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.9234 on 17 degrees of freedom\r\nMultiple R-squared:  0.1826,    Adjusted R-squared:  0.08646 \r\nF-statistic: 1.899 on 2 and 17 DF,  p-value: 0.1801\r\n\r\nPerformance\r\nEvaluation for Models Predicting a Continuous Outcome\r\nAccuracy\r\nA family of metrics commonly used for models predicting a continuous\r\noutcome is related to the accuracy of predictions. These metrics have a\r\nsingle most important unit in our predictions, the prediction error for\r\na single observation. Error (a.k.a. residual) for an observation is\r\ndefined as the difference between the observed value and the model\r\npredicted value for the outcome.\r\n\\[e_i = y_i - \\hat{y_i}\\]\r\nBelow is a list of commonly used metrics that are functions of\r\nprediction error.\r\nMean Absolute Error(MAE)\r\n\\[ MAE = \\frac{\\sum_{i=1}^{N} \\left | e_i\r\n\\right |}{N}\\]\r\nMean Squared Error(MSE)\r\n\\[ MSE = \\frac{\\sum_{i=1}^{N}\r\ne_i^{2}}{N}\\]\r\nRoot Mean Squared Error (RMSE)\r\n\\[ RMSE = \\sqrt{\\frac{\\sum_{i=1}^{N}\r\ne_i^{2}}{N}}\\]\r\nIf we take predictions from the second model, we can calculate these\r\nperformance metrics for the small demo dataset using the following\r\ncode.\r\n\r\n\r\nmod <- lm(target ~ 1 + V220 + V166,data=readability_sub)\r\n\r\nreadability_sub$pred <- predict(mod)\r\n\r\nreadability_sub[,c('target','pred')]\r\n\r\n        target       pred\r\n1  -2.06282395 -1.4780061\r\n2   0.58258607 -0.6337787\r\n3  -1.65313060 -0.9279212\r\n4  -0.87390681 -1.1415349\r\n5  -1.74049148 -0.6113060\r\n6  -3.63993555 -1.2717997\r\n7  -0.62284268 -0.2435638\r\n8  -0.34426981 -1.5272322\r\n9  -1.12298826 -0.7861256\r\n10 -0.99857142 -0.7347420\r\n11 -0.87656742 -0.5278772\r\n12 -0.03304643 -0.6005980\r\n13 -0.49529863 -1.0094665\r\n14  0.12453660 -0.4498485\r\n15  0.09678258 -0.5109152\r\n16  0.38422270 -0.6845919\r\n17 -0.58143038 -0.9489518\r\n18 -0.34324576 -0.9599963\r\n19 -0.39054205  0.1774767\r\n20 -0.67548411 -0.3956684\r\n\r\n# Mean absolute error\r\n\r\n  mean(abs(readability_sub$target - readability_sub$pred))\r\n\r\n[1] 0.6983844\r\n\r\n# Mean squared error\r\n\r\n  mean((readability_sub$target - readability_sub$pred)^2)\r\n\r\n[1] 0.7247307\r\n\r\n# Root mean squared error\r\n\r\n  sqrt(mean((readability_sub$target - readability_sub$pred)^2))\r\n\r\n[1] 0.8513112\r\n\r\nReduction in Total Amount of\r\nError\r\nAnother metric that may give a useful insight about a model’s\r\npredictive performance is the reduction in the total amount of error. If\r\nwe consider that we don’t have any information about the outcome, our\r\nbest guess would be the mean value of the outcome to make a prediction\r\nfor each observation coming from the same population. In other words,\r\nour model would yield a constant prediction for each future observation.\r\nWe can also consider this an intercept-only model or null model.\r\nIn our case, if we use the mean to predict the outcome for each\r\nobservation, the sum of squared error would be equal to 17.733\r\n\\[ SSR_{null} = \\sum_{i=1}^{N}\r\n(y-\\bar{y})^2\\]\r\n\r\n\r\ny_bar <- mean(readability_sub$target)\r\n\r\nssr_null <- sum((readability_sub$target-y_bar)^2)\r\n\r\nssr_null\r\n\r\n[1] 17.73309\r\n\r\nInstead, if we rely on our model to predict the outcome, the sum of\r\nsquared error would be equal to 14.495.\r\n\r\n\r\nssr_model <- sum((readability_sub$target - readability_sub$pred)^2)\r\n\r\nssr_model\r\n\r\n[1] 14.49461\r\n\r\nBy using our model instead of a simple mean, we improved our\r\npredictions such that the total amount of error is reduced from 17.733\r\nto 14.495. Therefore, we can say that the total amount of prediction\r\nerror is reduced by about 18.3% when we use our model instead of a\r\nsimple null model.\r\n\\[ 1-\\frac{SSR_{model}}{SSR_{null}}\r\n\\]\r\n\r\n\r\n1 - (ssr_model/ssr_null)\r\n\r\n[1] 0.1826235\r\n\r\nIn other words, \\(SSR_{null}\\)\r\nprovides a reference point when you do the worst job possible (using\r\nmean for predictions). If we have a perfect model, then SSR would be 0,\r\nthe best job you could do. So, we can evaluate a model’s predictive\r\nperformance by measuring where it stands between these two extreme\r\npoints.\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:05:29-08:00"
    },
    {
      "path": "lecture-3b.html",
      "title": "Bias - Variance Tradeoff and Cross-validation",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "10/05/2022",
      "contents": "\r\n\r\nContents\r\nHow many\r\nparameters does it take to draw an elephant?\r\nThe Principle of Parsimony\r\nBias - Variance Tradeoff\r\nBias and Variance of\r\nModel Predictions\r\nMoral of the\r\nStory: Underfitting vs. Overfitting\r\nFacing the\r\nReality\r\n\r\nUse\r\nof Resampling Methods to Balance Model Bias and Model Variance\r\nBack\r\nto the Elephant\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:05:30 ]\r\nHow many\r\nparameters does it take to draw an elephant?\r\nOnce upon a time, two physicists met, and one convinced the other\r\nthat the agreement between some model-based calculations and measured\r\nexperimental numbers was only superficial. In that conversation, von\r\nNeumann was quoted as saying this famous phrase “… with four\r\nparameters I can fit an elephant, and with five I can make him wiggle\r\nhis trunk.” You\r\ncan read the full story here.\r\nSince then, several people have tried to develop mathematical models\r\nthat can draw an elephant with as few parameters as possible. It has\r\nbecome an exciting activity when people want to make a point about how\r\ncomplex of a model one would need to understand what we observe in the\r\nreal world.\r\nNow, we will join them. See the following plot that has several data\r\npoints. Would you say there is an elephant there? Can you develop a\r\nmathematical model to fit these data points? How complex would that\r\nmodel be? How many parameters would you need?\r\n\r\n\r\n\r\nBelow is a web application with such a model. You can increase the\r\nnumber of parameters in this model from 1 to 70, and the model\r\npredictions will start to look like an elephant. Our quick exploration\r\naims to find the number of parameters you would use to model an\r\nelephant. Start manipulating the p (number of\r\nparameters) and examine how the model predicted contour changes. Stop\r\nwhen you believe you can convince someone else that it looks like an\r\nelephant.\r\nhttps://kourentzes.shinyapps.io/FitElephant/\r\nThe Principle of Parsimony\r\nBias - Variance Tradeoff\r\nWhen we use a model to predict an outcome, there are two primary\r\nsources of error: model error and sampling error.\r\nModel Error: Given that no model is a complete\r\nrepresentation of truth underlying observed data, every model is\r\nmisspecified. Conceptually, we can define the model error as the\r\ndistance between the model and the true generating mechanism underlying\r\ndata. Technically, for a given set of predictors, it is the difference\r\nbetween the expected value predicted by the model and the true value\r\nunderlying data. The term bias is also commonly used\r\nfor model error.\r\nSampling Error: Given that the amount of data is\r\nfixed during any modeling process, it will decrease the stability of\r\nparameter estimates for models with increasing complexity across samples\r\ndrawn from the same population. Consequently, this will increase the\r\nvariance of predictions (more variability of a predicted value across\r\ndifferent samples) for a given set of the same predictors. The terms\r\nestimation error or variance is also\r\nused for sampling error.\r\nThe essence of any modeling activity is to balance these two sources\r\nof error and find a stable model (generalizable across different\r\nsamples) with the least amount of bias.\r\nBias and Variance of\r\nModel Predictions\r\nWe will do a simple Monte Carlo experimentation to understand these\r\ntwo sources of error better. Suppose that there is a true generating\r\nmodel underlying some observed data. This model is\r\n\\[\r\ny = e^{(x-0.3)^2} - 1 + \\epsilon,\r\n\\]\r\nwhere \\(x\\) is a predictor variable\r\nequally spaced and ranges from 0 to 1, \\(\\epsilon\\) is a random error component. The\r\nerrors follow a normal distribution with a mean of zero and a standard\r\ndeviation of 0.1, and \\(y\\) is the\r\noutcome variable. Suppose we simulate a small observed data following\r\nthis model with a sample size of 20. Then, we use a straightforward\r\nlinear model to represent the observed simulated data.\r\n\\[\r\ny = \\beta_0 + \\beta_1x + \\epsilon\r\n\\]\r\n\r\n\r\nset.seed(09282021)\r\n\r\nN = 20\r\n\r\nx <- seq(0,1,length=20)\r\n\r\nx\r\n\r\n [1] 0.00000000 0.05263158 0.10526316 0.15789474 0.21052632 0.26315789\r\n [7] 0.31578947 0.36842105 0.42105263 0.47368421 0.52631579 0.57894737\r\n[13] 0.63157895 0.68421053 0.73684211 0.78947368 0.84210526 0.89473684\r\n[19] 0.94736842 1.00000000\r\n\r\ne <- rnorm(20,0,.1)\r\n\r\ne\r\n\r\n [1]  0.07372726  0.08253427  0.13678980 -0.04993081  0.10368134\r\n [6]  0.28473311 -0.03402811 -0.01834963 -0.02296964  0.02782503\r\n[11] -0.15425785 -0.13371024  0.03465939  0.21786527 -0.09607842\r\n[16]  0.07927619  0.11618340 -0.19217742 -0.07000210 -0.05165884\r\n\r\ny <- exp((x-0.3)^2) - 1 + e\r\n\r\ny\r\n\r\n [1]  0.167901540  0.145636360  0.175440469 -0.029531625  0.111719007\r\n [6]  0.286091377 -0.033778768 -0.013657214 -0.008208013  0.058450844\r\n[11] -0.101704649 -0.052791207  0.150875614  0.376935001  0.114176511\r\n[16]  0.349997226  0.457803769  0.232167369  0.450568829  0.580657384\r\n\r\nmod <- lm(y ~ 1 + x)\r\nmod\r\n\r\n\r\nCall:\r\nlm(formula = y ~ 1 + x)\r\n\r\nCoefficients:\r\n(Intercept)            x  \r\n   -0.00542      0.35272  \r\n\r\npredict(mod)\r\n\r\n           1            2            3            4            5 \r\n-0.005420071  0.013143883  0.031707837  0.050271791  0.068835745 \r\n           6            7            8            9           10 \r\n 0.087399698  0.105963652  0.124527606  0.143091560  0.161655514 \r\n          11           12           13           14           15 \r\n 0.180219468  0.198783422  0.217347376  0.235911330  0.254475284 \r\n          16           17           18           19           20 \r\n 0.273039238  0.291603192  0.310167146  0.328731099  0.347295053 \r\n\r\n\r\n\r\n\r\nThe solid line in this plot represents the true nature of the\r\nrelationship between \\(x\\) and \\(y\\). The observed data points do not lie on\r\nthis line due to the random error component (noise). If we use a simple\r\nlinear model, the gray dashed line represents the predicted relationship\r\nbetween \\(x\\) and \\(y\\).\r\nThis demonstration only represents a single dataset. Now, suppose\r\nthat we repeat the same process ten times. We will produce ten different\r\ndatasets with the same size (N=20) using the same predictor values\r\n(\\(x\\)) and true data generating model.\r\nThen, we will fit a simple linear model to each of these ten\r\ndatasets.\r\n\r\n\r\nset.seed(09282021)\r\n\r\nE  <- vector('list',10)\r\nY  <- vector('list',10)\r\nM1 <- vector('list',10)\r\n\r\nN = 20\r\n\r\nx <- seq(0,1,length=N)\r\n\r\nfor(i in 1:10){\r\n  \r\n  E[[i]]  <- rnorm(N,0,.1)\r\n  Y[[i]]  <- exp((x-0.3)^2) - 1 + E[[i]]\r\n  \r\n  M1[[i]] <- lm(Y[[i]] ~ 1 + x)\r\n}\r\n\r\n\r\n\r\n\r\np.1 <- ggplot()+\r\n  geom_function(fun = function(x) exp((x-.3)^2)-1)+\r\n  theme_bw()+\r\n  xlab('x')+\r\n  ylab('y')+\r\n  xlim(c(0,1))+\r\n  ylim(c(-0.25,1))\r\n\r\nfor(i in 1:10){\r\n  p.1 <- p.1 + geom_line(aes_string(x=x,y=predict(M1[[i]])),col='gray',lty=2)\r\n}\r\n\r\np.1\r\n\r\n\r\n\r\nThe solid line again represents the true nature of the relationship\r\nbetween \\(x\\) and \\(y\\). There are ten lines (gray, dashed),\r\nand each line represents a simple linear model fitted to a different\r\ndataset simulated using the same data generating mechanism. The table\r\nbelow provides a more detailed look at the fitted values from each\r\nreplication for every single \\(x\\)\r\nvalue.\r\n\r\n\r\n\r\n\r\n\r\nModel Predicted Value Across 10 Replications\r\n\r\n\r\n\r\n\r\n\r\nx\r\n\r\n\r\ny (TRUE)\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\n7\r\n\r\n\r\n8\r\n\r\n\r\n9\r\n\r\n\r\n10\r\n\r\n\r\nMean\r\n\r\n\r\nSD\r\n\r\n\r\n0.000\r\n\r\n\r\n0.094\r\n\r\n\r\n-0.005\r\n\r\n\r\n-0.144\r\n\r\n\r\n-0.112\r\n\r\n\r\n-0.154\r\n\r\n\r\n-0.065\r\n\r\n\r\n-0.093\r\n\r\n\r\n-0.114\r\n\r\n\r\n-0.133\r\n\r\n\r\n-0.140\r\n\r\n\r\n-0.080\r\n\r\n\r\n-0.107\r\n\r\n\r\n0.047\r\n\r\n\r\n0.053\r\n\r\n\r\n0.063\r\n\r\n\r\n0.013\r\n\r\n\r\n-0.113\r\n\r\n\r\n-0.084\r\n\r\n\r\n-0.121\r\n\r\n\r\n-0.035\r\n\r\n\r\n-0.065\r\n\r\n\r\n-0.087\r\n\r\n\r\n-0.108\r\n\r\n\r\n-0.113\r\n\r\n\r\n-0.051\r\n\r\n\r\n-0.079\r\n\r\n\r\n0.044\r\n\r\n\r\n0.105\r\n\r\n\r\n0.039\r\n\r\n\r\n0.032\r\n\r\n\r\n-0.081\r\n\r\n\r\n-0.056\r\n\r\n\r\n-0.088\r\n\r\n\r\n-0.006\r\n\r\n\r\n-0.037\r\n\r\n\r\n-0.060\r\n\r\n\r\n-0.082\r\n\r\n\r\n-0.085\r\n\r\n\r\n-0.022\r\n\r\n\r\n-0.051\r\n\r\n\r\n0.041\r\n\r\n\r\n0.158\r\n\r\n\r\n0.020\r\n\r\n\r\n0.050\r\n\r\n\r\n-0.049\r\n\r\n\r\n-0.028\r\n\r\n\r\n-0.056\r\n\r\n\r\n0.023\r\n\r\n\r\n-0.008\r\n\r\n\r\n-0.033\r\n\r\n\r\n-0.056\r\n\r\n\r\n-0.058\r\n\r\n\r\n0.007\r\n\r\n\r\n-0.024\r\n\r\n\r\n0.039\r\n\r\n\r\n0.211\r\n\r\n\r\n0.008\r\n\r\n\r\n0.069\r\n\r\n\r\n-0.017\r\n\r\n\r\n0.000\r\n\r\n\r\n-0.023\r\n\r\n\r\n0.052\r\n\r\n\r\n0.020\r\n\r\n\r\n-0.006\r\n\r\n\r\n-0.030\r\n\r\n\r\n-0.031\r\n\r\n\r\n0.036\r\n\r\n\r\n0.004\r\n\r\n\r\n0.036\r\n\r\n\r\n0.263\r\n\r\n\r\n0.001\r\n\r\n\r\n0.087\r\n\r\n\r\n0.015\r\n\r\n\r\n0.027\r\n\r\n\r\n0.010\r\n\r\n\r\n0.082\r\n\r\n\r\n0.048\r\n\r\n\r\n0.021\r\n\r\n\r\n-0.004\r\n\r\n\r\n-0.004\r\n\r\n\r\n0.065\r\n\r\n\r\n0.031\r\n\r\n\r\n0.034\r\n\r\n\r\n0.316\r\n\r\n\r\n0.000\r\n\r\n\r\n0.106\r\n\r\n\r\n0.046\r\n\r\n\r\n0.055\r\n\r\n\r\n0.042\r\n\r\n\r\n0.111\r\n\r\n\r\n0.076\r\n\r\n\r\n0.048\r\n\r\n\r\n0.022\r\n\r\n\r\n0.024\r\n\r\n\r\n0.094\r\n\r\n\r\n0.059\r\n\r\n\r\n0.032\r\n\r\n\r\n0.368\r\n\r\n\r\n0.005\r\n\r\n\r\n0.125\r\n\r\n\r\n0.078\r\n\r\n\r\n0.083\r\n\r\n\r\n0.075\r\n\r\n\r\n0.140\r\n\r\n\r\n0.105\r\n\r\n\r\n0.075\r\n\r\n\r\n0.048\r\n\r\n\r\n0.051\r\n\r\n\r\n0.124\r\n\r\n\r\n0.087\r\n\r\n\r\n0.031\r\n\r\n\r\n0.421\r\n\r\n\r\n0.015\r\n\r\n\r\n0.143\r\n\r\n\r\n0.110\r\n\r\n\r\n0.111\r\n\r\n\r\n0.108\r\n\r\n\r\n0.169\r\n\r\n\r\n0.133\r\n\r\n\r\n0.102\r\n\r\n\r\n0.074\r\n\r\n\r\n0.078\r\n\r\n\r\n0.153\r\n\r\n\r\n0.114\r\n\r\n\r\n0.030\r\n\r\n\r\n0.474\r\n\r\n\r\n0.031\r\n\r\n\r\n0.162\r\n\r\n\r\n0.142\r\n\r\n\r\n0.139\r\n\r\n\r\n0.140\r\n\r\n\r\n0.199\r\n\r\n\r\n0.161\r\n\r\n\r\n0.129\r\n\r\n\r\n0.099\r\n\r\n\r\n0.105\r\n\r\n\r\n0.182\r\n\r\n\r\n0.142\r\n\r\n\r\n0.030\r\n\r\n\r\n0.526\r\n\r\n\r\n0.053\r\n\r\n\r\n0.180\r\n\r\n\r\n0.174\r\n\r\n\r\n0.167\r\n\r\n\r\n0.173\r\n\r\n\r\n0.228\r\n\r\n\r\n0.189\r\n\r\n\r\n0.156\r\n\r\n\r\n0.125\r\n\r\n\r\n0.133\r\n\r\n\r\n0.211\r\n\r\n\r\n0.169\r\n\r\n\r\n0.031\r\n\r\n\r\n0.579\r\n\r\n\r\n0.081\r\n\r\n\r\n0.199\r\n\r\n\r\n0.205\r\n\r\n\r\n0.195\r\n\r\n\r\n0.206\r\n\r\n\r\n0.257\r\n\r\n\r\n0.218\r\n\r\n\r\n0.183\r\n\r\n\r\n0.151\r\n\r\n\r\n0.160\r\n\r\n\r\n0.240\r\n\r\n\r\n0.197\r\n\r\n\r\n0.031\r\n\r\n\r\n0.632\r\n\r\n\r\n0.116\r\n\r\n\r\n0.217\r\n\r\n\r\n0.237\r\n\r\n\r\n0.223\r\n\r\n\r\n0.239\r\n\r\n\r\n0.286\r\n\r\n\r\n0.246\r\n\r\n\r\n0.209\r\n\r\n\r\n0.177\r\n\r\n\r\n0.187\r\n\r\n\r\n0.269\r\n\r\n\r\n0.225\r\n\r\n\r\n0.033\r\n\r\n\r\n0.684\r\n\r\n\r\n0.159\r\n\r\n\r\n0.236\r\n\r\n\r\n0.269\r\n\r\n\r\n0.251\r\n\r\n\r\n0.271\r\n\r\n\r\n0.316\r\n\r\n\r\n0.274\r\n\r\n\r\n0.236\r\n\r\n\r\n0.203\r\n\r\n\r\n0.214\r\n\r\n\r\n0.298\r\n\r\n\r\n0.252\r\n\r\n\r\n0.035\r\n\r\n\r\n0.737\r\n\r\n\r\n0.210\r\n\r\n\r\n0.254\r\n\r\n\r\n0.301\r\n\r\n\r\n0.279\r\n\r\n\r\n0.304\r\n\r\n\r\n0.345\r\n\r\n\r\n0.302\r\n\r\n\r\n0.263\r\n\r\n\r\n0.229\r\n\r\n\r\n0.242\r\n\r\n\r\n0.327\r\n\r\n\r\n0.280\r\n\r\n\r\n0.037\r\n\r\n\r\n0.789\r\n\r\n\r\n0.271\r\n\r\n\r\n0.273\r\n\r\n\r\n0.333\r\n\r\n\r\n0.307\r\n\r\n\r\n0.337\r\n\r\n\r\n0.374\r\n\r\n\r\n0.330\r\n\r\n\r\n0.290\r\n\r\n\r\n0.255\r\n\r\n\r\n0.269\r\n\r\n\r\n0.357\r\n\r\n\r\n0.308\r\n\r\n\r\n0.039\r\n\r\n\r\n0.842\r\n\r\n\r\n0.342\r\n\r\n\r\n0.292\r\n\r\n\r\n0.365\r\n\r\n\r\n0.335\r\n\r\n\r\n0.369\r\n\r\n\r\n0.403\r\n\r\n\r\n0.359\r\n\r\n\r\n0.317\r\n\r\n\r\n0.281\r\n\r\n\r\n0.296\r\n\r\n\r\n0.386\r\n\r\n\r\n0.335\r\n\r\n\r\n0.042\r\n\r\n\r\n0.895\r\n\r\n\r\n0.424\r\n\r\n\r\n0.310\r\n\r\n\r\n0.396\r\n\r\n\r\n0.363\r\n\r\n\r\n0.402\r\n\r\n\r\n0.433\r\n\r\n\r\n0.387\r\n\r\n\r\n0.344\r\n\r\n\r\n0.306\r\n\r\n\r\n0.323\r\n\r\n\r\n0.415\r\n\r\n\r\n0.363\r\n\r\n\r\n0.045\r\n\r\n\r\n0.947\r\n\r\n\r\n0.521\r\n\r\n\r\n0.329\r\n\r\n\r\n0.428\r\n\r\n\r\n0.391\r\n\r\n\r\n0.435\r\n\r\n\r\n0.462\r\n\r\n\r\n0.415\r\n\r\n\r\n0.371\r\n\r\n\r\n0.332\r\n\r\n\r\n0.351\r\n\r\n\r\n0.444\r\n\r\n\r\n0.390\r\n\r\n\r\n0.048\r\n\r\n\r\n1.000\r\n\r\n\r\n0.632\r\n\r\n\r\n0.347\r\n\r\n\r\n0.460\r\n\r\n\r\n0.419\r\n\r\n\r\n0.467\r\n\r\n\r\n0.491\r\n\r\n\r\n0.443\r\n\r\n\r\n0.398\r\n\r\n\r\n0.358\r\n\r\n\r\n0.378\r\n\r\n\r\n0.473\r\n\r\n\r\n0.418\r\n\r\n\r\n0.051\r\n\r\n\r\n\r\n\r\n\r\nFor instance, when the \\(x\\) is\r\nequal to 0, the true value of \\(y\\)\r\nbased on the model would be 0.094. However, when we fit a linear model\r\nto 10 different datasets with the underlying true model, the average\r\npredicted value was -.107 with a standard deviation of 0.047 across ten\r\nreplications. Similarly, when the \\(x\\)\r\nis equal to 0.316, the true value of \\(y\\) based on the model would be 0, but the\r\naverage prediction was 0.059 with a standard deviation of 0.032 across\r\nten replications. A linear model provides biased estimates such that\r\nthere is an underestimation at the lower values of \\(x\\) and higher values of \\(x\\). At the same time, there is an\r\noverestimation in the middle of the range of \\(x\\).\r\nLet’s do the same experiment by fitting a more complex 6th-degree\r\npolynomial to the same datasets with the same underlying true model.\r\n\\[\r\ny = \\beta_0 + \\beta_1x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 +\r\n\\beta_5 x^5 + \\beta_6 x^6 + \\epsilon\r\n\\]\r\n\r\n\r\nset.seed(09282021)\r\n\r\nE  <- vector('list',10)\r\nY  <- vector('list',10)\r\nM6 <- vector('list',10)\r\n\r\nN = 20\r\n\r\nx <- seq(0,1,length=N)\r\n\r\nfor(i in 1:10){\r\n  \r\n  E[[i]]  <- rnorm(N,0,.1)\r\n  Y[[i]]  <- exp((x-0.3)^2) - 1 + E[[i]]\r\n  \r\n  M6[[i]] <- lm(Y[[i]] ~ 1 + x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))\r\n}\r\n\r\n\r\n\r\n\r\np.6 <- ggplot()+\r\n  geom_function(fun = function(x) exp((x-.3)^2)-1)+\r\n  theme_bw()+\r\n  xlab('x')+\r\n  ylab('y')+\r\n  xlim(c(0,1))+\r\n  ylim(c(-0.25,1))\r\n\r\nfor(i in 1:10){\r\n  p.6 <- p.6 + geom_line(aes_string(x=x,y=predict(M6[[i]])),col='gray',lty=2)\r\n}\r\n\r\np.6\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nModel Predicted Value Across 10 Replications\r\n\r\n\r\n\r\n\r\n\r\nx\r\n\r\n\r\ny (TRUE)\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\n7\r\n\r\n\r\n8\r\n\r\n\r\n9\r\n\r\n\r\n10\r\n\r\n\r\nMean\r\n\r\n\r\nSD\r\n\r\n\r\n0.000\r\n\r\n\r\n0.094\r\n\r\n\r\n0.192\r\n\r\n\r\n0.166\r\n\r\n\r\n0.164\r\n\r\n\r\n0.012\r\n\r\n\r\n-0.038\r\n\r\n\r\n-0.081\r\n\r\n\r\n0.211\r\n\r\n\r\n0.087\r\n\r\n\r\n0.060\r\n\r\n\r\n-0.147\r\n\r\n\r\n0.086\r\n\r\n\r\n0.105\r\n\r\n\r\n0.053\r\n\r\n\r\n0.063\r\n\r\n\r\n0.102\r\n\r\n\r\n-0.019\r\n\r\n\r\n0.054\r\n\r\n\r\n0.012\r\n\r\n\r\n0.127\r\n\r\n\r\n0.040\r\n\r\n\r\n0.043\r\n\r\n\r\n0.049\r\n\r\n\r\n0.019\r\n\r\n\r\n0.122\r\n\r\n\r\n0.048\r\n\r\n\r\n0.045\r\n\r\n\r\n0.105\r\n\r\n\r\n0.039\r\n\r\n\r\n0.107\r\n\r\n\r\n-0.071\r\n\r\n\r\n-0.001\r\n\r\n\r\n-0.005\r\n\r\n\r\n0.155\r\n\r\n\r\n0.099\r\n\r\n\r\n-0.036\r\n\r\n\r\n-0.015\r\n\r\n\r\n-0.007\r\n\r\n\r\n0.160\r\n\r\n\r\n0.025\r\n\r\n\r\n0.076\r\n\r\n\r\n0.158\r\n\r\n\r\n0.020\r\n\r\n\r\n0.130\r\n\r\n\r\n-0.061\r\n\r\n\r\n-0.026\r\n\r\n\r\n-0.023\r\n\r\n\r\n0.121\r\n\r\n\r\n0.112\r\n\r\n\r\n-0.069\r\n\r\n\r\n-0.068\r\n\r\n\r\n-0.028\r\n\r\n\r\n0.100\r\n\r\n\r\n0.010\r\n\r\n\r\n0.085\r\n\r\n\r\n0.211\r\n\r\n\r\n0.008\r\n\r\n\r\n0.133\r\n\r\n\r\n-0.031\r\n\r\n\r\n-0.036\r\n\r\n\r\n-0.033\r\n\r\n\r\n0.072\r\n\r\n\r\n0.096\r\n\r\n\r\n-0.082\r\n\r\n\r\n-0.094\r\n\r\n\r\n-0.047\r\n\r\n\r\n0.024\r\n\r\n\r\n-0.003\r\n\r\n\r\n0.082\r\n\r\n\r\n0.263\r\n\r\n\r\n0.001\r\n\r\n\r\n0.107\r\n\r\n\r\n-0.006\r\n\r\n\r\n-0.039\r\n\r\n\r\n-0.031\r\n\r\n\r\n0.036\r\n\r\n\r\n0.068\r\n\r\n\r\n-0.084\r\n\r\n\r\n-0.091\r\n\r\n\r\n-0.062\r\n\r\n\r\n-0.024\r\n\r\n\r\n-0.011\r\n\r\n\r\n0.069\r\n\r\n\r\n0.316\r\n\r\n\r\n0.000\r\n\r\n\r\n0.059\r\n\r\n\r\n0.008\r\n\r\n\r\n-0.036\r\n\r\n\r\n-0.019\r\n\r\n\r\n0.023\r\n\r\n\r\n0.039\r\n\r\n\r\n-0.078\r\n\r\n\r\n-0.064\r\n\r\n\r\n-0.068\r\n\r\n\r\n-0.030\r\n\r\n\r\n-0.015\r\n\r\n\r\n0.050\r\n\r\n\r\n0.368\r\n\r\n\r\n0.005\r\n\r\n\r\n0.008\r\n\r\n\r\n0.010\r\n\r\n\r\n-0.026\r\n\r\n\r\n0.002\r\n\r\n\r\n0.032\r\n\r\n\r\n0.021\r\n\r\n\r\n-0.061\r\n\r\n\r\n-0.024\r\n\r\n\r\n-0.061\r\n\r\n\r\n0.004\r\n\r\n\r\n-0.011\r\n\r\n\r\n0.034\r\n\r\n\r\n0.421\r\n\r\n\r\n0.015\r\n\r\n\r\n-0.030\r\n\r\n\r\n0.008\r\n\r\n\r\n-0.009\r\n\r\n\r\n0.029\r\n\r\n\r\n0.060\r\n\r\n\r\n0.018\r\n\r\n\r\n-0.029\r\n\r\n\r\n0.018\r\n\r\n\r\n-0.040\r\n\r\n\r\n0.064\r\n\r\n\r\n0.003\r\n\r\n\r\n0.033\r\n\r\n\r\n0.474\r\n\r\n\r\n0.031\r\n\r\n\r\n-0.041\r\n\r\n\r\n0.011\r\n\r\n\r\n0.018\r\n\r\n\r\n0.058\r\n\r\n\r\n0.097\r\n\r\n\r\n0.032\r\n\r\n\r\n0.017\r\n\r\n\r\n0.052\r\n\r\n\r\n-0.005\r\n\r\n\r\n0.134\r\n\r\n\r\n0.027\r\n\r\n\r\n0.040\r\n\r\n\r\n0.526\r\n\r\n\r\n0.053\r\n\r\n\r\n-0.016\r\n\r\n\r\n0.026\r\n\r\n\r\n0.056\r\n\r\n\r\n0.087\r\n\r\n\r\n0.137\r\n\r\n\r\n0.061\r\n\r\n\r\n0.078\r\n\r\n\r\n0.076\r\n\r\n\r\n0.042\r\n\r\n\r\n0.198\r\n\r\n\r\n0.061\r\n\r\n\r\n0.043\r\n\r\n\r\n0.579\r\n\r\n\r\n0.081\r\n\r\n\r\n0.041\r\n\r\n\r\n0.059\r\n\r\n\r\n0.102\r\n\r\n\r\n0.115\r\n\r\n\r\n0.175\r\n\r\n\r\n0.102\r\n\r\n\r\n0.148\r\n\r\n\r\n0.088\r\n\r\n\r\n0.094\r\n\r\n\r\n0.246\r\n\r\n\r\n0.103\r\n\r\n\r\n0.041\r\n\r\n\r\n0.632\r\n\r\n\r\n0.116\r\n\r\n\r\n0.121\r\n\r\n\r\n0.113\r\n\r\n\r\n0.154\r\n\r\n\r\n0.142\r\n\r\n\r\n0.209\r\n\r\n\r\n0.148\r\n\r\n\r\n0.220\r\n\r\n\r\n0.094\r\n\r\n\r\n0.145\r\n\r\n\r\n0.275\r\n\r\n\r\n0.150\r\n\r\n\r\n0.042\r\n\r\n\r\n0.684\r\n\r\n\r\n0.159\r\n\r\n\r\n0.207\r\n\r\n\r\n0.186\r\n\r\n\r\n0.209\r\n\r\n\r\n0.172\r\n\r\n\r\n0.243\r\n\r\n\r\n0.195\r\n\r\n\r\n0.283\r\n\r\n\r\n0.103\r\n\r\n\r\n0.190\r\n\r\n\r\n0.287\r\n\r\n\r\n0.198\r\n\r\n\r\n0.049\r\n\r\n\r\n0.737\r\n\r\n\r\n0.210\r\n\r\n\r\n0.281\r\n\r\n\r\n0.271\r\n\r\n\r\n0.263\r\n\r\n\r\n0.210\r\n\r\n\r\n0.280\r\n\r\n\r\n0.237\r\n\r\n\r\n0.329\r\n\r\n\r\n0.128\r\n\r\n\r\n0.223\r\n\r\n\r\n0.294\r\n\r\n\r\n0.247\r\n\r\n\r\n0.057\r\n\r\n\r\n0.789\r\n\r\n\r\n0.271\r\n\r\n\r\n0.329\r\n\r\n\r\n0.360\r\n\r\n\r\n0.314\r\n\r\n\r\n0.262\r\n\r\n\r\n0.330\r\n\r\n\r\n0.278\r\n\r\n\r\n0.352\r\n\r\n\r\n0.178\r\n\r\n\r\n0.246\r\n\r\n\r\n0.310\r\n\r\n\r\n0.294\r\n\r\n\r\n0.059\r\n\r\n\r\n0.842\r\n\r\n\r\n0.342\r\n\r\n\r\n0.347\r\n\r\n\r\n0.443\r\n\r\n\r\n0.364\r\n\r\n\r\n0.336\r\n\r\n\r\n0.399\r\n\r\n\r\n0.325\r\n\r\n\r\n0.355\r\n\r\n\r\n0.260\r\n\r\n\r\n0.270\r\n\r\n\r\n0.350\r\n\r\n\r\n0.345\r\n\r\n\r\n0.057\r\n\r\n\r\n0.895\r\n\r\n\r\n0.424\r\n\r\n\r\n0.354\r\n\r\n\r\n0.512\r\n\r\n\r\n0.420\r\n\r\n\r\n0.442\r\n\r\n\r\n0.491\r\n\r\n\r\n0.399\r\n\r\n\r\n0.355\r\n\r\n\r\n0.373\r\n\r\n\r\n0.318\r\n\r\n\r\n0.421\r\n\r\n\r\n0.407\r\n\r\n\r\n0.065\r\n\r\n\r\n0.947\r\n\r\n\r\n0.521\r\n\r\n\r\n0.402\r\n\r\n\r\n0.565\r\n\r\n\r\n0.497\r\n\r\n\r\n0.587\r\n\r\n\r\n0.602\r\n\r\n\r\n0.533\r\n\r\n\r\n0.385\r\n\r\n\r\n0.499\r\n\r\n\r\n0.428\r\n\r\n\r\n0.520\r\n\r\n\r\n0.500\r\n\r\n\r\n0.080\r\n\r\n\r\n1.000\r\n\r\n\r\n0.632\r\n\r\n\r\n0.586\r\n\r\n\r\n0.606\r\n\r\n\r\n0.622\r\n\r\n\r\n0.781\r\n\r\n\r\n0.715\r\n\r\n\r\n0.781\r\n\r\n\r\n0.507\r\n\r\n\r\n0.601\r\n\r\n\r\n0.663\r\n\r\n\r\n0.622\r\n\r\n\r\n0.651\r\n\r\n\r\n0.092\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nDISCUSSION\r\n\r\nCompare the numbers in these two tables and discuss the differences\r\nyou observe. What happened to predictions when you fit a more complex\r\nmodel (6th-degree polynomial) instead of a simple regression model? You\r\ncan examine the following plot that displays the average and range of\r\npredictions across 10 replications for every value of \\(x\\).\r\n\r\n\r\n\r\n\r\nWe can expand our experiment and examine a range of models from\r\nlinear to the 6th-degree polynomial. The following plots display what\r\nyou would see if you repeated this experiment by fitting a linear model,\r\nquadratic, cubic, quartic, quintic, and sextic model to the same\r\nsimulated datasets with the same underlying model. A table follows these\r\nplots that present the bias and standard deviation of predictions across\r\nten replications for comparisons.\r\n\\[\r\ny = \\beta_0 + \\beta_1x + \\epsilon\r\n\\] \\[\r\ny = \\beta_0 + \\beta_1x + \\beta_2 x^2 + \\epsilon\r\n\\] \\[\r\ny = \\beta_0 + \\beta_1x + \\beta_2 x^2 + \\beta_3 x^3 + \\epsilon\r\n\\] \\[\r\ny = \\beta_0 + \\beta_1x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 +\r\n\\epsilon\r\n\\] \\[\r\ny = \\beta_0 + \\beta_1x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 +\r\n\\beta_5 x^5  + \\epsilon\r\n\\]\r\n\\[\r\ny = \\beta_0 + \\beta_1x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 +\r\n\\beta_5 x^5 + \\beta_6 x^6 + \\epsilon\r\n\\]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nLinear Model\r\n\r\n\r\n\r\n\r\nQuadratic Model\r\n\r\n\r\n\r\n\r\nQubic Model\r\n\r\n\r\n\r\n\r\n4th Deg. Poly.\r\n\r\n\r\n\r\n\r\n5th Deg. Poly.\r\n\r\n\r\n\r\n\r\n6th Deg. Poly.\r\n\r\n\r\n\r\nx\r\n\r\n\r\ny (TRUE)\r\n\r\n\r\nBias\r\n\r\n\r\nSD\r\n\r\n\r\nBias\r\n\r\n\r\nSD\r\n\r\n\r\nBias\r\n\r\n\r\nSD\r\n\r\n\r\nBias\r\n\r\n\r\nSD\r\n\r\n\r\nBias\r\n\r\n\r\nSD\r\n\r\n\r\nBias\r\n\r\n\r\nSD\r\n\r\n\r\n0.000\r\n\r\n\r\n0.094\r\n\r\n\r\n-0.201\r\n\r\n\r\n0.047\r\n\r\n\r\n-0.010\r\n\r\n\r\n0.051\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.075\r\n\r\n\r\n-0.001\r\n\r\n\r\n0.073\r\n\r\n\r\n-0.018\r\n\r\n\r\n0.092\r\n\r\n\r\n-0.008\r\n\r\n\r\n0.105\r\n\r\n\r\n0.053\r\n\r\n\r\n0.063\r\n\r\n\r\n-0.142\r\n\r\n\r\n0.044\r\n\r\n\r\n-0.012\r\n\r\n\r\n0.046\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.047\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.049\r\n\r\n\r\n-0.004\r\n\r\n\r\n0.052\r\n\r\n\r\n-0.016\r\n\r\n\r\n0.045\r\n\r\n\r\n0.105\r\n\r\n\r\n0.039\r\n\r\n\r\n-0.090\r\n\r\n\r\n0.041\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.042\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.044\r\n\r\n\r\n-0.020\r\n\r\n\r\n0.064\r\n\r\n\r\n-0.004\r\n\r\n\r\n0.078\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.076\r\n\r\n\r\n0.158\r\n\r\n\r\n0.020\r\n\r\n\r\n-0.044\r\n\r\n\r\n0.039\r\n\r\n\r\n-0.014\r\n\r\n\r\n0.039\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.051\r\n\r\n\r\n-0.022\r\n\r\n\r\n0.075\r\n\r\n\r\n-0.011\r\n\r\n\r\n0.085\r\n\r\n\r\n-0.011\r\n\r\n\r\n0.085\r\n\r\n\r\n0.211\r\n\r\n\r\n0.008\r\n\r\n\r\n-0.004\r\n\r\n\r\n0.036\r\n\r\n\r\n-0.014\r\n\r\n\r\n0.036\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.057\r\n\r\n\r\n-0.020\r\n\r\n\r\n0.076\r\n\r\n\r\n-0.018\r\n\r\n\r\n0.078\r\n\r\n\r\n-0.011\r\n\r\n\r\n0.082\r\n\r\n\r\n0.263\r\n\r\n\r\n0.001\r\n\r\n\r\n0.030\r\n\r\n\r\n0.034\r\n\r\n\r\n-0.014\r\n\r\n\r\n0.034\r\n\r\n\r\n-0.012\r\n\r\n\r\n0.059\r\n\r\n\r\n-0.016\r\n\r\n\r\n0.069\r\n\r\n\r\n-0.023\r\n\r\n\r\n0.064\r\n\r\n\r\n-0.013\r\n\r\n\r\n0.069\r\n\r\n\r\n0.316\r\n\r\n\r\n0.000\r\n\r\n\r\n0.059\r\n\r\n\r\n0.032\r\n\r\n\r\n-0.012\r\n\r\n\r\n0.033\r\n\r\n\r\n-0.010\r\n\r\n\r\n0.056\r\n\r\n\r\n-0.010\r\n\r\n\r\n0.057\r\n\r\n\r\n-0.022\r\n\r\n\r\n0.048\r\n\r\n\r\n-0.015\r\n\r\n\r\n0.050\r\n\r\n\r\n0.368\r\n\r\n\r\n0.005\r\n\r\n\r\n0.082\r\n\r\n\r\n0.031\r\n\r\n\r\n-0.009\r\n\r\n\r\n0.032\r\n\r\n\r\n-0.007\r\n\r\n\r\n0.050\r\n\r\n\r\n-0.004\r\n\r\n\r\n0.043\r\n\r\n\r\n-0.017\r\n\r\n\r\n0.035\r\n\r\n\r\n-0.016\r\n\r\n\r\n0.034\r\n\r\n\r\n0.421\r\n\r\n\r\n0.015\r\n\r\n\r\n0.099\r\n\r\n\r\n0.030\r\n\r\n\r\n-0.004\r\n\r\n\r\n0.032\r\n\r\n\r\n-0.003\r\n\r\n\r\n0.042\r\n\r\n\r\n0.002\r\n\r\n\r\n0.031\r\n\r\n\r\n-0.007\r\n\r\n\r\n0.024\r\n\r\n\r\n-0.012\r\n\r\n\r\n0.033\r\n\r\n\r\n0.474\r\n\r\n\r\n0.031\r\n\r\n\r\n0.111\r\n\r\n\r\n0.030\r\n\r\n\r\n0.001\r\n\r\n\r\n0.032\r\n\r\n\r\n0.001\r\n\r\n\r\n0.035\r\n\r\n\r\n0.008\r\n\r\n\r\n0.024\r\n\r\n\r\n0.005\r\n\r\n\r\n0.021\r\n\r\n\r\n-0.004\r\n\r\n\r\n0.040\r\n\r\n\r\n0.526\r\n\r\n\r\n0.053\r\n\r\n\r\n0.117\r\n\r\n\r\n0.031\r\n\r\n\r\n0.006\r\n\r\n\r\n0.033\r\n\r\n\r\n0.006\r\n\r\n\r\n0.031\r\n\r\n\r\n0.013\r\n\r\n\r\n0.024\r\n\r\n\r\n0.017\r\n\r\n\r\n0.027\r\n\r\n\r\n0.008\r\n\r\n\r\n0.043\r\n\r\n\r\n0.579\r\n\r\n\r\n0.081\r\n\r\n\r\n0.116\r\n\r\n\r\n0.031\r\n\r\n\r\n0.012\r\n\r\n\r\n0.033\r\n\r\n\r\n0.011\r\n\r\n\r\n0.033\r\n\r\n\r\n0.017\r\n\r\n\r\n0.030\r\n\r\n\r\n0.027\r\n\r\n\r\n0.035\r\n\r\n\r\n0.022\r\n\r\n\r\n0.041\r\n\r\n\r\n0.632\r\n\r\n\r\n0.116\r\n\r\n\r\n0.108\r\n\r\n\r\n0.033\r\n\r\n\r\n0.018\r\n\r\n\r\n0.034\r\n\r\n\r\n0.017\r\n\r\n\r\n0.038\r\n\r\n\r\n0.020\r\n\r\n\r\n0.036\r\n\r\n\r\n0.032\r\n\r\n\r\n0.042\r\n\r\n\r\n0.033\r\n\r\n\r\n0.042\r\n\r\n\r\n0.684\r\n\r\n\r\n0.159\r\n\r\n\r\n0.093\r\n\r\n\r\n0.035\r\n\r\n\r\n0.023\r\n\r\n\r\n0.036\r\n\r\n\r\n0.021\r\n\r\n\r\n0.042\r\n\r\n\r\n0.021\r\n\r\n\r\n0.042\r\n\r\n\r\n0.032\r\n\r\n\r\n0.045\r\n\r\n\r\n0.039\r\n\r\n\r\n0.049\r\n\r\n\r\n0.737\r\n\r\n\r\n0.210\r\n\r\n\r\n0.070\r\n\r\n\r\n0.037\r\n\r\n\r\n0.026\r\n\r\n\r\n0.037\r\n\r\n\r\n0.024\r\n\r\n\r\n0.044\r\n\r\n\r\n0.020\r\n\r\n\r\n0.047\r\n\r\n\r\n0.027\r\n\r\n\r\n0.047\r\n\r\n\r\n0.037\r\n\r\n\r\n0.057\r\n\r\n\r\n0.789\r\n\r\n\r\n0.271\r\n\r\n\r\n0.037\r\n\r\n\r\n0.039\r\n\r\n\r\n0.027\r\n\r\n\r\n0.039\r\n\r\n\r\n0.025\r\n\r\n\r\n0.042\r\n\r\n\r\n0.018\r\n\r\n\r\n0.050\r\n\r\n\r\n0.016\r\n\r\n\r\n0.051\r\n\r\n\r\n0.024\r\n\r\n\r\n0.059\r\n\r\n\r\n0.842\r\n\r\n\r\n0.342\r\n\r\n\r\n-0.006\r\n\r\n\r\n0.042\r\n\r\n\r\n0.024\r\n\r\n\r\n0.042\r\n\r\n\r\n0.022\r\n\r\n\r\n0.039\r\n\r\n\r\n0.014\r\n\r\n\r\n0.052\r\n\r\n\r\n0.003\r\n\r\n\r\n0.057\r\n\r\n\r\n0.003\r\n\r\n\r\n0.057\r\n\r\n\r\n0.895\r\n\r\n\r\n0.424\r\n\r\n\r\n-0.062\r\n\r\n\r\n0.045\r\n\r\n\r\n0.015\r\n\r\n\r\n0.044\r\n\r\n\r\n0.015\r\n\r\n\r\n0.042\r\n\r\n\r\n0.008\r\n\r\n\r\n0.053\r\n\r\n\r\n-0.008\r\n\r\n\r\n0.061\r\n\r\n\r\n-0.017\r\n\r\n\r\n0.065\r\n\r\n\r\n0.947\r\n\r\n\r\n0.521\r\n\r\n\r\n-0.130\r\n\r\n\r\n0.048\r\n\r\n\r\n0.000\r\n\r\n\r\n0.048\r\n\r\n\r\n0.002\r\n\r\n\r\n0.062\r\n\r\n\r\n0.001\r\n\r\n\r\n0.063\r\n\r\n\r\n-0.009\r\n\r\n\r\n0.064\r\n\r\n\r\n-0.021\r\n\r\n\r\n0.080\r\n\r\n\r\n1.000\r\n\r\n\r\n0.632\r\n\r\n\r\n-0.214\r\n\r\n\r\n0.051\r\n\r\n\r\n-0.023\r\n\r\n\r\n0.052\r\n\r\n\r\n-0.020\r\n\r\n\r\n0.101\r\n\r\n\r\n-0.008\r\n\r\n\r\n0.095\r\n\r\n\r\n0.009\r\n\r\n\r\n0.106\r\n\r\n\r\n0.019\r\n\r\n\r\n0.092\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nDISCUSSION\r\n\r\nIf you had to choose one of these models for one of the simulated\r\ndatasets, which one would you choose? Why?\r\n\r\nMoral of the\r\nStory: Underfitting vs. Overfitting\r\nLarge model bias happens when we underfit and do not use all the\r\ninformation available in the dataset. An example of underfitting for the\r\nexperimentation above would be using a linear model to represent the\r\nrelationship between \\(x\\) and \\(y\\) for one of the sample datasets. Note\r\nthat there is always a model bias to some degree for all these six\r\nmodels because none of them is the true model. However, it is the most\r\nobvious for the linear model that doesn’t account for nonlinearity in\r\nthe dataset. On the other hand, you can see that the linear model is the\r\nmost robust to sampling variation. It is the most stable and provides\r\nthe most consistent predictions for different datasets (more minor\r\nvariation in predictions across ten replications).\r\nLarge model variance happens when we overfit and try to extract more\r\ninformation than available in the dataset. An example of overfitting for\r\nthe experimentation above would be using any model beyond the quadratic\r\nmodel. When this happens, we start modeling noise (error) in the sample\r\ndataset as if it provides some helpful information. In contrast, such\r\ninformation is unique to a specific sample dataset, and there is no\r\nguarantee that it will be replicable for other samples from the same\r\npopulation. Notice that the bias does not improve much for models beyond\r\nthe quadratic model; however, the variance of predictions keeps\r\nincreasing for more complex models. In other words, more complex models\r\nare less stable and not robust to sampling variation. The predictions\r\nfrom more complex models tend to vary more from sample to sample,\r\nalthough they are less biased. In other words, there is less confidence\r\nthat the model will be generalizable for observations outside our\r\nsample.\r\nCould you find that sweet model that provides a reasonable\r\nrepresentation of data and a reasonable amount of generalizability\r\n(consistent/stable predictions for observations other than the ones you\r\nused to develop the model)?\r\n\r\n\r\n\r\n\r\n\r\n\r\nFacing the Reality\r\nWhen try to understand/predict a phenomenon measured in some way in\r\nsocial and behavioral sciences, there is probably not a true model we\r\ncan use as a reference to understand the bias and variance of our\r\npredictions, as we had in our experimentation above. If there is such a\r\nthing that is a ‘true model,’ it is probably a very complex system with\r\nmany, many variables that affect the measured outcome variable. It would\r\nbe reasonable to acknowledge that there are some variables with\r\nrelatively larger important effects, there are many variables with small\r\neffects, and many others with tapering smaller effects, and the\r\ninteractions among all these variables. Our models are just\r\napproximations of this whole reality.\r\nSince we have a fixed amount of data, we only have limited\r\ninformation and can reveal these effects only to a certain degree. The\r\nmore data we have, the more and smaller effects we can detect and\r\nseparate from noise. So, the complexity of the model we can afford is\r\nlimited to the amount of data and information available in the data. The\r\nmost challenging part of any modeling activity is finding the amount of\r\ncomplexity we can afford with the sample data at hand and a model that\r\ncan perform well enough for out-of-sample observations.\r\nUse\r\nof Resampling Methods to Balance Model Bias and Model Variance\r\nCertain strategies are applied to avoid overfitting and find the\r\nsweet spot between model bias and model variance. This process is nicely\r\nillustrated in Boehmke and\r\nGreenwell (2020, Figure 2.1)\r\n\r\n\r\n\r\nWe first split data into two main components: the training and test\r\ndatasets. While there is no particular rule for the size of training and\r\ntest datasets, it is common to see 80-20 or 70-30 splits based on the\r\nsize of the original dataset. The training dataset is mainly used for\r\nexploring and model development, while the test set is mainly used to\r\nvalidate a final model’s performance. Different approaches may be used\r\nwhile doing the initial split of training and test datasets, such as\r\nsimple random sampling, stratified\r\nsampling, or down-sampling/up-sampling for\r\nimbalanced data (typically happens for classification problems when\r\nthere is a great imbalance among categories).\r\nCross-validating the model performance within the training set during\r\nthe exploration and model development is also a good strategy. It is\r\ntypically done by creating multiple partitions within the training set\r\nand testing models on each partition while optimizing the parameters.\r\nThere are different approaches for creating different partitions in the\r\ntraining dataset, such as k-fold\r\ncross-validation or bootstrapping, but\r\nk-fold cross-validation is the most common. In k-fold\r\ncross-validation, the training sample is randomly partitioned into\r\nk sets of equal size. A model is fitted to k-1 folds,\r\nand the remaining fold is used to test the model performance. It can be\r\nrepeated k times by treating a different fold as a hold-out\r\nset. Finally, the performance evaluation metric is aggregated (e.g.,\r\naverage) across k replications to get a k-fold\r\ncross-validation estimate of the performance evaluation metric. Once the\r\nmodel is optimized, a final model is developed as a result of this\r\ncross-validation process. The final model is trained using the whole\r\ntraining data and evaluated one final time on the test dataset to\r\nmeasure the generalizability of model predictions.\r\nBack to the Elephant\r\nIf you are curious more about drawing an elephant, a more\r\nrecent paper by Mayer, Khairy, and Howard (2010) provided a\r\nmathematical model that can draw an elephant with only four complex\r\nparameters (just like what von Neumann said). Below is an R code to\r\nreproduce their model using R.\r\nEven more, this paper by\r\nBoué (2019) argues that you can approximate any dataset of any\r\nmodality with a single parameter. Go figure!\r\n\r\n\r\n# 4 complex parameters\r\n\r\n  p1 <- 50-30i\r\n  p2 <- 18+8i\r\n  p3 <- 12-10i\r\n  p4 <- -14-60i\r\n\r\n  Cx <- c(0,50i,18i,12,0,-14)\r\n  Cy <- c(0,-60-30i,8i,-10i,0,0)\r\n\r\n# t, a parameter that can be interpreted as the elapsed time while going along\r\n# the path of the contour\r\n  \r\n  t <- seq(0,2*pi,length.out = 1000)\r\n\r\n# X-coordinates\r\n  \r\n  x <- c()\r\n  \r\n  A <- c(0,0,0,12,0,-14)  # Real part of Cx\r\n  B <- c(0,50,18,0,0,0)   # Imaginary part of Cx\r\n  \r\n  for(i in 1:length(t)){\r\n    k <- 0:5\r\n    x[i] <- sum(A*cos(k*t[i]) + B*sin(k*t[i])) # Eq 1\r\n  }\r\n  \r\n# Y-coordinates\r\n  \r\n  y <- c()\r\n  \r\n  A <- c(0,-60,0,0,0,0)     # Real part of Cy\r\n  B <- c(0,-30,8,-10,0,0)   # Imaginary part of Cy\r\n  \r\n  for(i in 1:length(t)){\r\n    k <- 0:5\r\n    y[i] <- sum(A*cos(k*t[i]) + B*sin(k*t[i])) # Eq 2\r\n  }\r\n  \r\n# Function to draw the elephant\r\n\r\n  plot(y,-x,type='l')\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:05:45-08:00"
    },
    {
      "path": "lecture-4.html",
      "title": "Regularized (Penalized) Linear Regression",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "10/11/2022",
      "contents": "\r\n\r\nContents\r\nRegularization\r\nRidge\r\nRegression\r\nRidge\r\nPenalty\r\nModel\r\nEstimation\r\nUsing\r\nRidge Regression to Predict Readability Scores\r\n\r\nLasso\r\nRegression\r\nModel\r\nEstimation\r\nUsing\r\nLasso Regression to Predict the Readability Scores\r\n\r\nElastic Net\r\nUsing\r\nElastic Net to Predict the Readability Scores\r\n\r\nUsing the Prediction\r\nModel for a New Text\r\n\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:05:46 ]\r\nRegularization\r\nRegularization is a general strategy to incorporate additional\r\npenalty terms into the model fitting process and is used not just for\r\nregression but a variety of other models. The idea behind the\r\nregularization is to constrain the size of regression coefficients to\r\nreduce their sampling variation and, hence, reduce the variance of model\r\npredictions. These constraints are typically incorporated into the loss\r\nfunction to be optimized. There are two commonly used regularization\r\nstrategies: ridge penalty and lasso\r\npenalty. In addition, there is also elastic\r\nnet, a mixture of these two strategies.\r\nRidge Regression\r\nRidge Penalty\r\nRemember that we formulated the loss function for the linear\r\nregression as the sum of squared residuals across all observations. For\r\nridge regression, we add a penalty term to this loss function, which is\r\na function of all the regression coefficients in the model. Assuming\r\nthat there are P regression coefficients in the model, the penalty term\r\nfor the ridge regression would be \\[\\lambda\r\n\\sum_{i=1}^{P}\\beta_p^2,\\] where \\(\\lambda\\) is a parameter that penalizes the\r\nregression coefficients when they get larger. Therefore, when we fit a\r\nregression model with ridge penalty, the loss function to minimize\r\nbecomes\r\n\\[Loss = \\sum_{i=1}^{N}\\epsilon_{(i)}^2 +\r\n\\lambda \\sum_{p=1}^{P}\\beta_p^2,\\]\r\n\\[Loss = SSR + \\lambda\r\n\\sum_{i=1}^{P}\\beta_p^2.\\]\r\nLet’s consider the same example from the previous class. Suppose we\r\nfit a simple linear regression model such that the readability score is\r\nthe outcome (\\(Y\\)) and the Feature 220\r\nis the predictor(\\(X\\)). Our regression\r\nmodel is\r\n\\[Y = \\beta_0  + \\beta_1X +\r\n\\epsilon,\\]\r\nlet’s assume the set of coefficients are {\\(\\beta_0,\\beta_1\\)} = {-1.5,2}, so my model\r\nis \\[Y = -1.5  + 2X + \\epsilon.\\]\r\nThen, the value of the loss function when \\(\\lambda=0.2\\) would be equal to 19.02.\r\n\r\n\r\nreadability_sub <- read.csv('./data/readability_sub.csv',header=TRUE)\r\n\r\nd <-  readability_sub[,c('V220','target')]\r\n\r\nb0 = -1.5\r\nb1 = 2\r\n\r\nd$predicted <- b0 + b1*d$V220\r\nd$error     <- d$target - d$predicted\r\n\r\nd\r\n\r\n          V220      target  predicted       error\r\n1  -0.13908258 -2.06282395 -1.7781652 -0.28465879\r\n2   0.21764143  0.58258607 -1.0647171  1.64730321\r\n3   0.05812133 -1.65313060 -1.3837573 -0.26937327\r\n4   0.02526429 -0.87390681 -1.4494714  0.57556460\r\n5   0.22430885 -1.74049148 -1.0513823 -0.68910918\r\n6  -0.07795373 -3.63993555 -1.6559075 -1.98402809\r\n7   0.43400714 -0.62284268 -0.6319857  0.00914304\r\n8  -0.24364550 -0.34426981 -1.9872910  1.64302120\r\n9   0.15893717 -1.12298826 -1.1821257  0.05913740\r\n10  0.14496475 -0.99857142 -1.2100705  0.21149908\r\n11  0.34222975 -0.87656742 -0.8155405 -0.06102693\r\n12  0.25219145 -0.03304643 -0.9956171  0.96257066\r\n13  0.03532625 -0.49529863 -1.4293475  0.93404886\r\n14  0.36410633  0.12453660 -0.7717873  0.89632394\r\n15  0.29988593  0.09678258 -0.9002281  0.99701073\r\n16  0.19837037  0.38422270 -1.1032593  1.48748196\r\n17  0.07807041 -0.58143038 -1.3438592  0.76242880\r\n18  0.07935690 -0.34324576 -1.3412862  0.99804044\r\n19  0.57000953 -0.39054205 -0.3599809 -0.03056111\r\n20  0.34523284 -0.67548411 -0.8095343  0.13405021\r\n\r\nlambda = 0.2\r\n\r\nloss <- sum((d$error)^2) + lambda*(b0^2 + b1^2)\r\n\r\nloss \r\n\r\n[1] 19.01657\r\n\r\nNotice that when \\(\\lambda\\) is\r\nequal to zero, the loss function is identical to SSR; therefore, it\r\nbecomes a linear regression with no regularization. As the value of\r\n\\(\\lambda\\) increases, the degree of\r\npenalty linearly increases. The \\(\\lambda\\) can technically take any positive\r\nvalue between 0 and \\(\\infty\\).\r\nAs we did in the previous lecture, imagine that we computed the loss\r\nfunction with the ridge penalty term for every possible combination of\r\nthe intercept (\\(\\beta_0\\)) and the\r\nslope (\\(\\beta_1\\)). Let’s say the\r\nplausible range for the intercept is from -10 to 10 and the plausible\r\nrange for the slope is from -2 to 2. Now, we also have to think of\r\ndifferent values of \\(\\lambda\\) because\r\nthe surface we try to minimize is dependent on the value \\(\\lambda\\) and different values of \\(\\lambda\\) yield different estimates of\r\n\\(\\beta_0\\) and \\(\\beta_1\\).\r\n\r\n\r\n\r\n\r\n\r\n\r\nModel Estimation\r\nMatrix Solution\r\nThe matrix solution we learned before for regression without\r\nregularization can also be applied to estimate the coefficients from\r\nridge regression given the \\(\\lambda\\)\r\nvalue. Given that\r\n\\(\\mathbf{Y}\\) is an N x 1 column\r\nvector of observed values for the outcome variable,\r\n\\(\\mathbf{X}\\) is an N x (P+1)\r\ndesign matrix for the set of predictor variables,\r\nincluding an intercept term,\r\n\\(\\boldsymbol{\\beta}\\) is an (P+1)\r\nx 1 column vector of regression coefficients,\r\n\\(\\mathbf{I}\\) is a (P+1) x (P+1)\r\nidentity matrix,\r\nand \\(\\lambda\\) is a positive\r\nreal-valued number,\r\nthe ridge regression coefficients can be estimated using the\r\nfollowing matrix operation.\r\n\\[\\hat{\\boldsymbol{\\beta}} =\r\n(\\mathbf{X^T}\\mathbf{X} + \\lambda\r\n\\mathbf{I})^{-1}\\mathbf{X^T}\\mathbf{Y}\\] Suppose we want to\r\npredict the readability score using the two predictors, Feature 220\r\n(\\(X_1\\)) and Feature 166 (\\(X_2\\)). Our model will be\r\n\\[Y_{(i)} = \\beta_0  + \\beta_1X_{1(i)} +\r\n\\beta_2X_{2(i)} + \\epsilon_{(i)}.\\] If we estimate the ridge\r\nregression coefficients by using \\(\\lambda=.5\\), the estimates would be {\\(\\beta_0,\\beta_1,\\beta_2\\)} =\r\n{-.915,1.169,-0.22}.\r\n\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(1,readability_sub$V220,readability_sub$V166))\r\n\r\nlambda <- 0.5\r\n\r\nbeta <- solve(t(X)%*%X + lambda*diag(ncol(X)))%*%t(X)%*%Y\r\n\r\nbeta \r\n\r\n           [,1]\r\n[1,] -0.9151087\r\n[2,]  1.1691920\r\n[3,] -0.2206188\r\n\r\nIf we change the value of \\(\\lambda\\) to 2, we will get different\r\nestimates for the regression coefficients.\r\n\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(1,readability_sub$V220,readability_sub$V166))\r\n\r\nlambda <- 2\r\n\r\nbeta <- solve(t(X)%*%X + lambda*diag(ncol(X)))%*%t(X)%*%Y\r\n\r\nbeta \r\n\r\n           [,1]\r\n[1,] -0.7550986\r\n[2,]  0.4685138\r\n[3,] -0.1152953\r\n\r\nWe can manipulate the value of \\(\\lambda\\) from 0 to 100 with increments of\r\n.1 and calculate the regression coefficients. Note the regression\r\ncoefficients will shrink toward zero but will never be exactly equal to\r\nzero in ridge regression.\r\n\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(1,readability_sub$V220,readability_sub$V166))\r\n\r\nlambda <- seq(0,100,.1)\r\n\r\nbeta     <- data.frame(matrix(nrow=length(lambda),ncol=4))\r\nbeta[,1] <- lambda\r\n\r\nfor(i in 1:length(lambda)){\r\n  beta[i,2:4] <- t(solve(t(X)%*%X + lambda[i]*diag(ncol(X)))%*%t(X)%*%Y)\r\n}\r\n\r\nggplot(data = beta)+\r\n  geom_line(aes(x=X1,y=X2))+\r\n  geom_line(aes(x=X1,y=X3))+\r\n  geom_line(aes(x=X1,y=X4))+\r\n  xlab(expression(lambda))+\r\n  ylab('')+\r\n  theme_bw()+\r\n  annotate(geom='text',x=1.5,y=1.5,label=expression(beta[1]))+\r\n  annotate(geom='text',x=3,y=-.17,label=expression(beta[2]))+\r\n  annotate(geom='text',x=2,y=-.9,label=expression(beta[0]))\r\n\r\n\r\n\r\nStandardized Variables\r\nWe haven’t considered a critical issue for the model estimation. This\r\nissue is not necessarily important if you have only one predictor;\r\nhowever, it is critical whenever you have more than one predictor.\r\nDifferent variables have different scales, and therefore the magnitude\r\nof the regression coefficients for different variables will depend on\r\nthe variables’ scales. A regression coefficient for a predictor with a\r\nrange from 0 to 100 will be very different from a regression coefficient\r\nfor a predictor from 0 to 1. Therefore, if we work with the\r\nunstandardized variables, the ridge penalty will be amplified for the\r\ncoefficients of those variables with a more extensive range of\r\nvalues.\r\nTherefore, we must standardize variables before we use ridge\r\nregression. Let’s do the example in the previous section, but we now\r\nfirst standardize the variables in our model.\r\n\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(readability_sub$V220,readability_sub$V166))\r\n\r\n# Standardize Y\r\n\r\n  Y <- scale(Y)\r\n  \r\n  Y\r\n\r\n             [,1]\r\n [1,] -1.34512103\r\n [2,]  1.39315702\r\n [3,] -0.92104526\r\n [4,] -0.11446655\r\n [5,] -1.01147297\r\n [6,] -2.97759768\r\n [7,]  0.14541127\r\n [8,]  0.43376355\r\n [9,] -0.37229209\r\n[10,] -0.24350754\r\n[11,] -0.11722056\r\n[12,]  0.75591253\r\n[13,]  0.27743280\r\n[14,]  0.91902756\r\n[15,]  0.89029923\r\n[16,]  1.18783003\r\n[17,]  0.18827737\r\n[18,]  0.43482354\r\n[19,]  0.38586691\r\n[20,]  0.09092186\r\nattr(,\"scaled:center\")\r\n[1] -0.7633224\r\nattr(,\"scaled:scale\")\r\n[1] 0.9660852\r\n\r\n# Standardized X\r\n  \r\n  X <- scale(X)\r\n  X\r\n\r\n             [,1]        [,2]\r\n [1,] -1.54285661  1.44758852\r\n [2,]  0.24727019 -0.47711825\r\n [3,] -0.55323997 -0.97867834\r\n [4,] -0.71812450  1.41817232\r\n [5,]  0.28072891 -0.62244962\r\n [6,] -1.23609737  0.11236158\r\n [7,]  1.33304531  0.34607530\r\n [8,] -2.06757849 -1.22697345\r\n [9,] -0.04732188  0.05882229\r\n[10,] -0.11743882 -1.24554362\r\n[11,]  0.87248435  1.93772977\r\n[12,]  0.42065052  0.13025831\r\n[13,] -0.66763117 -0.40479141\r\n[14,]  0.98226625  1.39073712\r\n[15,]  0.65999286  0.25182543\r\n[16,]  0.15056337 -0.28808443\r\n[17,] -0.45313072  0.02862469\r\n[18,] -0.44667480  0.25187100\r\n[19,]  2.01553800 -2.00805114\r\n[20,]  0.88755458 -0.12237606\r\nattr(,\"scaled:center\")\r\n[1] 0.1683671 0.1005784\r\nattr(,\"scaled:scale\")\r\n[1] 0.19927304 0.06196686\r\n\r\nWhen we standardize the variables, the mean of all variables becomes\r\nzero. So, the intercept estimate for any regression model with\r\nstandardized variables is guaranteed to be zero. Note that our design\r\nmatrix doesn’t have a column of ones because it is unnecessary (it would\r\nbe a column of zeros if we had one).\r\nFirst, check the regression model’s coefficients with standardized\r\nvariables when there is no ridge penalty.\r\n\r\n\r\nlambda <- 0\r\n\r\nbeta.s <- solve(t(X)%*%X + lambda*diag(ncol(X)))%*%t(X)%*%Y\r\n\r\nbeta.s \r\n\r\n            [,1]\r\n[1,]  0.42003881\r\n[2,] -0.06335594\r\n\r\nNow, let’s increase the ridge penalty to 0.5.\r\n\r\n\r\nlambda <- 0.5\r\n\r\nbeta.s <- solve(t(X)%*%X + lambda*diag(ncol(X)))%*%t(X)%*%Y\r\n\r\nbeta.s \r\n\r\n            [,1]\r\n[1,]  0.40931629\r\n[2,] -0.06215875\r\n\r\nBelow, we can manipulate the value of \\(\\lambda\\) from 0 to 100 with increments of\r\n.1 as we did before and calculate the standardized regression\r\ncoefficients.\r\n\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(readability_sub$V220,readability_sub$V166))\r\n\r\nY <- scale(Y)\r\nX <- scale(X)\r\n\r\nlambda <- seq(0,100,.1)\r\n\r\nbeta     <- data.frame(matrix(nrow=length(lambda),ncol=3))\r\nbeta[,1] <- lambda\r\n\r\nfor(i in 1:length(lambda)){\r\n  beta[i,2:3] <- t(solve(t(X)%*%X + lambda[i]*diag(ncol(X)))%*%t(X)%*%Y)\r\n}\r\n\r\nggplot(data = beta)+\r\n  geom_line(aes(x=X1,y=X2))+\r\n  geom_line(aes(x=X1,y=X3))+\r\n  xlab(expression(lambda))+\r\n  ylab('')+\r\n  theme_bw()+\r\n  geom_hline(yintercept=0,lty=2) + \r\n  annotate(geom='text',x=3,y=.4,label=expression(beta[1]))+\r\n  annotate(geom='text',x=2,y=-.075,label=expression(beta[2]))\r\n\r\n\r\n\r\nglmnet() function\r\nSimilar to the lm function, we can use the\r\nglmnet() function from the glmnet package to\r\nrun a regression model with ridge penalty. There are many arguments for\r\nthe glmnet() function. For now, the arguments we need to\r\nknow are\r\nx: an N x P input matrix, where N is the number of\r\nobservations and P is the number of predictors\r\ny: an N x 1 input matrix for the outcome variable\r\nalpha: a mixing constant for lasso and ridge penalty.\r\nWhen it is zero, the ridge regression is conducted\r\nlambda: penalty term\r\nintercept: set FALSE to avoid intercept for\r\nstandardized variables\r\nIf you want to fit the linear regression without regularization, you\r\ncan specify alpha = 0 and lambda = 0.\r\n\r\n\r\n#install.packages('glmnet')\r\n\r\nrequire(glmnet)\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(readability_sub$V220,readability_sub$V166))\r\nY <- scale(Y)\r\nX <- scale(X)\r\n\r\nmod <- glmnet(x = X,\r\n              y = Y,\r\n              family = 'gaussian',\r\n              alpha = 0,\r\n              lambda = 0,\r\n              intercept=FALSE)\r\n\r\n\r\ncoef(mod)\r\n\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                     s0\r\n(Intercept)  .         \r\nV1           0.42003881\r\nV2          -0.06335594\r\n\r\nWe can also increase the penalty term (\\(\\lambda\\)).\r\n\r\n\r\n#install.packages('glmnet')\r\n\r\nrequire(glmnet)\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(readability_sub$V220,readability_sub$V166))\r\nY <- scale(Y)\r\nX <- scale(X)\r\n\r\nmod <- glmnet(x = X,\r\n              y = Y,\r\n              family = 'gaussian',\r\n              alpha = 0,\r\n              lambda = 0.5,\r\n              intercept=FALSE)\r\n\r\n\r\ncoef(mod)\r\n\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                     s0\r\n(Intercept)  .         \r\nV1           0.27809880\r\nV2          -0.04571182\r\n\r\n\r\n\r\nNOTE\r\n\r\nA careful eye should catch the fact that the coefficient estimates we\r\nobtained from the glmnet() function for the two\r\nstandardized variables (Feature 220 and Feature 166) are different than\r\nour matrix calculations above when the penalty term (\\(\\lambda\\)) is 0.5. When we apply the matrix\r\nsolution above for the ridge regression, we obtained the estimates of\r\n0.409 and -0.062 for the two predictors, respectively, at \\(\\lambda\\) = 0.5. When we enter the same\r\nvalue in glmnet(), we obtain the estimates of 0.278 and\r\n-0.046. So, what is wrong? Where does this discrepancy come from?\r\nThere is nothing wrong. It appears that what lambda\r\nargument in glmnet indicates is \\(\\frac{\\lambda}{N}\\). In most statistics\r\ntextbooks, the penalty term for the ridge regression is specified as\r\n\\[\\lambda\r\n\\sum_{i=1}^{P}\\beta_p^2.\\]\r\nOn the other hand, if we examine Equation 1-3 in this paper\r\nwritten by the developers of the glmnet package, we can see\r\nthat the penalty term applied is equivalent of\r\n\\[\\lambda N\r\n\\sum_{i=1}^{P}\\beta_p^2.\\]\r\nTherefore, if we want identical results, we should use \\(\\lambda\\) = 0.5/20.\r\n\r\n\r\nN = 20\r\n\r\nmod <- glmnet(x = X,\r\n              y = Y,\r\n              family = 'gaussian',\r\n              alpha = 0,\r\n              lambda = 0.5/N,\r\n              intercept=FALSE)\r\n\r\n\r\ncoef(mod)\r\n\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                     s0\r\n(Intercept)  .         \r\nV1           0.40958102\r\nV2          -0.06218857\r\n\r\nNote that these numbers are still slightly different. We can\r\nattribute this difference to the numerical approximation\r\nglmnet is using when optimizing the loss function.\r\nglmnet doesn’t use the closed-form matrix solution for\r\nridge regression. This is a good thing because there is not always a\r\nclosed form solution for different types of regularization approaches\r\n(e.g., lasso). Therefore, the computational approximation in\r\nglmnet is very needed moving forward.\r\n\r\nTuning the Hyperparameter\r\n\\(\\lambda\\)\r\nIn ridge regression, the \\(\\lambda\\)\r\nparameter is called a hyperparameter. In the context of\r\nmachine learning, the parameters in a model can be classified into two\r\ntypes: parameters and hyperparameters. The parameters\r\nare typically estimated from data and not set by users. In the context\r\nof ridge regression, regression coefficients, {\\(\\beta_0,\\beta_1,...,\\beta_P\\)}, are\r\nparameters to be estimated from data. On the other hand, the\r\nhyperparameters are not estimable, most of the time,\r\nbecause there are no first-order or second-order derivatives for these\r\nhyperparameters. Therefore, they must be set by the users. In the\r\ncontext of ridge regression, the penalty term, {\\(\\lambda\\)}, is a hyperparameter.\r\nThe process of deciding what value to use for a hyperparameter is\r\ncalled tuning, and it is usually a trial-error process.\r\nThe idea is simple. We try many different hyperparameter values and\r\ncheck how well the model performs based on specific criteria (e.g., MAE,\r\nMSE, RMSE) using k-fold cross-validation. Then, we pick the value of a\r\nhyperparameter that provides the best performance.\r\nUsing\r\nRidge Regression to Predict Readability Scores\r\nPlease review the following notebook for applying Ridge Regresison to\r\npredict readability scores from all 768 features using the whole\r\ndataset.\r\nPredicting\r\nReadability Scores using the Ridge Regression\r\nLasso Regression\r\nLasso regression is very similar to the Ridge regression. The only\r\ndifference is that it applies a different penalty to the loss function.\r\nAssuming that there are P regression coefficients in the model, the\r\npenalty term for the ridge regression would be\r\n\\[\\lambda \\sum_{i=1}^{P}\r\n|\\beta_p|,\\]\r\nwhere \\(\\lambda\\) is again the\r\npenalty constant and \\(|\\beta_p|\\) is\r\nthe absolute value of the regression coefficient for the \\(p^{th}\\) parameter. Lasso regression also\r\npenalizes the regression coefficients when they get larger but\r\ndifferently. When we fit a regression model with a lasso penalty, the\r\nloss function to minimize becomes\r\n\\[Loss = \\sum_{i=1}^{N}\\epsilon_{(i)}^2 +\r\n\\lambda \\sum_{i=1}^{P}|\\beta_p|,\\]\r\n\\[Loss = SSR + \\lambda\r\n\\sum_{i=1}^{P}|\\beta_p|.\\]\r\nLet’s consider the same example where we fit a simple linear\r\nregression model: the readability score is the outcome (\\(Y\\)) and Feature 229 is the predictor(\\(X\\)). Our regression model is\r\n\\[Y = \\beta_0  + \\beta_1X +\r\n\\epsilon,\\]\r\nand let’s assume the set of coefficients are {\\(\\beta_0,\\beta_1\\)} = {-1.5,2}, so my model\r\nis \\[Y = -1.5 + 2X + \\epsilon.\\]\r\nThen, the value of the loss function when \\(\\lambda=0.2\\) would equal 18.467.\r\n\r\n\r\nreadability_sub <- read.csv('./data/readability_sub.csv',header=TRUE)\r\n\r\nd <-  readability_sub[,c('V220','target')]\r\n\r\nb0 = -1.5\r\nb1 = 2\r\n\r\nd$predicted <- b0 + b1*d$V220\r\nd$error     <- d$target - d$predicted\r\n\r\nd\r\n\r\n          V220      target  predicted       error\r\n1  -0.13908258 -2.06282395 -1.7781652 -0.28465879\r\n2   0.21764143  0.58258607 -1.0647171  1.64730321\r\n3   0.05812133 -1.65313060 -1.3837573 -0.26937327\r\n4   0.02526429 -0.87390681 -1.4494714  0.57556460\r\n5   0.22430885 -1.74049148 -1.0513823 -0.68910918\r\n6  -0.07795373 -3.63993555 -1.6559075 -1.98402809\r\n7   0.43400714 -0.62284268 -0.6319857  0.00914304\r\n8  -0.24364550 -0.34426981 -1.9872910  1.64302120\r\n9   0.15893717 -1.12298826 -1.1821257  0.05913740\r\n10  0.14496475 -0.99857142 -1.2100705  0.21149908\r\n11  0.34222975 -0.87656742 -0.8155405 -0.06102693\r\n12  0.25219145 -0.03304643 -0.9956171  0.96257066\r\n13  0.03532625 -0.49529863 -1.4293475  0.93404886\r\n14  0.36410633  0.12453660 -0.7717873  0.89632394\r\n15  0.29988593  0.09678258 -0.9002281  0.99701073\r\n16  0.19837037  0.38422270 -1.1032593  1.48748196\r\n17  0.07807041 -0.58143038 -1.3438592  0.76242880\r\n18  0.07935690 -0.34324576 -1.3412862  0.99804044\r\n19  0.57000953 -0.39054205 -0.3599809 -0.03056111\r\n20  0.34523284 -0.67548411 -0.8095343  0.13405021\r\n\r\nlambda = 0.2\r\n\r\nloss <- sum((d$error)^2) + lambda*(abs(b0) + abs(b1))\r\n\r\nloss \r\n\r\n[1] 18.46657\r\n\r\nWhen \\(\\lambda\\) is equal to 0, the\r\nloss function is again identical to SSR; therefore, it becomes a linear\r\nregression with no regularization. Below is a demonstration of what\r\nhappens to the loss function and the regression coefficients for\r\nincreasing levels of loss penalty (\\(\\lambda\\)).\r\n\r\n\r\n\r\nModel Estimation\r\nUnfortunately, there is no closed-form solution for lasso regression\r\ndue to the absolute value terms in the loss function. The only way to\r\nestimate the coefficients of the lasso regression is to optimize the\r\nloss function using numerical techniques and obtain computational\r\napproximations of the regression coefficients. Similar to ridge\r\nregression, glmnet is an engine we can use to estimate the\r\ncoefficients of the lasso regression.\r\nglmnet() function\r\nWe can fit the lasso regression by setting the alpha=\r\nargument to 1 in glmnet() and specifying the penalty term\r\n(\\(\\lambda\\)).\r\n\r\n\r\nY <-  as.matrix(readability_sub$target)\r\nX <-  as.matrix(cbind(readability_sub$V220,readability_sub$V166))\r\nY <- scale(Y)\r\nX <- scale(X)\r\n\r\nmod <- glmnet(x = X,\r\n              y = Y,\r\n              family = 'gaussian',\r\n              alpha = 1,\r\n              lambda = 0.2,\r\n              intercept=FALSE)\r\n\r\n\r\ncoef(mod)\r\n\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                   s0\r\n(Intercept) .        \r\nV1          0.2174345\r\nV2          .        \r\n\r\nNotice that there is a . symbol for the coefficient of\r\nthe second predictor. The . symbol indicates that it is\r\nequal to zero. While the regression coefficients in the ridge regression\r\nshrink to zero, they do not necessarily end up being exactly equal to\r\nzero. In contrast, lasso regression may yield a value of zero for some\r\ncoefficients in the model. For this reason, lasso regression may be used\r\nas a variable selection algorithm. The variables with coefficients equal\r\nto zero may be discarded from future considerations as they are not\r\ncrucial for predicting the outcome.\r\nTuning \\(\\lambda\\)\r\nWe implement a similar strategy for finding the optimal value of\r\n\\(\\lambda\\). We try many different\r\nvalues of \\(\\lambda\\) and check how\r\nwell the model performs based on specific criteria (e.g., MAE, MSE,\r\nRMSE) using k-fold cross-validation. Then, we pick the value of \\(\\lambda\\) that provides the best\r\nperformance.\r\nUsing\r\nLasso Regression to Predict the Readability Scores\r\nPlease review the following notebook for applying Lasso Regresison to\r\npredict readability scores from all 768 features using the whole\r\ndataset.\r\nPredicting\r\nReadability Scores using the Lasso Regression\r\nElastic Net\r\nElastic net combines the two types of penalty into one by mixing them\r\nwith some weighted average. The penalty term for the elastic net could\r\nbe written as\r\n\\[\\lambda \\left[ (1-\\alpha)\\sum_{i=1}^{P}\r\n\\beta_p^2 + \\alpha\\sum_{i=1}^{P} |\\beta_p|)\\right].\\]\r\nNote that this term reduces to\r\n\\[\\lambda \\sum_{i=1}^{P}\r\n\\beta_p^2\\]\r\nwhen \\(\\alpha\\) is equal to 1 and\r\nto\r\n\\[\\lambda \\sum_{i=1}^{P}\r\n|\\beta_p|\\]\r\nwhen \\(\\alpha\\) is equal to 0.\r\nWhen \\(\\alpha\\) is set to 1, this is\r\nequivalent to ridge regression. When \\(\\alpha\\) equals 0, this is the equivalent\r\nof lasso regression. When \\(\\alpha\\)\r\ntakes any value between 0 and 1, this term becomes a weighted average of\r\nthe ridge penalty and lasso penalty. In Elastic Net, two hyperparameters\r\nwill be tuned, \\(\\alpha\\) and \\(\\lambda\\). We can consider all possible\r\ncombinations of these two hyperparameters and try to find the optimal\r\ncombination using 10-fold cross-validation.\r\nUsing\r\nElastic Net to Predict the Readability Scores\r\nPlease review the following notebook for applying Elastic Net to\r\npredict readability scores from all 768 features using the whole\r\ndataset.\r\nPredicting\r\nReadability Scores using the Elastic Net\r\nUsing the Prediction\r\nModel for a New Text\r\nPlease review the following notebook for predicting the readability\r\nof a given text with the existing model objects\r\nPredicting\r\nReadability Scores for a new text\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:05:57-08:00"
    },
    {
      "path": "lecture-5.html",
      "title": "An Overview of the Logistic Regression and Regularization",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "11/3/2022",
      "contents": "\r\n\r\nContents\r\n1. Overview of the Logistic\r\nRegression\r\n1.1. Linear Probability Model\r\n1.2. Description of\r\nLogistic Regression Model\r\n1.3. Model\r\nEstimation\r\n1.3.1. The concept of\r\nlikelihood\r\n1.3.2. Maximum likelihood\r\nestimation (MLE)\r\n1.3.3. The concept of the\r\nlog-likelihood\r\n1.3.4. MLE for\r\nLogistic Regression coefficients\r\n1.3.5. Logistic Loss function\r\n1.3.6. The\r\nglm function\r\n1.3.7.\r\nThe glmnet function\r\n\r\n1.4.\r\nEvaluating the Predictive Performance for Logistic Regression Models\r\n1.4.1. Accuracy of Class\r\nProbabilities\r\n1.4.2. Accuracy of Class\r\nPredictions\r\n\r\n1.5.\r\nBuilding a Prediction Model for Recidivism using the caret\r\npackage\r\n\r\n2. Regularization in\r\nLogistic Regression\r\n2.1 Ridge\r\nPenalty\r\nBuilding\r\na Classification Model with Ridge Penalty for Recidivism using the\r\ncaret package\r\n\r\n2.2. Lasso\r\nPenalty\r\nBuilding\r\na Classification Model with Lasso Penalty for Recidivism using the\r\ncaret package\r\n\r\n2.3. Elastic Net\r\nBuilding\r\na Classification Model with Elastic Net for Recidivism using the\r\ncaret package\r\n\r\n\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:05:58 ]\r\n1. Overview of the Logistic\r\nRegression\r\nLogistic regression is a type of model that can be used to predict a\r\nbinary outcome variable. Linear and logistic regression are indeed\r\nmembers of the same family of models called generalized linear\r\nmodels. While linear regression can also technically be used to\r\npredict a binary outcome, the bounded nature of a binary outcome, [0,1],\r\nmakes the linear regression solution suboptimal. Logistic regression is\r\na more appropriate model that considers the bounded nature of the binary\r\noutcome when making predictions.\r\nThe binary outcomes can be coded in various ways in the data, such as\r\n0 vs. 1, True vs. False, Yes vs. No, and Success vs. Failure. The rest\r\nof the notes assume that the outcome variable is coded as 0s and 1s. We\r\nare interested in predicting the probability that future observation\r\nwill belong to the class of 1s. The notes in this section will first\r\nintroduce a suboptimal solution to predict a binary outcome by fitting a\r\nlinear probability model using linear regression and discuss the\r\nlimitations of this approach. Then, the logistic regression model and\r\nits estimation will be demonstrated. Finally, we will discuss various\r\nmetrics that can be used to evaluate the performance of a logistic\r\nregression model.\r\nThroughout these notes, we will use the Recidivism\r\ndataset from the NIJ competition to discuss different aspects of\r\nlogistic regression and demonstrations. This data and variables in this\r\ndata were discussed in detail in Lecture 1\r\nand Lecture\r\n2a – Section 6. The outcome of interest to predict in this dataset\r\nis whether or not an individual will be recidivated in the second year\r\nafter initial release. To make demonstrations easier, I randomly sample\r\n20 observations from this data. Eight observations in this data have a\r\nvalue of 1 for the outcome (recidivated), and 12 observations have a\r\nvalue of 0 (not recidivated).\r\n\r\n\r\n# Import the randomly sample 20 observations from the recidivism dataset\r\n\r\nrecidivism_sub <- read.csv('./data/recidivism_sub.csv',header=TRUE)\r\n\r\n# Outcome variable\r\n\r\ntable(recidivism_sub$Recidivism_Arrest_Year2)\r\n\r\n\r\n 0  1 \r\n12  8 \r\n\r\n1.1. Linear Probability Model\r\nA linear probability model fits a typical regression model to a\r\nbinary outcome. When the outcome is binary, the predictions from a\r\nlinear regression model can be considered as the probability of the\r\noutcome being equal to 1,\r\n\\[\\hat{Y} = P(Y = 1).\\]\r\nSuppose we want to predict the recidivism in the second year\r\n(Recidivism_Arrest_Year2) by using the number of dependents\r\nthey have. Then, we could fit this using the lm\r\nfunction.\r\n\\[\\hat{Y} = P(Y = 1) = \\beta_0 + \\beta_1X\r\n+ \\epsilon\\]\r\n\r\n\r\nmod <- lm(Recidivism_Arrest_Year2 ~ 1 + Dependents,\r\n          data = recidivism_sub)\r\nsummary(mod)\r\n\r\n\r\nCall:\r\nlm(formula = Recidivism_Arrest_Year2 ~ 1 + Dependents, data = recidivism_sub)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-0.7500 -0.0625  0.0000  0.2500  0.5000 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value  Pr(>|t|)    \r\n(Intercept)  0.75000    0.12949   5.792 0.0000173 ***\r\nDependents  -0.25000    0.06825  -3.663   0.00178 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.3909 on 18 degrees of freedom\r\nMultiple R-squared:  0.4271,    Adjusted R-squared:  0.3953 \r\nF-statistic: 13.42 on 1 and 18 DF,  p-value: 0.001779\r\n\r\nThe intercept is 0.75 and the slope for the predictor,\r\nDependents, is -.25. We can interpret the intercept and\r\nslope as the following for this example. Note that the predicted values\r\nfrom this model can now be interpreted as probability predictions\r\nbecause the outcome is binary.\r\nIntercept (0.75): When the number of dependents is equal to 0,\r\nthe probability of being recidivated in Year 2 is 0.75.\r\nSlope (-0.25): For every additional dependent (one unit increase\r\nin X) the individual has, the probability of being recidivated in Year 2\r\nis reduced by .25.\r\nThe intercept and slope still represent the best-fitting line to our\r\ndata, and this fitted line can be shown here.\r\n\r\n\r\n\r\nSuppose we want to calculate the model’s predicted probability of\r\nbeing recidivated in Year 2 for a different number of dependents a\r\nparolee has. Let’s assume that the number of dependents can be from 0 to\r\n10. What would be the predicted probability of being recidivated in Year\r\n2 for a parolee with eight dependents?\r\n\r\n\r\nX <- data.frame(Dependents = 0:10)\r\npredict(mod,newdata = X)\r\n\r\n                        1                         2 \r\n 0.7500000000000000000000  0.4999999999999999444888 \r\n                        3                         4 \r\n 0.2499999999999998889777 -0.0000000000000002220446 \r\n                        5                         6 \r\n-0.2500000000000002220446 -0.5000000000000002220446 \r\n                        7                         8 \r\n-0.7500000000000004440892 -1.0000000000000004440892 \r\n                        9                        10 \r\n-1.2500000000000004440892 -1.5000000000000004440892 \r\n                       11 \r\n-1.7500000000000004440892 \r\n\r\nIt is not reasonable for a probability prediction to be negative. One\r\nof the major issues with using linear regression to predict a binary\r\noutcome using a linear-probability model is that the model predictions\r\ncan go outside of the boundary [0,1] and yield unreasonable predictions.\r\nSo, a linear regression model may not be the best tool to predict a\r\nbinary outcome. We should use a model that respects the boundaries of\r\nthe outcome variable.\r\n1.2. Description of\r\nLogistic Regression Model\r\nTo overcome the limitations of the linear probability model, we\r\nbundle our prediction model in a sigmoid function. Suppose there is a\r\nreal-valued function of \\(a\\) such\r\nthat\r\n\\[ f(a) = \\frac{e^a}{1 + e^a}.\r\n\\]\r\nThe output of this function is always between 0 and 1 regardless of\r\nthe value of \\(a\\). The sigmoid\r\nfunction is an appropriate choice for the logistic regression because it\r\nassures that the output is always bounded between 0 and 1.\r\nNote that this function can also be written as the following, and\r\nthey are mathematically equivalent.\r\n\\[ f(a) = \\frac{1}{1 + e^{-a}}.\r\n\\]\r\n\r\n\r\n\r\nIf we revisit the previous example, we can specify a logistic\r\nregression model to predict the probability of being recidivated in Year\r\n2 as the following by using the number of dependents a parolee has as\r\nthe predictor,\r\n\\[P(Y=1) =  \\frac{1}{1 +\r\ne^{-(\\beta_0+\\beta_1X)}}.\\]\r\nWhen values of a predictor variable is entered into the equation, the\r\nmodel output can be directly interpreted as the probability of the\r\nbinary outcome being equal to 1 (or whatever category and meaning a\r\nvalue of 1 represents). Then, we assume that the actual outcome follows\r\na binomial distribution with the predicted probability.\r\n\\[ P(Y=1) = p \\]\r\n\\[ Y \\sim Binomial(p)\\]\r\nSuppose the coefficient estimates of this model are \\(\\beta_0\\)=1.33 and \\(\\beta_1\\)=-1.62. Then, for instance, we can\r\ncompute the probability of being recidivated for a parolee with 8\r\ndependents as the following:\r\n\\[P(Y=1) =  \\frac{1}{1+e^{-(1.33-1.62\r\n\\times 8)}} = 0.0000088951098.\\]\r\n\r\n\r\n\r\n\r\n\r\nb0 = 1.33\r\nb1 = -1.62\r\nx = 0:10\r\ny = 1/(1+exp(-(b0+b1*x)))\r\ndata.frame(number.of.dependents = x, probability=y)\r\n\r\n   number.of.dependents     probability\r\n1                     0 0.7908406347869\r\n2                     1 0.4280038670685\r\n3                     2 0.1289808521462\r\n4                     3 0.0284705876901\r\n5                     4 0.0057659655589\r\n6                     5 0.0011463789579\r\n7                     6 0.0002270757033\r\n8                     7 0.0000449461727\r\n9                     8 0.0000088951098\r\n10                    9 0.0000017603432\r\n11                   10 0.0000003483701\r\n\r\n\r\n\r\n\r\nIn its original form, it is difficult to interpret the logistic\r\nregression parameters because a one unit increase in the predictor is no\r\nlonger linearly related to the probability of the outcome being equal to\r\n1 due to the nonlinear nature of the sigmoid function.\r\nThe most common presentation of logistic regression is obtained after\r\na bit of algebraic manipulation to rewrite the model equation. The\r\nlogistic regression model above can also be specified as the following\r\nwithout any loss of meaning as they are mathematically equivalent.\r\n\\[ln \\left [ \\frac{P(Y=1)}{1-P(Y=1)}\r\n\\right] =  \\beta_0+\\beta_1X.\\]\r\nThe term on the left side of the equation is known as the\r\nlogit. It is essentially the natural logarithm of odds.\r\nSo, when the outcome is a binary variable, the logit transformation of\r\nthe probability that the outcome is equal to 1 can be represented as a\r\nlinear equation. It provides a more straightforward interpretation. For\r\ninstance, we can now say that when the number of dependents is equal to\r\nzero, the predicted logit is equal to 1.33 (intercept), and for every\r\nadditional dependent, the logit decreases by 1.62 (slope).\r\nIt is common to transform the logit to odds when interpreting the\r\nparameters. For instance, we can say that when the number of dependents\r\nis equal to zero, the odds of being recidivated is 3.78 (\\(e^{1.33}\\)), and for every additional\r\ndependent the odds of being recidivated is reduced by about 80% (\\(1 - e^{-1.62}\\)).\r\nThe right side of the equation can be expanded by adding more\r\npredictors, adding polynomial terms of the predictors, or adding\r\ninteractions among predictors. A model with only the main effects of\r\n\\(P\\) predictors can be written as\r\n\\[ ln \\left [ \\frac{P(Y=1)}{1-P(Y=1)}\r\n\\right] = \\beta_0  + \\sum_{p=1}^{P} \\beta_pX_{p},\\] and the\r\ncoefficients can be interpreted as\r\n\\(\\beta_0\\): the predicted logit\r\nwhen the values for all the predictor variables in the model are equal\r\nto zero. \\(e^{\\beta_0}\\), the predicted\r\nodds of the outcome being equal to 1 when the values for all the\r\npredictor variables in the model are equal to zero.\r\n\\(\\beta_p\\): the change in the\r\npredicted logit for one unit increases in \\(X_p\\) when the values for all other\r\npredictors in the model are held constant. For every one unit in\r\nincrease in \\(X_p\\), the odds of the\r\noutcome being equal to 1 is multiplied by \\(e^{\\beta_p}\\) when the values for all other\r\npredictors in the model are held constant. In other words, \\(e^{\\beta_p}\\) is odds ratio, the ratio of\r\nodds at \\(\\beta_p = a+1\\) to the odds\r\nat \\(\\beta_p = a\\).\r\nIt is essential that you get familiar with the three concepts\r\n(probability, odds, logit) and how these three are related to each other\r\nfor interpreting the logistic regression parameters.\r\n\r\n\r\n\r\n\r\n\r\nNOTE\r\n\r\nThe sigmoid function is not the only tool to be used for modeling a\r\nbinary outcome. One can also use the cumulative standard normal\r\ndistribution function, \\(\\phi(a)\\), and\r\nthe output of \\(\\phi(a)\\) is also\r\nbounded between 0 and 1. When \\(\\phi\\)\r\nis used to transform the prediction model, this is known as\r\nprobit regression and serves the same purpose as the\r\nlogistic regression, which is to predict the probability of a binary\r\noutcome being equal to 1. However, it is always easier and more pleasant\r\nto work with logarithmic functions, which have better computational\r\nproperties. Therefore, logistic regression is more commonly used than\r\nprobit regression.\r\n\r\n1.3. Model Estimation\r\n1.3.1. The concept of likelihood\r\nIt is essential to understand the likelihood concept\r\nfor estimating the coefficients of a logistic regression model. We will\r\nconsider a simple example of flipping coins for this.\r\nSuppose you flip the same coin 20 times and observe the following\r\noutcome. We don’t know whether this is a fair coin in which the\r\nprobability of observing a head or tail is equal to 0.5.\r\n\\[\\mathbf{Y} = \\left ( H,H,H,T,H,H,H,T,H,T\r\n\\right )\\]\r\nSuppose we define \\(p\\) as the\r\nprobability of observing a head when we flip this coin. By definition,\r\nthe probability of observing a tail is \\(1-p\\).\r\n\\[P(Y=H) = p\\]\r\n\\[P(Y=T) = 1 - p\\]\r\nThen, we can calculate the likelihood of our observations of heads\r\nand tails as a function of \\(p\\).\r\n\\[ \\mathfrak{L}(\\mathbf{Y}|p) = p \\times p\r\n\\times p \\times (1-p) \\times p \\times p \\times p \\times (1-p) \\times p\r\n\\times (1-p) \\]\r\n\\[ \\mathfrak{L}(\\mathbf{Y}|p) = p^7 \\times\r\n(1-p)^3 \\] For instance, if we say that this is a fair coin and,\r\ntherefore, \\(p\\) is equal to 0.5, then\r\nthe likelihood of observing seven heads and three tails would be equal\r\nto\r\n\\[ \\mathfrak{L}(\\mathbf{Y}|p = 0.5) =\r\n0.5^7 \\times (1-0.5)^3 = 0.0009765625\\] On the other hand,\r\nanother person can say this is probably not a fair coin, and the \\(p\\) should be something higher than 0.5.\r\nHow about 0.65?\r\n\\[ \\mathfrak{L}(\\mathbf{Y}|p = 0.65) =\r\n0.65^7 \\times (1-0.65)^3 = 0.00210183\\] Based on our observation,\r\nwe can say that an estimate of \\(p\\)\r\nbeing equal to 0.65 is more likely than an estimate of \\(p\\) being equal to 0.5. Our observation of\r\n7 heads and 3 tails is more likely if we estimate \\(p\\) as 0.65 rather than 0.5.\r\n1.3.2. Maximum likelihood\r\nestimation (MLE)\r\nThen, what would be the best estimate of \\(p\\) given our observed data (seven heads\r\nand three tails)? We can try every possible value of \\(p\\) between 0 and 1 and calculate the\r\nlikelihood of our data (\\(\\mathbf{Y}\\)). Then, we can pick the value\r\nthat makes our data most likely (largest likelihood) to observe as our\r\nbest estimate. Given the data we observed (7 heads and three tails), it\r\nis called the maximum likelihood estimate of \\(p\\).\r\n\r\n\r\np <- seq(0,1,.001)\r\n\r\nL <- p^7*(1-p)^3\r\n\r\nggplot()+\r\n  geom_line(aes(x=p,y=L)) + \r\n  theme_bw() + \r\n  xlab('Probability of Observing a Head (p)')+\r\n  ylab('Likelihood of Observing 7 Heads and 3 Tails')+\r\n  geom_vline(xintercept=p[which.max(L)],lty=2)\r\n\r\n\r\n\r\nWe can show that the \\(p\\) value\r\nthat makes the likelihood largest is 0.7, and the likelihood of\r\nobserving seven heads and three tails is 0.002223566 when \\(p\\) is equal to 0.7. Therefore, the maximum\r\nlikelihood estimate of the probability of observing a head for this\r\nparticular coin is 0.7, given the ten observations we have made.\r\n\r\n\r\nL[which.max(L)]\r\n\r\n[1] 0.002223566\r\n\r\np[which.max(L)]\r\n\r\n[1] 0.7\r\n\r\nNote that our estimate can change and be updated if we continue\r\ncollecting more data by flipping the same coin and recording our\r\nobservations.\r\n1.3.3. The concept of the\r\nlog-likelihood\r\nThe computation of likelihood requires the multiplication of so many\r\n\\(p\\) values, and when you multiply\r\nvalues between 0 and 1, the result gets smaller and smaller. It creates\r\nproblems when you multiply so many of these small \\(p\\) values due to the maximum precision any\r\ncomputer can handle. For instance, you can see the minimum number that\r\ncan be represented in R in your local machine.\r\n\r\n\r\n.Machine$double.xmin\r\n\r\n[1] 2.225074e-308\r\n\r\nWhen you have hundreds of thousands of observations, it is probably\r\nnot a good idea to work directly with likelihood. Instead, we work with\r\nthe log of likelihood (log-likelihood). The log-likelihood has two main\r\nadvantages:\r\nWe are less concerned about the precision of small numbers our\r\ncomputer can handle.\r\nLog-likelihood has better mathematical properties for\r\noptimization problems (the log of the product of two numbers equals the\r\nsum of the log of the two numbers).\r\nThe point that maximizes likelihood is the same number that\r\nmaximizes the log-likelihood, so our end results (MLE estimate) do not\r\ncare if we use log-likelihood instead of likelihood.\r\n\\[ ln(\\mathfrak{L}(\\mathbf{Y}|p)) =\r\nln(lop^7 \\times (1-p)^3) \\] \\[\r\nln(\\mathfrak{L}(\\mathbf{Y}|p)) = ln(p^7) + ln((1-p)^3) \\] \\[ ln(\\mathfrak{L}(\\mathbf{Y}|p)) = 7 \\times ln(p)\r\n+ 3 \\times ln(1-p) \\]\r\n\r\n\r\np <- seq(0,1,.001)\r\n\r\nlogL <- log(p)*7 + log(1-p)*3\r\n\r\nggplot()+\r\n  geom_line(aes(x=p,y=logL)) + \r\n  theme_bw() + \r\n  xlab('Probability of Observing a Head (p)')+\r\n  ylab('Loglikelihood of Observing 7 Heads and 3 Tails')+\r\n  geom_vline(xintercept=p[which.max(logL)],lty=2)\r\n\r\n\r\n\r\n\r\n\r\nlogL[which.max(logL)]\r\n\r\n[1] -6.108643\r\n\r\np[which.max(logL)]\r\n\r\n[1] 0.7\r\n\r\n1.3.4. MLE for\r\nLogistic Regression coefficients\r\nNow, we can apply these concepts to estimate the logistic regression\r\ncoefficients. Let’s revisit our previous example in which we predict the\r\nprobability of being recidivated in Year 2, given the number of\r\ndependents a parolee has. Our model can be written as the following.\r\n\\[ln \\left [ \\frac{P_i(Y=1)}{1-P_i(Y=1)}\r\n\\right] =  \\beta_0+\\beta_1X_i.\\]\r\nNote that \\(X\\) and \\(P\\) have a subscript \\(i\\) to indicate that each individual may\r\nhave a different X value, and therefore each individual will have a\r\ndifferent probability. Our observed outcome is a set of 0s and 1s.\r\nRemember that eight individuals were recidivated (Y=1), and 12 were not\r\nrecidivated (Y=0).\r\n\r\n\r\nrecidivism_sub$Recidivism_Arrest_Year2\r\n\r\n [1] 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0\r\n\r\nGiven a set of coefficients, {\\(\\beta_0,\\beta_1\\)}, we can calculate the\r\nlogit for every observation using the model equation and then transform\r\nthis logit to a probability, \\(P_i(Y=1)\\). Finally, we can calculate the\r\nlog of the probability for each observation and sum them across\r\nobservations to obtain the log-likelihood of observing this set of\r\nobservations (12 zeros and eight ones). Suppose that we have two\r\nguesstimates for {\\(\\beta_0,\\beta_1\\)},\r\nwhich are 0.5 and -0.8, respectively. These coefficients imply the\r\nfollowing predicted model.\r\n\r\n\r\n\r\nIf these two coefficients were our estimates, how likely would we\r\nobserve the outcome in our data, given the number of dependents? The\r\nbelow R code first finds the predicted logit for every observation,\r\nassuming that \\(\\beta_0\\) = 0.5 and\r\n\\(\\beta_1\\) = -0.8.\r\n\r\n\r\nb0 = 0.5\r\nb1 = -0.8\r\n\r\nx = recidivism_sub$Dependents\r\n\r\ny = recidivism_sub$Recidivism_Arrest_Year2\r\n\r\npred_logit <- b0 + b1*x\r\n\r\npred_prob1 <- exp(pred_logit)/(1+exp(pred_logit))\r\n\r\npred_prob0 <- 1 - pred_prob1 \r\n\r\ndata.frame(Dependents      = x, \r\n           Recidivated     = y, \r\n           Prob1 = pred_prob1,\r\n           Prob0 = pred_prob0)\r\n\r\n   Dependents Recidivated     Prob1     Prob0\r\n1           0           1 0.6224593 0.3775407\r\n2           1           1 0.4255575 0.5744425\r\n3           2           0 0.2497399 0.7502601\r\n4           1           0 0.4255575 0.5744425\r\n5           1           1 0.4255575 0.5744425\r\n6           0           1 0.6224593 0.3775407\r\n7           0           1 0.6224593 0.3775407\r\n8           1           1 0.4255575 0.5744425\r\n9           3           0 0.1301085 0.8698915\r\n10          0           0 0.6224593 0.3775407\r\n11          1           0 0.4255575 0.5744425\r\n12          0           0 0.6224593 0.3775407\r\n13          0           1 0.6224593 0.3775407\r\n14          3           0 0.1301085 0.8698915\r\n15          3           0 0.1301085 0.8698915\r\n16          0           1 0.6224593 0.3775407\r\n17          3           0 0.1301085 0.8698915\r\n18          3           0 0.1301085 0.8698915\r\n19          3           0 0.1301085 0.8698915\r\n20          3           0 0.1301085 0.8698915\r\n\r\n\r\n\r\nlogL <-  y*log(pred_prob1) + (1-y)*log(pred_prob0)\r\nsum(logL)\r\n\r\n[1] -9.253358\r\n\r\nWe can summarize this by saying that if our model coefficients were\r\n\\(\\beta_0\\) = 0.5 and \\(\\beta_1\\) = -0.8, then the log of the\r\nlikelihood of observing the outcome in our data would be -9.25.\r\n\\[\\mathbf{Y} = \\left (\r\n1,0,1,0,0,0,0,1,1,0,0,1,0,0,0,1,0,0,0,0 \\right )\\]\r\n\\[ \\mathfrak{logL}(\\mathbf{Y}|\\beta_0 =\r\n0.5,\\beta_1 = -0.8) = -9.25\\]\r\nThe critical question is whether or not there is another pair of\r\nvalues we can assign to \\(\\beta_0\\) and\r\n\\(\\beta_1\\) that would provide a higher\r\nlikelihood of data. If there are, then they would be better estimates\r\nfor our model. If we can find such a pair with the maximum\r\nlog-likelihood of data, then they would be our maximum likelihood\r\nestimates for the given model.\r\nWe can approach this problem crudely to gain some intuition about\r\nMaximum Likelihood Estimation. Suppose that a reasonable range of values\r\nfor \\(\\beta_0\\) is from -3 to 3, and\r\nfor \\(\\beta_1\\) is from -3 to 3. Let’s\r\nthink about every possible combinations of values for \\(\\beta_0\\) and \\(\\beta_1\\) within these ranges with\r\nincrements of .01. Then, let’s calculate the log-likelihood of data for\r\nevery possible combination and plot these in a 3D plot as a function of\r\n\\(\\beta_0\\) and \\(\\beta_1\\).\r\n\r\ngrid    <- expand.grid(b0=seq(-3,3,.01),b1=seq(-3,3,.01))           \r\n\r\ngrid$logL <- NA\r\n\r\nfor(i in 1○:nrow(grid)){\r\n  \r\n  x = recidivism_sub$Dependents\r\n  y = recidivism_sub$Recidivism_Arrest_Year2\r\n\r\n  pred_logit    <- grid[i,]$b0 + grid[i,]$b1*x\r\n  pred_prob1    <- exp(pred_logit)/(1+exp(pred_logit))\r\n  pred_prob0    <- 1 - pred_prob1 \r\n  logL          <- y*log(pred_prob1) + (1-y)*log(pred_prob0)\r\n  grid[i,]$logL <- sum(logL)\r\n  \r\n  print(i)\r\n}\r\n\r\n  \r\nrequire(plotly)\r\n\r\nplot_ly(grid, x = ~b0, y = ~b1, z = ~logL, \r\n        marker = list(color = ~logL,\r\n                      showscale = FALSE,\r\n                      cmin=min(grid$logL),\r\n                      cmax=max(grid$logL),cauto=F),\r\n        width=600,height=600) %>% \r\n  add_markers()\r\n\r\n\r\n\r\n\r\nWhat is the maximum point of this surface? Our simple search\r\nindicates that it is -8.30, and the set of \\(\\beta_0\\) and \\(\\beta_1\\) coefficients that make the\r\nobserved data most likely is 1.33 and -1.62.\r\n\r\n\r\ngrid[which.max(grid$logL),]\r\n\r\n        b0    b1      logL\r\n83372 1.33 -1.62 -8.306004\r\n\r\nTherefore, given our dataset with 20 observations, our maximum\r\nlikelihood estimates for the coefficients of the logistic regression\r\nmodel above are 1.33 and -1.62.\r\n\\[ln \\left [ \\frac{P_i(Y=1)}{1-P_i(Y=1)}\r\n\\right] =  1.33 - 1.62 \\times X_i.\\]\r\n1.3.5. Logistic Loss function\r\nBelow is a compact way of writing likelihood and log-likelihood in\r\nmathematical notation. For simplification purposes, we write \\(P_i\\) to represent \\(P_i(Y=1)\\).\r\n\\[\r\n\\mathfrak{L}(\\mathbf{Y}|\\boldsymbol\\beta) = \\prod_{i=1}^{N} P_i^{y_i}\r\n\\times (1-P_i)^{1-y_i}\\]\r\n\\[\r\n\\mathfrak{logL}(\\mathbf{Y}|\\boldsymbol\\beta) = \\sum_{i=1}^{N} Y_i \\times\r\nln(P_i) + (1-Y_i) \\times ln(1-P_i)\\]\r\nThe final equation above, \\(\\mathfrak{logL}(\\mathbf{Y}|\\boldsymbol\\beta)\\),\r\nis known as the logistic loss function.\r\nBy finding the set of coefficients in a model, \\(\\boldsymbol\\beta = (\\beta_0,\r\n\\beta_1,...,\\beta_P)\\), that maximizes this quantity, we obtain\r\nthe maximum likelihood estimates of the coefficients for the logistic\r\nregression model.\r\nUnfortunately, the naive crude search we applied above would be\r\ninefficient when you have a complex model with many predictors. Another\r\nunfortunate thing is that there is no closed-form solution (as we had\r\nfor the linear regression) for the logistic regression. Therefore, the\r\nonly way to estimate the logistic regression coefficients is to use\r\nnumerical approximations and computational algorithms to maximize the\r\nlogistic loss function. Luckily, we have tools available to accomplish\r\nthis task.\r\n\r\n\r\nNOTE\r\n\r\nWhy do we not use least square estimation and minimize the sum of\r\nsquared residuals when estimating the coefficients of the logistic\r\nregression model? We can certainly use the sum of squared residuals as\r\nour loss function and minimize it to estimate the coefficients for the\r\nlogistic regression, just like we did for the linear regression. The\r\ncomplication is that the sum of the squared residuals function yields a\r\nnon-convex surface when the outcome is binary as opposed to a convex\r\nsurface obtained from the logistic loss function. Non-convex\r\noptimization problems are more challenging than convex optimization\r\nproblems, and they are more vulnerable to finding sub-optimal solutions\r\n(local minima/maxima). Therefore, the logistic loss function and\r\nmaximizing it is preferred when estimating the coefficients of a\r\nlogistic regression model.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n1.3.6. The glm function\r\nThe glm() function as a part of the base\r\nstats package can be used to estimate the logistic\r\nregression coefficients. The use of the glm() function is\r\nvery similar to the lm() function. The only difference is\r\nthat we specify the family='binomial' argument to fit the\r\nlogistic regression by maximizing the logistic loss function.\r\n\r\n\r\nmod <- glm(Recidivism_Arrest_Year2 ~ 1 + Dependents,\r\n           data   = recidivism_sub,\r\n           family = 'binomial')\r\n\r\nsummary(mod)\r\n\r\n\r\nCall:\r\nglm(formula = Recidivism_Arrest_Year2 ~ 1 + Dependents, family = \"binomial\", \r\n    data = recidivism_sub)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-1.7670  -0.3125  -0.2412   0.6864   1.3031  \r\n\r\nCoefficients:\r\n            Estimate Std. Error z value Pr(>|z|)  \r\n(Intercept)   1.3255     0.8204   1.616   0.1061  \r\nDependents   -1.6161     0.7269  -2.223   0.0262 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 26.920  on 19  degrees of freedom\r\nResidual deviance: 16.612  on 18  degrees of freedom\r\nAIC: 20.612\r\n\r\nNumber of Fisher Scoring iterations: 5\r\n\r\nIn the Coefficients table, the numbers under the\r\nEstimate column are the estimated coefficients for the\r\nlogistic regression model. The quantity labeled as the Residual\r\nDeviance in the output is twice the maximized\r\nlog-likelihood,\r\n\\[ Deviance  = -2 \\times\r\n\\mathfrak{logL}(\\mathbf{Y}|\\boldsymbol\\beta). \\]\r\nNotice that the coefficient estimates from the glm()\r\nfunction are very close to our crude estimates from a brute-force search\r\nin an earlier section (1.33 and -1.62). From our simple search, we found\r\nthe maximum log-likelihood as -8.3. So, if we multiply that number by\r\n-2, that equals 16.6, which is the number reported in this output as\r\nResidual Deviance.\r\n1.3.7. The glmnet\r\nfunction\r\nYou can also use the glmnet() function from the\r\nglmnet package to fit the logistic regression. The\r\nadvantage of the glmnet package is that it allows\r\nregularization while fitting the logistic regression. You can set the\r\nalpha=0 and lambda=0 arguments to obtain the\r\ncoefficient estimates without penalty.\r\n\r\n\r\nrequire(glmnet)\r\n\r\nmod <- glmnet(x         = cbind(0,recidivism_sub$Dependents),\r\n              y         = factor(recidivism_sub$Recidivism_Arrest_Year2),\r\n              family    = 'binomial',\r\n              alpha     = 0,\r\n              lambda    = 0,\r\n              intercept = TRUE)\r\n\r\ncoef(mod)\r\n\r\n3 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                   s0\r\n(Intercept)  1.325172\r\nV1           .       \r\nV2          -1.615664\r\n\r\nThe x argument is the input matrix for predictors, and\r\nthe\r\ny' argument is a vector of binary response outcome. Theglmnetrequires they’\r\nargument to be a factor with two levels.\r\nNote that I defined the x argument above as\r\ncbind(0,recidivism_sub$Dependents) because\r\nglmnet requires the x to be a matrix with at\r\nleast two columns. So, I added a column of zeros to trick the function\r\nand force it to run. That column of zeros has zero impact on the\r\nestimation.\r\n1.4.\r\nEvaluating the Predictive Performance for Logistic Regression\r\nModels\r\nWhen the outcome is a binary variable, classification models, such as\r\nlogistic regression), typically yield a probability estimate for a class\r\nmembership (or a continuous-valued prediction between 0 and 1).\r\nFor instance, we can obtain the model predictive probabilities of\r\nbeing recidivated in Year 2 after fitting a simple logistic regression\r\nas the number of dependents is the predictor.\r\n\r\n\r\nmod <- glm(Recidivism_Arrest_Year2 ~ 1 + Dependents,\r\n           data   = recidivism_sub,\r\n           family = 'binomial')\r\n\r\nrecidivism_sub$pred_prob <- predict(mod,type='response')\r\n\r\nrecidivism_sub[,c('ID','Dependents','Recidivism_Arrest_Year2','pred_prob')]\r\n\r\n      ID Dependents Recidivism_Arrest_Year2  pred_prob\r\n1  21953          0                       1 0.79010012\r\n2   8255          1                       1 0.42785504\r\n3   9110          2                       0 0.12934698\r\n4  20795          1                       0 0.42785504\r\n5   5569          1                       1 0.42785504\r\n6  14124          0                       1 0.79010012\r\n7  24979          0                       1 0.79010012\r\n8   4827          1                       1 0.42785504\r\n9  26586          3                       0 0.02866814\r\n10 17777          0                       0 0.79010012\r\n11 22269          1                       0 0.42785504\r\n12 25016          0                       0 0.79010012\r\n13 24138          0                       1 0.79010012\r\n14 12261          3                       0 0.02866814\r\n15 15417          3                       0 0.02866814\r\n16 14695          0                       1 0.79010012\r\n17  4371          3                       0 0.02866814\r\n18 13529          3                       0 0.02866814\r\n19 25046          3                       0 0.02866814\r\n20  5340          3                       0 0.02866814\r\n\r\nIn an ideal situation where a model does a perfect job of predicting\r\na binary outcome, we expect all those observations in Group 0 (Not\r\nRecidivated) to have a predicted probability of 0 and all those\r\nobservations in Group 1 (Recidivated) to have a predicted probability of\r\n1. So, predicted values close to 0 for observations in Group 0 and those\r\nclose to 1 for Group 1 are desirable.\r\nOne way to look at the quality of predictions for a binary outcome is\r\nto examine the distribution of predictions for both categories of the\r\noutcome variable. For this small demo set with 20 observations, we can\r\nsee that these distributions are not separated very well. This plot\r\nimplies that our model predictions don’t properly separate the two\r\nclasses in the outcome variable. The more separation between the\r\ndistribution of two classes, the better the model performance is.\r\nWhat would such a plot look like for a model with perfect\r\nclassification?\r\nThis visualization is a very intuitive initial look at the model\r\nperformance. Later, we will see numerical summaries of the separation\r\nbetween these two distributions.\r\n\r\n\r\n\r\n1.4.1. Accuracy of Class\r\nProbabilities\r\nWe can calculate MAE and RMSE to measure the accuracy of predicted\r\nprobabilities when the outcome is binary, and they have the same\r\ndefinition as in the continuous case.\r\n\r\n\r\nouts  <- recidivism_sub$Recidivism_Arrest_Year2\r\npreds <- recidivism_sub$pred_prob\r\n\r\n# Mean absolute error\r\n\r\nmean(abs(outs - preds))\r\n\r\n[1] 0.2765934\r\n\r\n# Root mean squared error\r\n\r\nsqrt(mean((outs - preds)^2))\r\n\r\n[1] 0.376793\r\n\r\n1.4.2. Accuracy of Class\r\nPredictions\r\nConfusion Matrix and\r\nRelated Metrics\r\nIn most situations, the accuracy of class probabilities is not very\r\nuseful because one has to decide and place these observations in a group\r\nbased on these probabilities for practical reasons. For instance,\r\nsuppose you are predicting whether or not an individual will be\r\nrecidivated so you can take action based on this decision. Then, it\r\nwould be best if you transformed this continuous probability (or\r\nprobability-like score) predicted by a model into a binary prediction.\r\nTherefore, one has to determine an arbitrary cut-off value. Once a\r\ncut-off value is determined, then we can generate class predictions.\r\nConsider that we use a cut-off value of 0.5. So, if an observation\r\nhas a predicted class probability less than 0.5, we predict that this\r\nperson is in Group 0 (Not Recidivated). Likewise, if an observation has\r\na predicted class probability higher than 0.5, we predict that this\r\nperson is in Group 1.\r\nIn the code below, I convert the class probabilities to class\r\npredictions using a cut-off value of 0.5.\r\n\r\n\r\nrecidivism_sub$pred_class <- ifelse(recidivism_sub$pred_prob>.5,1,0)\r\n\r\nrecidivism_sub[,c('ID','Dependents','Recidivism_Arrest_Year2','pred_prob','pred_class')]\r\n\r\n      ID Dependents Recidivism_Arrest_Year2  pred_prob pred_class\r\n1  21953          0                       1 0.79010012          1\r\n2   8255          1                       1 0.42785504          0\r\n3   9110          2                       0 0.12934698          0\r\n4  20795          1                       0 0.42785504          0\r\n5   5569          1                       1 0.42785504          0\r\n6  14124          0                       1 0.79010012          1\r\n7  24979          0                       1 0.79010012          1\r\n8   4827          1                       1 0.42785504          0\r\n9  26586          3                       0 0.02866814          0\r\n10 17777          0                       0 0.79010012          1\r\n11 22269          1                       0 0.42785504          0\r\n12 25016          0                       0 0.79010012          1\r\n13 24138          0                       1 0.79010012          1\r\n14 12261          3                       0 0.02866814          0\r\n15 15417          3                       0 0.02866814          0\r\n16 14695          0                       1 0.79010012          1\r\n17  4371          3                       0 0.02866814          0\r\n18 13529          3                       0 0.02866814          0\r\n19 25046          3                       0 0.02866814          0\r\n20  5340          3                       0 0.02866814          0\r\n\r\nWe can summarize the relationship between the binary outcome and\r\nbinary prediction in a 2 x 2 table. This table is commonly referred to\r\nas confusion matrix.\r\n\r\n\r\ntab <- table(recidivism_sub$pred_class,\r\n             recidivism_sub$Recidivism_Arrest_Year2,\r\n             dnn = c('Predicted','Observed'))\r\ntab\r\n\r\n         Observed\r\nPredicted  0  1\r\n        0 10  3\r\n        1  2  5\r\n\r\nThe table indicates that there are 12 observations with an outcome\r\nvalue of 0. The model accurately predicts the class membership as 0 for\r\nten observation while inaccurately predicts class membership as 1 for\r\ntwo observations. The table also indicates that there are eight\r\nobservations with an outcome value of 1. The model accurately predicts\r\nthe class membership as 1 for 5 observations while inaccurately predicts\r\nclass membership as 0 for three observations.\r\nBased on the elements of this table, we can define four key\r\nconcepts:\r\nTrue Positives(TP): True positives are the\r\nobservations where both the outcome and prediction are equal to\r\n1.\r\nTrue Negative(TN): True negatives are the\r\nobservations where both the outcome and prediction are equal to\r\n0.\r\nFalse Positives(FP): False positives are the\r\nobservations where the outcome is 0 but the prediction is 1.\r\nFalse Negatives(FN): False negatives are the\r\nobservations where the outcome is 1 but the prediction is 0.\r\n\r\n\r\ntn <- tab[1,1]\r\ntp <- tab[2,2]\r\nfp <- tab[2,1]\r\nfn <- tab[1,2]\r\n\r\n\r\nSeveral metrics can be defined based on these four variables. We\r\ndefine a few important metrics below.\r\naccuracy: Overall accuracy simply represent the\r\nproportion of correct predictions.\r\n\\[ACC = \\frac{TP + TN}{TP + TN + FP +\r\nFN}\\]\r\n\r\n\r\nacc <- (tp + tn)/(tp+tn+fp+fn)\r\n\r\nacc\r\n\r\n[1] 0.75\r\n\r\nTrue Positive Rate (Sensitivity): True positive\r\nrate (a.k.a. sensitivity) is the proportion of correct predictions for\r\nthose observations the outcome is 1 (event is observed).\r\n\\[TPR = \\frac{TP}{TP + FN}\\]\r\n\r\n\r\ntpr <- (tp)/(tp+fn)\r\n\r\ntpr\r\n\r\n[1] 0.625\r\n\r\nTrue Negative Rate (Specificity): True negative\r\nrate (a.k.a. specificity) is the proportion of correct predictions for\r\nthose observations the outcome is 0 (event is not observed).\r\n\\[TNR = \\frac{TN}{TN + FP}\\]\r\n\r\n\r\ntnr <- (tn)/(tn+fp)\r\n\r\ntnr\r\n\r\n[1] 0.8333333\r\n\r\nPositive predicted value (Precision): Positive\r\npredicted value (a.k.a. precision) is the proportion of correct\r\ndecisions when the model predicts that the outcome is 1.\r\n\\[PPV = \\frac{TP}{TP + FP}\\]\r\n\r\n\r\nppv <- (tp)/(tp+fp)\r\n\r\nppv\r\n\r\n[1] 0.7142857\r\n\r\nF1 score: F1 score is a metric that combines both\r\nPPV and TPR.\r\n\\[F1 = 2*\\frac{PPV*TPR}{PPV +\r\nTPR}\\]\r\n\r\n\r\nf1 <- (2*ppv*tpr)/(ppv+tpr)\r\n\r\nf1\r\n\r\n[1] 0.6666667\r\n\r\nArea Under\r\nthe Receiver Operating Curve (AUC or AUROC)\r\nThe confusion matrix and related metrics all depend on the arbitrary\r\ncut-off value one picks when transforming continuous predicted\r\nprobabilities to binary predicted classes. We can change the cut-off\r\nvalue to optimize certain metrics, and there is always a trade-off\r\nbetween these metrics for different cut-off values. For instance, let’s\r\npick different cut-off values and calculate these metrics for each\r\none.\r\n\r\n\r\n# Write a generic function to return the metric for a given vector of observed \r\n# outcome, predicted probability and cut-off value\r\n\r\ncmat <- function(x,y,cut){\r\n  # x, a vector of predicted probabilities\r\n  # y, a vector of observed outcomes\r\n  # cut, user-defined cut-off value\r\n \r\n  x_ <- ifelse(x>cut,1,0)\r\n    \r\n  tn <- sum(x_==0 & y==0)\r\n  tp <- sum(x_==1 & y==1)\r\n  fp <- sum(x_==1 & y==0)\r\n  fn <- sum(x_==0 & y==1)\r\n  \r\n  acc <- (tp + tn)/(tp+tn+fp+fn)\r\n  tpr <- (tp)/(tp+fn)\r\n  tnr <- (tn)/(tn+fp)\r\n  ppv <- (tp)/(tp+fp)\r\n  f1 <- (2*ppv*tpr)/(ppv+tpr)\r\n\r\n  return(list(acc=acc,tpr=tpr,tnr=tnr,ppv=ppv,f1=f1))\r\n}\r\n\r\n# Try it out\r\n\r\n  #cmat(x=recidivism_sub$pred_prob,\r\n  #     y=recidivism_sub$Recidivism_Arrest_Year2,\r\n  #     cut=0.5)\r\n\r\n# Do it for different cut-off values\r\n\r\nmetrics <- data.frame(cut=seq(0,1,0.01),\r\n                      acc=NA,\r\n                      tpr=NA,\r\n                      tnr=NA,\r\n                      ppv=NA,\r\n                      f1=NA)\r\n\r\n\r\nfor(i in 1:nrow(metrics)){\r\n  \r\n  cmat_ <- cmat(x   = recidivism_sub$pred_prob,\r\n                y   = recidivism_sub$Recidivism_Arrest_Year2,\r\n                cut = metrics[i,1])\r\n  \r\n  metrics[i,2:6] = c(cmat_$acc,cmat_$tpr,cmat_$tnr,cmat_$ppv,cmat_$f1)\r\n\r\n}\r\n\r\nmetrics\r\n\r\n     cut  acc   tpr       tnr       ppv        f1\r\n1   0.00 0.40 1.000 0.0000000 0.4000000 0.5714286\r\n2   0.01 0.40 1.000 0.0000000 0.4000000 0.5714286\r\n3   0.02 0.40 1.000 0.0000000 0.4000000 0.5714286\r\n4   0.03 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n5   0.04 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n6   0.05 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n7   0.06 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n8   0.07 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n9   0.08 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n10  0.09 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n11  0.10 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n12  0.11 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n13  0.12 0.75 1.000 0.5833333 0.6153846 0.7619048\r\n14  0.13 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n15  0.14 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n16  0.15 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n17  0.16 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n18  0.17 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n19  0.18 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n20  0.19 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n21  0.20 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n22  0.21 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n23  0.22 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n24  0.23 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n25  0.24 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n26  0.25 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n27  0.26 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n28  0.27 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n29  0.28 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n30  0.29 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n31  0.30 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n32  0.31 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n33  0.32 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n34  0.33 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n35  0.34 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n36  0.35 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n37  0.36 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n38  0.37 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n39  0.38 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n40  0.39 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n41  0.40 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n42  0.41 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n43  0.42 0.80 1.000 0.6666667 0.6666667 0.8000000\r\n44  0.43 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n45  0.44 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n46  0.45 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n47  0.46 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n48  0.47 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n49  0.48 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n50  0.49 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n51  0.50 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n52  0.51 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n53  0.52 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n54  0.53 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n55  0.54 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n56  0.55 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n57  0.56 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n58  0.57 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n59  0.58 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n60  0.59 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n61  0.60 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n62  0.61 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n63  0.62 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n64  0.63 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n65  0.64 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n66  0.65 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n67  0.66 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n68  0.67 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n69  0.68 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n70  0.69 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n71  0.70 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n72  0.71 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n73  0.72 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n74  0.73 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n75  0.74 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n76  0.75 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n77  0.76 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n78  0.77 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n79  0.78 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n80  0.79 0.75 0.625 0.8333333 0.7142857 0.6666667\r\n81  0.80 0.60 0.000 1.0000000       NaN       NaN\r\n82  0.81 0.60 0.000 1.0000000       NaN       NaN\r\n83  0.82 0.60 0.000 1.0000000       NaN       NaN\r\n84  0.83 0.60 0.000 1.0000000       NaN       NaN\r\n85  0.84 0.60 0.000 1.0000000       NaN       NaN\r\n86  0.85 0.60 0.000 1.0000000       NaN       NaN\r\n87  0.86 0.60 0.000 1.0000000       NaN       NaN\r\n88  0.87 0.60 0.000 1.0000000       NaN       NaN\r\n89  0.88 0.60 0.000 1.0000000       NaN       NaN\r\n90  0.89 0.60 0.000 1.0000000       NaN       NaN\r\n91  0.90 0.60 0.000 1.0000000       NaN       NaN\r\n92  0.91 0.60 0.000 1.0000000       NaN       NaN\r\n93  0.92 0.60 0.000 1.0000000       NaN       NaN\r\n94  0.93 0.60 0.000 1.0000000       NaN       NaN\r\n95  0.94 0.60 0.000 1.0000000       NaN       NaN\r\n96  0.95 0.60 0.000 1.0000000       NaN       NaN\r\n97  0.96 0.60 0.000 1.0000000       NaN       NaN\r\n98  0.97 0.60 0.000 1.0000000       NaN       NaN\r\n99  0.98 0.60 0.000 1.0000000       NaN       NaN\r\n100 0.99 0.60 0.000 1.0000000       NaN       NaN\r\n101 1.00 0.60 0.000 1.0000000       NaN       NaN\r\n\r\nNotice the trade-off between TPR (sensitivity) and TNR (specificity).\r\nThe more conservative cut-off value we pick (a higher probability), the\r\nhigher TNR is and the lower TPR is.\r\nA receiver operating characteristic curve (ROC) is plot that\r\nrepresents this dynamic relationship between TPR and TNR for varying\r\nlevels of a cut-off value. It looks a little strange here because we\r\nonly have 20 observations.\r\n\r\n\r\nggplot(data = metrics, aes(x=1-tnr,y=tpr))+\r\n  geom_line()+\r\n  xlab('1-TNR (FP)')+\r\n  ylab('TPR')+\r\n  geom_abline(lty=2)+\r\n  theme_bw()\r\n\r\n\r\n\r\nROC may be a useful plot to inform about model’s predictive power as\r\nwell as choosing an optimal cut-off value. The area under the ROC curve\r\n(AUC or AUROC) is typically used to evaluate the predictive power of\r\nclassification models. The diagonal line in this plot represents a\r\nhypothetical model with no predictive power and AUC for the diagonal\r\nline is 0.5 (it is half of the whole square). The more ROC curve\r\nresembles with the diagonal line, less the predictive power is. It is\r\nnot easy to calculate AUC by hand or straight formula as it requires\r\ncalculus and numerical approximations. There are many alternatives in R\r\nto calculate AUC. We will use the cutpointr package as it\r\nalso provides other tools to select an optimal cut-off point.\r\n\r\n\r\n# install.packages('cutpointr')\r\n\r\nrequire(cutpointr)\r\n\r\ncut.obj <- cutpointr(x     = recidivism_sub$pred_prob,\r\n                     class = recidivism_sub$Recidivism_Arrest_Year2)\r\n\r\nplot(cut.obj)\r\n\r\n\r\ncut.obj$optimal_cutpoint\r\n\r\n[1] 0.427855\r\n\r\ncut.obj$AUC\r\n\r\n[1] 0.8541667\r\n\r\nWe see that AUC for the predictions in the hypothetical dataset is\r\n0.854. This can be considered as good in terms of predictive power. The\r\ncloser AUC is to 1, the more predictive power the model has. The closer\r\nAUC is to 0.5, the closer predictive power is to random guessing. The\r\nmagnitude of AUC is also closely related to how well the predicted\r\nprobabilities are separated for two classes.\r\nThe cutpointr package provides more in terms of finding\r\noptimal values to maximize certain metrics. For instance, suppose we\r\nwant to find the optimal cut-off value that maximizes the sum of\r\nspecificity and sensitivity.\r\n\r\n\r\ncut.obj <- cutpointr(x      = recidivism_sub$pred_prob,\r\n                     class  = recidivism_sub$Recidivism_Arrest_Year2,\r\n                     method = maximize_metric,\r\n                     metric = sum_sens_spec)\r\n\r\ncut.obj$optimal_cutpoint\r\n\r\n[1] 0.427855\r\n\r\nplot(cut.obj)\r\n\r\n\r\n\r\nYou can also create custom cost functions to maximize if you can\r\nattach a $ value to TP, FP, TN, FN. For instance, suppose that a\r\ntrue-positive prediction made by the model brings a 5 dollars profit and\r\na false-positive prediction made by the model costs 1 dollars. A\r\ntrue-negative or a false-negative doesn’t cost or profit anything. Then,\r\nyou can find an optimal cut-off value that maximizes the total\r\nprofit.\r\n\r\n\r\n# Custom function\r\n\r\ncost <- function(tp,fp,tn,fn,...){\r\n  my.cost <- matrix(5*tp - 1*fp + 0*tn + 0*fn, ncol = 1)\r\n  colnames(my.cost) <- 'my.cost'\r\n  return(my.cost)\r\n}\r\n\r\ncut.obj <- cutpointr(x      = recidivism_sub$pred_prob,\r\n                     class  = recidivism_sub$Recidivism_Arrest_Year2,\r\n                     method = maximize_metric,\r\n                     metric = cost)\r\n\r\ncut.obj$optimal_cutpoint\r\n\r\n[1] 0.427855\r\n\r\nplot(cut.obj)\r\n\r\n\r\n\r\nCheck this\r\npage to find more information about the cutpointr\r\npackage.\r\n1.5.\r\nBuilding a Prediction Model for Recidivism using the caret\r\npackage\r\nPlease review the following notebook that builds a classification\r\nmodel using the logistic regression for the full recidivism dataset.\r\nBuilding\r\na Logistic Regression Model\r\n2. Regularization in\r\nLogistic Regression\r\nThe regularization works similarly in logistic regression, as\r\ndiscussed in linear regression. We add penalty terms to the loss\r\nfunction to avoid large coefficients, and we reduce model variance by\r\nincluding a penalty term in exchange for adding bias. Optimizing the\r\npenalty degree via tuning, we can typically get models with better\r\nperformance than a logistic regression with no regularization.\r\n2.1 Ridge Penalty\r\nThe loss function with ridge penalty applied in logistic regression\r\nis the following:\r\n\\[\r\n\\mathfrak{logL}(\\mathbf{Y}|\\boldsymbol\\beta) = \\left ( \\sum_{i=1}^{N}\r\nY_i \\times ln(P_i) + (1-Y_i) \\times ln(1-P_i) \\right ) -\r\n\\frac{\\lambda}{2} \\sum_{i=1}^{P}\\beta_p^2\\]\r\nNotice that we subtract the penalty from the logistic loss because we\r\nmaximize the quantity. The penalty term has the same effect in the\r\nlogistic regression, and it will pull the regression coefficients toward\r\nzero but will not make them exactly equal to zero. Below, you can see a\r\nplot of change in logistic regression coefficients for some of the\r\nvariables from a model that will be fit in the next section at\r\nincreasing levels of ridge penalty.\r\n\r\n\r\n\r\n\r\n\r\nNOTE\r\n\r\nThe glmnet package divides the value of the loss\r\nfunction by sample size (\\(N\\)) during\r\nthe optimization (Equation 12 and 2-3 in this paper).\r\nIt technically does not change anything in terms of the optimal\r\nsolution. On the other hand, we should be aware that the ridge penalty\r\nbeing applied in the `glmnet package has become\r\n\\[N\\frac{\\lambda}{2}\\sum_{i=1}^{P}\\beta_p^2.\\]\r\nThis information may be necessary while searching plausible values of\r\n\\(\\lambda\\) for the glmnet\r\npackage.\r\n\r\nBuilding\r\na Classification Model with Ridge Penalty for Recidivism using the\r\ncaret package\r\nPlease review the following notebook that builds a classification\r\nmodel using the logistic regression with ridge penalty for the full\r\nrecidivism dataset.\r\nBuilding\r\na Classification Model with Ridge Penalty\r\n2.2. Lasso Penalty\r\nThe loss function with the lasso penalty applied in logistic\r\nregression is the following: \\[\r\n\\mathfrak{logL}(\\mathbf{Y}|\\boldsymbol\\beta) = \\left ( \\sum_{i=1}^{N}\r\nY_i \\times ln(P_i) + (1-Y_i) \\times ln(1-P_i) \\right)- \\lambda\r\n\\sum_{i=1}^{P}|\\beta_p|\\]\r\nThe penalty term has the same effect in the logistic regression, and\r\nit will make the regression coefficients equal to zero when a\r\nsufficiently large \\(\\lambda\\) value is\r\napplied. Below, you can see a plot of change in logistic regression\r\ncoefficients for some of the variables from a model that will be fit in\r\nthe next section at increasing levels of lasso penalty.\r\n\r\n\r\n\r\n\r\n\r\nNOTE\r\n\r\nNote that the glmnet package divides the value of the\r\nloss function by sample (\\(N\\)) during\r\nthe optimization (Equation 12 and 2-3 in this paper).\r\nYou should be aware that the lasso penalty being applied in the `glmnet\r\npackage has become\r\n\\[N\\lambda\\sum_{i=1}^{P}|\\beta_p|.\\]\r\nThis information may be important while searching plausible values of\r\n\\(\\lambda\\) for the glmnet\r\npackage.\r\n\r\nBuilding\r\na Classification Model with Lasso Penalty for Recidivism using the\r\ncaret package\r\nPlease review the following notebook that builds a classification\r\nmodel using the logistic regression with lasso penalty for the full\r\nrecidivism dataset.\r\nBuilding\r\na Classification Model with Lasso Penalty\r\n2.3. Elastic Net\r\nThe loss function with the elastic applied is the following:\r\n\\[\r\n\\mathfrak{logL}(\\mathbf{Y}|\\boldsymbol\\beta) = \\left ( \\sum_{i=1}^{N}\r\nY_i \\times ln(P_i) + (1-Y_i) \\times ln(1-P_i) \\right)- \\left\r\n((1-\\alpha)\\frac{\\lambda}{2} \\sum_{i=1}^{P}\\beta_p^2 + \\alpha  \\lambda\r\n\\sum_{i=1}^{P}|\\beta_p| \\right)\\]\r\nBuilding\r\na Classification Model with Elastic Net for Recidivism using the\r\ncaret package\r\nPlease review the following notebook that builds a classification\r\nmodel using the logistic regression with elastic net for the full\r\nrecidivism dataset.\r\nBuilding\r\na Classification Model with Elastic Net\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:07:13-08:00"
    },
    {
      "path": "lecture-6a.html",
      "title": "K-Nearest Neighbors",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "11/10/2022",
      "contents": "\r\n\r\nContents\r\n1. Distance Between Two\r\nVectors\r\n2.\r\nK-Nearest Neighbors\r\n3.\r\nPrediction with K-Nearest Neighbors (Do-It-Yourself)\r\n3.1.\r\nPredicting a continuous outcome with the KNN algorithm\r\n3.2.\r\nPredicting a binary outcome with the KNN algorithm\r\n\r\n4. Kernels to Weight the\r\nNeighbors\r\n5.\r\nPredicting a continuous outcome with the KNN algorithm via\r\ncaret:train()\r\n6.\r\nPredicting a binary outcome with the KNN algorithm via\r\ncaret:train()\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:07:14 ]\r\n1. Distance Between Two\r\nVectors\r\nMeasuring the distance between two data points is at the core of the\r\nK Nearest Neighbors (KNN) algorithm, and it is essential to understand\r\nthe concept of distance between two vectors.\r\nImagine that each observation in a dataset lives in a\r\nP-dimensional space, where P is the number of\r\npredictors.\r\nObsevation 1: \\(\\mathbf{A} = (A_1, A_2,\r\nA_3, ..., A_P)\\)\r\nObsevation 2: \\(\\mathbf{B} = (B_1, B_2,\r\nB_3, ..., B_P)\\)\r\nA general definition of distance between two vectors is the\r\nMinkowski Distance. The Minkowski Distance can be\r\ndefined as\r\n\\[\\left ( \\sum_{i=1}^{P}|A_i-B_i|^q \\right\r\n)^{\\frac{1}{q}},\\] where \\(q\\)\r\ncan take any positive value.\r\nFor simplicity, suppose that we have two observations and three\r\npredictors, and we observe the following values for the two observations\r\non these three predictors.\r\nObservation 1: (20,25,30)\r\nObservation 2: (80,90,75)\r\n\r\n\r\n\r\nIf we assume that the \\(q=1\\) for\r\nthe Minkowski equation above, then we can calculate the distance as the\r\nfollowing:\r\n\r\n\r\nA <- c(20,25,30)\r\nB <- c(80,90,75)\r\n\r\n\r\nsum(abs(A - B))\r\n\r\n[1] 170\r\n\r\nWhen \\(q\\) is equal to 1 for the\r\nMinkowski equation, it becomes a special case known as Manhattan\r\nDistance. Manhattan Distance between these two data points is\r\nvisualized below.\r\n\r\n\r\n\r\nIf we assume that the \\(q=2\\) for\r\nthe Minkowski equation above, then we can calculate the distance as the\r\nfollowing:\r\n\r\n\r\nA <- c(20,25,30)\r\nB <- c(80,90,75)\r\n\r\n\r\n(sum(abs(A - B)^2))^(1/2)\r\n\r\n[1] 99.24717\r\n\r\nWhen \\(q\\) is equal to 2 for the\r\nMinkowski equation, it is also a special case known as Euclidian\r\nDistance. The euclidian distance between these two data points\r\nis visualized below.\r\n\r\n\r\n\r\n2. K-Nearest Neighbors\r\nGiven that there are \\(N\\)\r\nobservations in a dataset, a distance between any observation and \\(N-1\\) remaining observations can be\r\ncomputed using Minkowski distance (with a user-defined choice of \\(q\\) value). Then, for any given\r\nobservation, we can rank order the remaining observations based on how\r\nclose they are to the given observation and then decide the K nearest\r\nneighbors (\\(K = 1, 2, 3, ..., N-1\\)),\r\nK observations closest to the given observation based on their\r\ndistance.\r\nSuppose that there are ten observations measured on three predictor\r\nvariables (X1, X2, and X3) with the following values.\r\n\r\n\r\nd <- data.frame(x1 =c(20,25,30,42,10,60,65,55,80,90),\r\n                x2 =c(10,15,12,20,45,75,70,80,85,90),\r\n                x3 =c(25,30,35,20,40,80,85,90,92,95),\r\n                label= c('A','B','C','D','E','F','G','H','I','J'))\r\n\r\nd\r\n\r\n   x1 x2 x3 label\r\n1  20 10 25     A\r\n2  25 15 30     B\r\n3  30 12 35     C\r\n4  42 20 20     D\r\n5  10 45 40     E\r\n6  60 75 80     F\r\n7  65 70 85     G\r\n8  55 80 90     H\r\n9  80 85 92     I\r\n10 90 90 95     J\r\n\r\n\r\n\r\n\r\nGiven that there are ten observations, we can calculate the distance\r\nbetween all 45 pairs of observations (e.g., Euclidian distance).\r\n\r\n\r\ndist <- as.data.frame(t(combn(1:10,2)))\r\ndist$euclidian <- NA\r\n\r\nfor(i in 1:nrow(dist)){\r\n  \r\n  a <- d[dist[i,1],1:3]\r\n  b <- d[dist[i,2],1:3]\r\n  dist[i,]$euclidian <- sqrt(sum((a-b)^2))\r\n  \r\n}\r\n\r\ndist\r\n\r\n   V1 V2  euclidian\r\n1   1  2   8.660254\r\n2   1  3  14.282857\r\n3   1  4  24.677925\r\n4   1  5  39.370039\r\n5   1  6  94.074439\r\n6   1  7  96.046864\r\n7   1  8 101.734950\r\n8   1  9 117.106789\r\n9   1 10 127.279221\r\n10  2  3   7.681146\r\n11  2  4  20.346990\r\n12  2  5  35.000000\r\n13  2  6  85.586214\r\n14  2  7  87.464278\r\n15  2  8  93.407708\r\n16  2  9 108.485022\r\n17  2 10 118.638105\r\n18  3  4  20.808652\r\n19  3  5  38.910153\r\n20  3  6  83.030115\r\n21  3  7  84.196199\r\n22  3  8  90.961530\r\n23  3  9 105.252078\r\n24  3 10 115.256236\r\n25  4  5  45.265881\r\n26  4  6  83.360662\r\n27  4  7  85.170417\r\n28  4  8  93.107465\r\n29  4  9 104.177733\r\n30  4 10 113.265176\r\n31  5  6  70.710678\r\n32  5  7  75.332596\r\n33  5  8  75.828754\r\n34  5  9  95.937480\r\n35  5 10 107.004673\r\n36  6  7   8.660254\r\n37  6  8  12.247449\r\n38  6  9  25.377155\r\n39  6 10  36.742346\r\n40  7  8  15.000000\r\n41  7  9  22.338308\r\n42  7 10  33.541020\r\n43  8  9  25.573424\r\n44  8 10  36.742346\r\n45  9 10  11.575837\r\n\r\nFor instance, we can find the three closest observations to\r\nPoint E (3-Nearest Neighbors). As seen below, the\r\n3-Nearest Neighbors for Point E in this dataset would\r\nbe Point B, Point C, and Point\r\nA.\r\n\r\n\r\n# Point E is the fifth observation in the dataset\r\n\r\nloc <- which(dist[,1]==5 | dist[,2]==5)\r\n\r\ntmp <- dist[loc,]\r\n\r\ntmp[order(tmp$euclidian),]\r\n\r\n   V1 V2 euclidian\r\n12  2  5  35.00000\r\n19  3  5  38.91015\r\n4   1  5  39.37004\r\n25  4  5  45.26588\r\n31  5  6  70.71068\r\n32  5  7  75.33260\r\n33  5  8  75.82875\r\n34  5  9  95.93748\r\n35  5 10 107.00467\r\n\r\n\r\n\r\nNOTE 1\r\n\r\nThe \\(q\\) in the Minkowski distance\r\nequation and \\(K\\) in the K-nearest\r\nneighbor are user-defined hyperparameters in the KNN algorithm. As a\r\nresearcher and model builder, you can pick any values for \\(q\\) and \\(K\\). They can be tuned using a similar\r\napproach applied in earlier classes for regularized regression models.\r\nOne can pick a set of values for these hyperparameters and apply a grid\r\nsearch to find the combination that provides the best predictive\r\nperformance.\r\nIt is typical to observe overfitting (high model variance, low model\r\nbias) for small values of K and underfitting (low model variance, high\r\nmodel bias) for large values of K. In general, people tend to focus\r\ntheir grid search for K around \\(\\sqrt{N}\\).\r\n\r\n\r\n\r\nNOTE 2\r\n\r\nIt is essential to remember that the distance calculation between two\r\nobservations is highly dependent on the scale of measurement for the\r\npredictor variables. If predictors are on different scales, the distance\r\nmetric formula will favor the differences in predictors with larger\r\nscales, and it is not ideal. Therefore, it is essential to center and\r\nscale all predictors before the KNN algorithm so each predictor\r\nsimilarly contributes to the distance metric calculation.\r\n\r\n3.\r\nPrediction with K-Nearest Neighbors (Do-It-Yourself)\r\nGiven that we learned about distance calculation and how to identify\r\nK-nearest neighbors based on a distance metric, the prediction in KNN is\r\nstraightforward.\r\nBelow is a list of steps for predicting an outcome for a given\r\nobservation.\r\nCalculate the distance between the observation and the remaining\r\n\\(N-1\\) observations in the data (with\r\na user choice of \\(q\\) in Minkowski\r\ndistance).\r\nRank order the observations based on the calculated distance, and\r\nchoose the K-nearest neighbor. (with a user choice of \\(K\\))\r\nCalculate the mean of the observed outcome in the K-nearest\r\nneighbors as your prediction.\r\nNote that Step 3 applies regardless of the type of outcome. If the\r\noutcome variable is continuous, we calculate the average outcome for the\r\nK-nearest neighbors as our prediction. If the outcome variable is binary\r\n(e.g., 0 vs. 1), then the proportion of observing each class among the\r\nK-nearest neighbors yields predicted probabilities for each class.\r\nBelow, I provide an example for both types of outcome using the\r\nReadability and Recidivism datasets.\r\n3.1.\r\nPredicting a continuous outcome with the KNN algorithm\r\nThe code below is identical to the code we used in earlier classes\r\nfor data preparation of the Readability datasets. Note that this is only\r\nto demonstrate the logic of model predictions in the context of\r\nK-nearest neighbors. So, we are using the whole dataset. In the next\r\nsection, we will demonstrate the full workflow of model training and\r\ntuning with 10-fold cross-validation using the\r\ncaret::train() function. 1. Import the data 2. Write a\r\nrecipe for processing variables 3. Apply the recipe to the dataset\r\n\r\n\r\n# Import the dataset\r\n\r\nreadability <- read.csv('./data/readability_features.csv',header=TRUE)\r\n\r\n# Write the recipe\r\n\r\nrequire(recipes)\r\n\r\nblueprint_readability <- recipe(x     = readability,\r\n                                vars  = colnames(readability),\r\n                                roles = c(rep('predictor',768),'outcome')) %>%\r\n  step_zv(all_numeric()) %>%\r\n  step_nzv(all_numeric()) %>%\r\n  step_normalize(all_numeric_predictors()) \r\n\r\n# Apply the recipe\r\n\r\nbaked_read <- blueprint_readability %>% \r\n  prep(training = readability) %>%\r\n  bake(new_data = readability)\r\n\r\n\r\nOur final dataset (baked_read) has 2834 observations and\r\n769 columns (768 predictors; the last column is target outcome). Suppose\r\nwe would like to predict the readability score for the first\r\nobservation. The code below will calculate the Minkowski distance (with\r\n\\(q=2\\)) between the first observation\r\nand each of the remaining 2833 observations by using the first 768\r\ncolumns of the dataset (predictors).\r\n\r\n\r\ndist <- data.frame(obs = 2:2834,dist = NA,target=NA)\r\n\r\nfor(i in 1:2833){\r\n  \r\n  a <- as.matrix(baked_read[1,1:768])\r\n  b <- as.matrix(baked_read[i+1,1:768])\r\n  dist[i,]$dist   <- sqrt(sum((a-b)^2))\r\n  dist[i,]$target <- baked_read[i+1,]$target\r\n\r\n  #print(i)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nWe now rank-order the observations from closest to the most distant\r\nand then choose the 20 nearest observations (K=20). Finally, we can\r\ncalculate the average of the observed outcome for the 20 nearest\r\nneighbors, which will become our prediction of the readability score for\r\nthe first observation.\r\n\r\n\r\n# Rank order the observations from closest to the most distant\r\n\r\ndist <- dist[order(dist$dist),]\r\n\r\n# Check the 20-nearest neighbors\r\n\r\ndist[1:20,]\r\n\r\n      obs     dist     target\r\n2440 2441 24.18419  0.5589749\r\n44     45 24.37057 -0.5863595\r\n1991 1992 24.91154  0.1430485\r\n2263 2264 25.26260 -0.9034530\r\n2521 2522 25.26789 -0.6358878\r\n2418 2419 25.41072 -0.2127907\r\n1529 1530 25.66245 -1.8725131\r\n238   239 25.92757 -0.5610845\r\n237   238 26.30142 -0.8889601\r\n1519 1520 26.40373 -0.6159237\r\n2243 2244 26.50242 -0.3327295\r\n1553 1554 26.57041 -1.8843523\r\n1570 1571 26.60936 -1.1336779\r\n2153 2154 26.61727 -1.1141251\r\n75     76 26.63733 -0.6056466\r\n2348 2349 26.68325 -0.1593255\r\n1188 1189 26.85316 -1.2394727\r\n2312 2313 26.95360 -0.2532137\r\n2178 2179 27.04694 -1.0298868\r\n2016 2017 27.05989  0.1398929\r\n\r\n\r\n\r\n# Mean target for the 20-nearest observations\r\n\r\nmean(dist[1:20,]$target)\r\n\r\n[1] -0.6593743\r\n\r\n\r\n\r\n# Check the actual observed value of reability for the first observation\r\n\r\nreadability[1,]$target\r\n\r\n[1] -0.3402591\r\n\r\n3.2.\r\nPredicting a binary outcome with the KNN algorithm\r\nWe can follow the same procedures to predict Recidivism in the second\r\nyear after an individual’s initial release from prison.\r\n\r\n\r\n# Import data\r\n\r\nrecidivism <- read.csv('./data/recidivism_y1 removed and recoded.csv',header=TRUE)\r\n\r\n# Write the recipe\r\n\r\n  outcome <- c('Recidivism_Arrest_Year2')\r\n  \r\n  id      <- c('ID')\r\n  \r\n  categorical <- c('Residence_PUMA',\r\n                   'Prison_Offense',\r\n                   'Age_at_Release',\r\n                   'Supervision_Level_First',\r\n                   'Education_Level',\r\n                   'Prison_Years',\r\n                   'Gender',\r\n                   'Race',\r\n                   'Gang_Affiliated',\r\n                   'Prior_Arrest_Episodes_DVCharges',\r\n                   'Prior_Arrest_Episodes_GunCharges',\r\n                   'Prior_Conviction_Episodes_Viol',\r\n                   'Prior_Conviction_Episodes_PPViolationCharges',\r\n                   'Prior_Conviction_Episodes_DomesticViolenceCharges',\r\n                   'Prior_Conviction_Episodes_GunCharges',\r\n                   'Prior_Revocations_Parole',\r\n                   'Prior_Revocations_Probation',\r\n                   'Condition_MH_SA',\r\n                   'Condition_Cog_Ed',\r\n                   'Condition_Other',\r\n                   'Violations_ElectronicMonitoring',\r\n                   'Violations_Instruction',\r\n                   'Violations_FailToReport',\r\n                   'Violations_MoveWithoutPermission',\r\n                   'Employment_Exempt') \r\n\r\n  numeric   <- c('Supervision_Risk_Score_First',\r\n                 'Dependents',\r\n                 'Prior_Arrest_Episodes_Felony',\r\n                 'Prior_Arrest_Episodes_Misd',\r\n                 'Prior_Arrest_Episodes_Violent',\r\n                 'Prior_Arrest_Episodes_Property',\r\n                 'Prior_Arrest_Episodes_Drug',\r\n                 'Prior_Arrest_Episodes_PPViolationCharges',\r\n                 'Prior_Conviction_Episodes_Felony',\r\n                 'Prior_Conviction_Episodes_Misd',\r\n                 'Prior_Conviction_Episodes_Prop',\r\n                 'Prior_Conviction_Episodes_Drug',\r\n                 'Delinquency_Reports',\r\n                 'Program_Attendances',\r\n                 'Program_UnexcusedAbsences',\r\n                 'Residence_Changes',\r\n                 'Avg_Days_per_DrugTest',\r\n                 'Jobs_Per_Year')\r\n  \r\n  props      <- c('DrugTests_THC_Positive',\r\n                  'DrugTests_Cocaine_Positive',\r\n                  'DrugTests_Meth_Positive',\r\n                  'DrugTests_Other_Positive',\r\n                  'Percent_Days_Employed')\r\n  \r\n\r\n  for(i in categorical){\r\n    \r\n    recidivism[,i] <- as.factor(recidivism[,i])\r\n    \r\n  }\r\n  # Blueprint for processing variables\r\n    \r\n  blueprint_recidivism <- recipe(x  = recidivism,\r\n                    vars  = c(categorical,numeric,props,outcome,id),\r\n                    roles = c(rep('predictor',48),'outcome','ID')) %>%\r\n    step_indicate_na(all_of(categorical),all_of(numeric),all_of(props)) %>%\r\n    step_zv(all_numeric()) %>%\r\n    step_impute_mean(all_of(numeric),all_of(props)) %>%\r\n    step_impute_mode(all_of(categorical)) %>%\r\n    step_logit(all_of(props),offset=.001) %>%\r\n    step_poly(all_of(numeric),all_of(props),degree=2) %>%\r\n    step_normalize(paste0(numeric,'_poly_1'),\r\n                   paste0(numeric,'_poly_2'),\r\n                   paste0(props,'_poly_1'),\r\n                   paste0(props,'_poly_2')) %>%\r\n    step_dummy(all_of(categorical),one_hot=TRUE) %>%\r\n    step_num2factor(Recidivism_Arrest_Year2,\r\n                  transform = function(x) x + 1,\r\n                  levels=c('No','Yes'))\r\n\r\n# Apply the recipe\r\n\r\nbaked_recidivism <- blueprint_recidivism %>% \r\n  prep(training = recidivism) %>% \r\n  bake(new_data = recidivism)\r\n\r\n\r\nThe final dataset (baked_recidivism) has 18111\r\nobservations and 144 columns (the first column is the outcome variable,\r\nthe second column is the ID variable, and remaining 142 columns are\r\npredictors). Now, suppose that we would like to predict the probability\r\nof Recidivism for the first individual. The code below will calculate\r\nthe Minkowski distance (with \\(q=2\\))\r\nbetween the first individual and each of the remaining 18,110\r\nindividuals by using values of the 142 predictors in this dataset.\r\n\r\n\r\ndist2 <- data.frame(obs = 2:18111,dist = NA,target=NA)\r\n\r\nfor(i in 1:18110){\r\n  \r\n  a <- as.matrix(baked_recidivism[1,3:144])\r\n  b <- as.matrix(baked_recidivism[i+1,3:144])\r\n  dist2[i,]$dist   <- sqrt(sum((a-b)^2))\r\n  dist2[i,]$target <- as.character(baked_recidivism[i+1,]$Recidivism_Arrest_Year2)\r\n\r\n  #print(i)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nWe now rank-order the individuals from closest to the most distant\r\nand then choose the 100-nearest observations (K=100). Then, we calculate\r\nproportion of individuals who were recidivated (YES) and not recidivated\r\n(NO) among these 100-nearest neighbors. These proportions predict the\r\nprobability of being recidivated or not recidivated for the first\r\nindividual.\r\n\r\n\r\n# Rank order the observations from closest to the most distant\r\n\r\ndist2 <- dist2[order(dist2$dist),]\r\n\r\n# Check the 100-nearest neighbors\r\n\r\ndist2[1:100,]\r\n\r\n        obs     dist target\r\n7069   7070 6.216708     No\r\n14203 14204 6.255963     No\r\n1573   1574 6.383890     No\r\n4526   4527 6.679704     No\r\n8445   8446 7.011824     No\r\n6023   6024 7.251224     No\r\n7786   7787 7.269879     No\r\n564     565 7.279444    Yes\r\n8767   8768 7.288118     No\r\n4645   4646 7.358620     No\r\n4042   4043 7.375563     No\r\n9112   9113 7.385485     No\r\n5315   5316 7.405087     No\r\n4094   4095 7.536276     No\r\n9731   9732 7.565588     No\r\n830     831 7.633862     No\r\n14384 14385 7.644471     No\r\n2932   2933 7.660397    Yes\r\n646     647 7.676351    Yes\r\n6384   6385 7.684824    Yes\r\n13574 13575 7.698020    Yes\r\n8468   8469 7.721216     No\r\n1028   1029 7.733411    Yes\r\n5307   5308 7.739071    Yes\r\n15431 15432 7.745690    Yes\r\n2947   2948 7.756829     No\r\n2948   2949 7.765037     No\r\n4230   4231 7.775368     No\r\n596     597 7.784595     No\r\n4167   4168 7.784612     No\r\n1006   1007 7.812405     No\r\n3390   3391 7.874554     No\r\n9071   9072 7.909254     No\r\n8331   8332 7.918238     No\r\n9104   9105 7.924227     No\r\n3229   3230 7.930461     No\r\n13537 13538 7.938787     No\r\n2714   2715 7.945400     No\r\n1156   1157 7.953271     No\r\n1697   1698 7.974193     No\r\n14784 14785 7.990007    Yes\r\n7202   7203 7.993035     No\r\n3690   3691 7.995380     No\r\n1918   1919 8.001320    Yes\r\n11531 11532 8.029120    Yes\r\n10446 10447 8.047488     No\r\n1901   1902 8.057717     No\r\n2300   2301 8.071222     No\r\n8224   8225 8.083153    Yes\r\n14277 14278 8.084527    Yes\r\n12032 12033 8.089992    Yes\r\n14276 14277 8.119004     No\r\n1771   1772 8.130169     No\r\n4744   4745 8.131978     No\r\n5922   5923 8.142912     No\r\n10762 10763 8.147908    Yes\r\n4875   4876 8.165558    Yes\r\n9875   9876 8.169483     No\r\n9728   9729 8.180874     No\r\n1197   1198 8.201112     No\r\n12474 12475 8.203781     No\r\n5807   5808 8.203803     No\r\n8924   8925 8.205562     No\r\n15616 15617 8.209297     No\r\n3939   3940 8.211146    Yes\r\n9135   9136 8.228498     No\r\n2123   2124 8.239376     No\r\n3027   3028 8.240339    Yes\r\n5797   5798 8.241649     No\r\n11356 11357 8.257729     No\r\n13821 13822 8.264409     No\r\n3886   3887 8.266251     No\r\n4462   4463 8.270711    Yes\r\n11885 11886 8.274784     No\r\n10755 10756 8.306296    Yes\r\n11092 11093 8.306444     No\r\n16023 16024 8.306558    Yes\r\n14527 14528 8.308691    Yes\r\n5304   5305 8.309684    Yes\r\n2159   2160 8.314671     No\r\n417     418 8.321942     No\r\n3885   3886 8.325970     No\r\n1041   1042 8.335102    Yes\r\n7768   7769 8.344739     No\r\n5144   5145 8.345242     No\r\n822     823 8.348941    Yes\r\n2904   2905 8.351296     No\r\n1579   1580 8.358877     No\r\n385     386 8.365923    Yes\r\n15929 15930 8.368133     No\r\n616     617 8.368361     No\r\n7434   7435 8.371817     No\r\n3262   3263 8.375772     No\r\n11763 11764 8.377018     No\r\n713     714 8.379589     No\r\n5718   5719 8.383483     No\r\n7314   7315 8.384174     No\r\n3317   3318 8.393829    Yes\r\n4584   4585 8.411941    Yes\r\n8946   8947 8.418526     No\r\n\r\n\r\n\r\n# Mean target for the 100-nearest observations\r\n\r\ntable(dist2[1:100,]$target)\r\n\r\n\r\n No Yes \r\n 72  28 \r\n\r\n  # This indicates that the predicted probability of being recidivated is 0.28\r\n  # for the first individual given the observed data for 100 most similar \r\n  # observations\r\n\r\n\r\n\r\n\r\n# Check the actual observed outcome for the first individual\r\n\r\nrecidivism[1,]$Recidivism_Arrest_Year2\r\n\r\n[1] 0\r\n\r\n4. Kernels to Weight the\r\nNeighbors\r\nIn the previous section, we tried to understand how KNN predicts a\r\ntarget outcome by simply averaging the observed value for the target\r\noutcome from K-nearest neighbors. It was a simple average by equally\r\nweighing each neighbor.\r\nAnother way of averaging the target outcome from K-nearest neighbors\r\nwould be to weigh each neighbor according to its distance and calculate\r\na weighted average. A simple way to weigh each neighbor is to use the\r\ninverse of the distance. For instance, consider the earlier example\r\nwhere we find the 20-nearest neighbor for the first observation in the\r\nreadability dataset.\r\n\r\n\r\ndist <- dist[order(dist$dist),]\r\n\r\nk_neighbors <- dist[1:20,]\r\n\r\nk_neighbors\r\n\r\n      obs     dist     target\r\n2440 2441 24.18419  0.5589749\r\n44     45 24.37057 -0.5863595\r\n1991 1992 24.91154  0.1430485\r\n2263 2264 25.26260 -0.9034530\r\n2521 2522 25.26789 -0.6358878\r\n2418 2419 25.41072 -0.2127907\r\n1529 1530 25.66245 -1.8725131\r\n238   239 25.92757 -0.5610845\r\n237   238 26.30142 -0.8889601\r\n1519 1520 26.40373 -0.6159237\r\n2243 2244 26.50242 -0.3327295\r\n1553 1554 26.57041 -1.8843523\r\n1570 1571 26.60936 -1.1336779\r\n2153 2154 26.61727 -1.1141251\r\n75     76 26.63733 -0.6056466\r\n2348 2349 26.68325 -0.1593255\r\n1188 1189 26.85316 -1.2394727\r\n2312 2313 26.95360 -0.2532137\r\n2178 2179 27.04694 -1.0298868\r\n2016 2017 27.05989  0.1398929\r\n\r\nWe can assign a weight to each neighbor by taking the inverse of\r\ntheir distance and rescaling them such that the sum of the weights\r\nequals 1.\r\n\r\n\r\nk_neighbors$weight <- 1/k_neighbors$dist\r\nk_neighbors$weight <- k_neighbors$weight/sum(k_neighbors$weight)\r\n\r\n\r\nk_neighbors\r\n\r\n      obs     dist     target     weight\r\n2440 2441 24.18419  0.5589749 0.05382110\r\n44     45 24.37057 -0.5863595 0.05340950\r\n1991 1992 24.91154  0.1430485 0.05224967\r\n2263 2264 25.26260 -0.9034530 0.05152360\r\n2521 2522 25.26789 -0.6358878 0.05151279\r\n2418 2419 25.41072 -0.2127907 0.05122326\r\n1529 1530 25.66245 -1.8725131 0.05072080\r\n238   239 25.92757 -0.5610845 0.05020216\r\n237   238 26.30142 -0.8889601 0.04948858\r\n1519 1520 26.40373 -0.6159237 0.04929682\r\n2243 2244 26.50242 -0.3327295 0.04911324\r\n1553 1554 26.57041 -1.8843523 0.04898757\r\n1570 1571 26.60936 -1.1336779 0.04891586\r\n2153 2154 26.61727 -1.1141251 0.04890133\r\n75     76 26.63733 -0.6056466 0.04886451\r\n2348 2349 26.68325 -0.1593255 0.04878041\r\n1188 1189 26.85316 -1.2394727 0.04847175\r\n2312 2313 26.95360 -0.2532137 0.04829114\r\n2178 2179 27.04694 -1.0298868 0.04812447\r\n2016 2017 27.05989  0.1398929 0.04810145\r\n\r\nThen, we can compute a weighted average of the target scores instead\r\nof a simple average.\r\n\r\n\r\n# Weighted Mean target for the 20-nearest observations\r\n\r\nsum(k_neighbors$target*k_neighbors$weight)\r\n\r\n[1] -0.6525591\r\n\r\nSeveral kernel functions can be used to assign weight to K-nearest\r\nneighbors (e.g., epanechnikov, quartic, triweight, tricube, gaussian,\r\ncosine). For all of them, closest neighbors are assigned higher weights\r\nwhile the weight gets smaller as the distance increases, and they\r\nslightly differ the way they assign the weight. Below is a demonstration\r\nof how assigned weight changes as a function of distance for different\r\nkernel functions.\r\n\r\n\r\n\r\n\r\n\r\nNOTE 3\r\n\r\nWhich kernel function should we use for weighing the distance? The\r\ntype of kernel function can also be considered a hyperparameter to\r\ntune.\r\n\r\n5.\r\nPredicting a continuous outcome with the KNN algorithm via\r\ncaret:train()\r\nPlease review the following notebook that builds a prediction model\r\nusing the K-nearest neighbor algorithm for the readability dataset.\r\nBuilding\r\na Prediction Model using KNN\r\n6.\r\nPredicting a binary outcome with the KNN algorithm via\r\ncaret:train()\r\nPlease review the following notebook that builds a classification\r\nmodel using the K-nearest neighbor algorithm for the full recidivism\r\ndataset.\r\nBuilding\r\na Classification Model using KNN\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:07:27-08:00"
    },
    {
      "path": "lecture-6b.html",
      "title": "Introduction to Decision Trees",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "11/11/2022",
      "contents": "\r\n\r\nContents\r\n1. Decision\r\nTrees\r\n1.1. Basics of a decision tree\r\n1.1.1. Tree\r\nStructure\r\n1.1.2.\r\nPredictions\r\n1.1.3. Loss\r\nfunction\r\n1.1.4. Growing a\r\ntree\r\n1.1.5. Pruning a\r\ntree\r\n\r\n1.2. Growing trees with\r\nthe rpart() function\r\n1.2.1. A\r\nsimple example\r\n1.2.2. Tree\r\npruning by increasing the complexity parameter\r\n\r\n\r\n2.\r\nPredicting a continuous outcome with a Decision Tree via\r\ncaret:train()\r\n3.\r\nPredicting a binary outcome with a Decision Tree via\r\ncaret:train()\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:07:28 ]\r\n1. Decision Trees\r\nWe will again consider the toy readability dataset (N=20) we used to\r\npredict a readability score from Feature 220 and Feature 166\r\n\r\n\r\nreadability_sub <- read.csv('./data/readability_sub.csv',header=TRUE)\r\n\r\nreadability_sub[,c('V220','V166','target')]\r\n\r\n          V220        V166      target\r\n1  -0.13908258  0.19028091 -2.06282395\r\n2   0.21764143  0.07101288  0.58258607\r\n3   0.05812133  0.03993277 -1.65313060\r\n4   0.02526429  0.18845809 -0.87390681\r\n5   0.22430885  0.06200715 -1.74049148\r\n6  -0.07795373  0.10754109 -3.63993555\r\n7   0.43400714  0.12202360 -0.62284268\r\n8  -0.24364550  0.02454670 -0.34426981\r\n9   0.15893717  0.10422343 -1.12298826\r\n10  0.14496475  0.02339597 -0.99857142\r\n11  0.34222975  0.22065343 -0.87656742\r\n12  0.25219145  0.10865010 -0.03304643\r\n13  0.03532625  0.07549474 -0.49529863\r\n14  0.36410633  0.18675801  0.12453660\r\n15  0.29988593  0.11618323  0.09678258\r\n16  0.19837037  0.08272671  0.38422270\r\n17  0.07807041  0.10235218 -0.58143038\r\n18  0.07935690  0.11618605 -0.34324576\r\n19  0.57000953 -0.02385423 -0.39054205\r\n20  0.34523284  0.09299514 -0.67548411\r\n\r\n1.1. Basics of a decision tree\r\n1.1.1. Tree Structure\r\nLet’s imagine a simple tree model to predict readability scores from\r\nFeature 220.\r\n\r\n\r\n\r\nThis model splits the sample into two pieces using a split\r\npoint of 0.2 for the predictor variable (Feature 220). There\r\nare 11 observations with Feature 220 less than 0.2 and nine with Feature\r\n220 larger than 0.2. The top of the tree model is called root\r\nnode, and the \\(R_1\\) and\r\n\\(R_2\\) in this model are called\r\nterminal nodes. This model has two terminal nodes.\r\nThere is a number assigned to each terminal node. These numbers are the\r\naverage values for the target outcome (readability score) for those\r\nobservations in that specific node. It can be symbolically shown as\r\n\\[\\bar{Y_t} = \\frac{1}{n_t}\\sum_{i\\epsilon\r\nR_t} Y_i,\\]\r\nwhere \\(n_t\\) is the number of\r\nobservations in a terminal node, and \\(R_t\\) represents the set of observations in\r\nthe \\(t_{th}\\) node.\r\nThere is also a concept of depth of a tree. The root\r\nnode is counted as depth 0, and each split increases the depth of the\r\ntree by one. In this case, we can say that this tree model has a depth\r\nof one.\r\nWe can increase the complexity of our tree model by splitting the\r\nfirst node into two more nodes using a split point of 0. Now, our model\r\nhas a depth of two and a total of three terminal nodes. Each terminal\r\nnode is assigned a score by computing the average outcome for those\r\nobservations in that node.\r\n\r\n\r\n\r\nThe tree model can have nodes from splitting another variable. For\r\ninstance, the model below first splits the observations based on Feature\r\n220, then based on Feature 166, yielding a tree model with three nodes\r\nwith a depth of two. This tree model’s complexity is the same as the\r\nprevious one’s; the only difference is that we have nodes from two\r\npredictors instead of one.\r\n\r\n\r\n\r\nA final example is another tree model with increasing complexity and\r\nhaving a depth of three and four nodes. It first splits observations\r\nbased on whether or not Feature 220 is less than 0.2, then splits\r\nobservations based on whether or not Feature 220 is less than 0, and\r\nfinally splits observations based on whether or not Feature 166 is less\r\nthan 0.1.\r\n\r\n\r\n\r\n1.1.2. Predictions\r\nSuppose you developed a tree model and decided to use this model to\r\nmake predictions for new observations. Let’s assume our model is the\r\nbelow. How do we use this model to make predictions for new\r\nobservations?\r\n\r\n\r\n\r\nSuppose that there is a new reading passage to predict the\r\nreadability score based on this tree model. The value for Feature 220\r\nfor this new reading passage is - 0.5, and the value for Feature 166 is\r\n0.\r\nTo predict the readability score for this reading passage, we must\r\ndecide which node this passage would belong to in this tree model. You\r\ncan trace a path starting from the root node (top of\r\nthe tree) and see where this reading passage will end.\r\nAs you can see below, this reading passage would be in the first node\r\nbased on the observed values for these two predictors, and so we predict\r\nthat the readability score for this new reading passage is equal to\r\n-2.016.\r\n\r\n\r\n\r\nIf we have another new reading passage with the value for Feature 220\r\nas 0.1 and Feature 166 as 0, then it would end up in the second node and\r\nwe would predict the readability score as -0.691.\r\n\r\n\r\n\r\n1.1.3. Loss function\r\nWhen we fit a tree model, the algorithm decides the best split that\r\nminimizes the sum of squared errors. The sum of squared error from a\r\ntree model can be shown as\r\n\\[ SSE = \\sum_{t=1}^{T} \\sum_{i \\epsilon\r\nR_t} (Y_i - \\hat{Y}_{R_{t}})^2,\\]\r\nwhere \\(T\\) is the total number of\r\nterminal nodes in the tree model, and \\(\\hat{Y}_{R_{t}}\\) is the prediction for the\r\nobservations in the \\(t^{th}\\) node\r\n(average target outcome for those observations in the \\(t^{th}\\) node).\r\n1.1.4. Growing a tree\r\nDeciding on a root node and then growing a tree model from that root\r\nnode can become computationally exhaustive depending on the size of the\r\ndataset and the number of variables. The decision tree algorithm\r\nsearches all variables designated as predictors in the dataset at all\r\npossible split points for these variables, calculates the SSE for all\r\npossible splits, and then finds the split that would reduce the\r\nprediction error the most. The search continues by growing the tree\r\nmodel sequentially until there is no more split left that would give\r\nbetter predictions.\r\nI will demonstrate the logic of this search process with the toy\r\ndataset (N=20) and two predictor variables: Feature 220 and Feature 166.\r\nBefore we start our search process, we should come up with a baseline\r\nSSE to decide whether any future split will improve our predictions. We\r\ncan imagine that a model with no split and using the mean of all 20\r\nobservations to predict the target outcome is the simplest baseline\r\nmodel. So, if we compute the sum of squared residuals assuming that we\r\npredict every observation with the mean of these 20 observations, that\r\ncould be a baseline measure. As you see below, if we use the mean target\r\nscore as our prediction for the 20 observations, the sum of squared\r\nresiduals would be 17.73. We will improve this number over the baseline\r\nby growing a tree model.\r\n\r\n\r\n# average outcome\r\n\r\nmu <- mean(readability_sub$target)\r\n\r\n# SSE for baseline model\r\n\r\nsum((readability_sub$target - mu)^2)\r\n\r\n[1] 17.73309\r\n\r\nFind the root node\r\nThe first step in building the tree model is to find the root node.\r\nIn this case, we have two candidates for the root node: Feature 220 and\r\nFeature 166. We want to know which predictor should be the root node and\r\nwhat value we should use to split to improve the baseline SSE the most.\r\nThe following is what the process would look\r\nPick a split point\r\nDivide 20 observations into two nodes based on the split\r\npoint\r\nCalculate the average target outcome within each node as\r\npredictions for the observations in that node\r\nCalculate SSE within each node using the predictions from Step 3,\r\nand sum them all across two nodes.\r\nRepeat Steps 1 - 4 for every possible split point, and find the\r\nbest split point with the minimum SSE\r\nRepeat Steps 1 - 5 for every possible predictor.\r\nThe code below implements Step 1 - Step 5 for the predictor\r\nFeature 220.\r\n\r\n\r\nsplits1 <- seq(-0.24,0.56,0.001)\r\n\r\n# For every split point, partition the observations into two groups and then\r\n# make predictions based on the partitioning. Save the SSE in an empy object\r\n\r\nsse1 <- c()\r\n\r\nfor(i in 1:length(splits1)){\r\n  \r\n  gr1 <- which(readability_sub$V220 <= splits1[i])\r\n  gr2 <- which(readability_sub$V220 > splits1[i])\r\n  \r\n  pr1 <- mean(readability_sub[gr1,]$target)\r\n  pr2 <- mean(readability_sub[gr2,]$target)\r\n  \r\n  sse1[i] = sum((readability_sub[gr1,]$target - pr1)^2) + \r\n            sum((readability_sub[gr2,]$target - pr2)^2)\r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=splits1,y=sse1))+\r\n  xlab('Split Points (Feature 220)')+\r\n  ylab('Sum of Squared Residuals')+\r\n  theme_bw()+\r\n  geom_vline(xintercept = -0.026,lty=2)+\r\n  annotate('text',x=-0.0,y=16,label='-0.026')\r\n\r\n\r\nmin(sse1)\r\n\r\n[1] 12.1976\r\n\r\nThe search process indicates that the best split point for Feature\r\n220 is -0.026. If we divide the observations into two nodes based on\r\nFeature 220 using the split point -0.026, SSE would be equal to 12.20, a\r\nsignificant improvement over the baseline model.\r\nWe will now repeat the same process for Feature 166. The code below\r\nimplements Step 1 - Step 5 for the predictor Feature\r\n166.\r\n\r\n\r\nsplits2 <- seq(-0.02,0.22,.001)\r\n\r\n# For every split point, partition the observations into two groups and then\r\n# make predictions based on the partitioning. Save the SSE in an empyy object\r\n\r\nsse2 <- c()\r\n\r\nfor(i in 1:length(splits2)){\r\n  \r\n  gr1 <- which(readability_sub$V166 <= splits2[i])\r\n  gr2 <- which(readability_sub$V166 > splits2[i])\r\n  \r\n  pr1 <- mean(readability_sub[gr1,]$target)\r\n  pr2 <- mean(readability_sub[gr2,]$target)\r\n  \r\n  sse2[i] = sum((readability_sub[gr1,]$target - pr1)^2) + \r\n            sum((readability_sub[gr2,]$target - pr2)^2)\r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=splits2,y=sse2))+\r\n  xlab('Split Points (Feature 166)')+\r\n  ylab('Sum of Squared Residuals')+\r\n  theme_bw()+\r\n  geom_vline(xintercept = 0.189,lty=2)+\r\n  annotate('text',x=0.195,y=18,label='0.189')\r\n\r\n\r\nmin(sse2)\r\n\r\n[1] 16.62429\r\n\r\nThe search process indicates that the best split point for Feature\r\n166 is 0.19. If we divide the observations into two nodes based on\r\nFeature 166 using the split point 0.19, SSE would be equal to 16.62,\r\nalso an improvement over the baseline model.\r\nWe have two choices for the root node (because we\r\nassume we only have two predictors):\r\nFeature 220, Best split point = -0.026, SSE = 12.20\r\nFeature 166, Best split point = 0.189, SSE = 16.62\r\nBoth options improve our predictions over the baseline (SSE = 17.73),\r\nbut one is better. Therefore, our final decision to start growing our\r\ntree is to pick Feature 220 as our root node and split\r\nit at -0.07. Our tree model starts growing!\r\n\r\n\r\n\r\nNow, we have to decide if we should add another split to either one\r\nof these two nodes. There are now four possible split scenarios.\r\nFor the first terminal node on the left, we can split the\r\nobservations based on Feature 220.\r\nFor the first terminal node on the left, we can split the\r\nobservations based on Feature 166.\r\nFor the second terminal node on the right, we can split the\r\nobservations based on Feature 220.\r\nFor the second terminal node on the right, we can split the\r\nobservations based on Feature 166.\r\nFor each of these scenarios, we can implement Step 1 - Step 4,\r\nidentifying the best split point and what SSE that split yields. The\r\ncode below implements these steps for all four scenarios.\r\nScenario 1:\r\n\r\n\r\nnode1 <- c(1,6,8)\r\nnode2 <- c(2,3,4,5,7,9,10,11,12,13,14,15,16,17,18,19,20)\r\n\r\n# Splits based on Feature220 on node 1\r\n\r\nsplits1 <- seq(-0.24,0.56,0.001)\r\n\r\nsse1 <- c()\r\n\r\nfor(i in 1:length(splits1)){\r\n  \r\n  gr1 <- which(readability_sub[node1,]$V220 <= splits1[i])\r\n  gr2 <- which(readability_sub[node1,]$V220 > splits1[i])\r\n  gr3 <- node2\r\n  \r\n  pr1 <- mean(readability_sub[node1[gr1],]$target)\r\n  pr2 <- mean(readability_sub[node1[gr2],]$target)\r\n  pr3 <- mean(readability_sub[node2,]$target)\r\n\r\n    \r\n  sse1[i] = sum((readability_sub[node1[gr1],]$target - pr1)^2) + \r\n            sum((readability_sub[node1[gr2],]$target - pr2)^2) +\r\n            sum((readability_sub[node2,]$target - pr3)^2) \r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=splits1,y=sse1))+\r\n  xlab('Split Points (Feature 220) on Node 1')+\r\n  ylab('Sum of Squared Residuals')+\r\n  theme_bw()+\r\n  geom_vline(xintercept = -0.19,lty=2)+\r\n  annotate('text',x=-0.23,y=12,label='-0.19')\r\n\r\n\r\nmin(sse1)\r\n\r\n[1] 8.007196\r\n\r\nScenario 2:\r\n\r\n\r\n# Splits based on Feature 166 for node 1\r\n\r\nsplits2 <- seq(-0.02,0.22,.001)\r\n\r\nsse2 <- c()\r\n\r\nfor(i in 1:length(splits2)){\r\n  \r\n  gr1 <- which(readability_sub[node1,]$V166 <= splits2[i])\r\n  gr2 <- which(readability_sub[node1,]$V166 > splits2[i])\r\n  gr3 <- node2\r\n  \r\n  pr1 <- mean(readability_sub[node1[gr1],]$target)\r\n  pr2 <- mean(readability_sub[node1[gr2],]$target)\r\n  pr3 <- mean(readability_sub[node2,]$target)\r\n\r\n    \r\n  sse2[i] = sum((readability_sub[node1[gr1],]$target - pr1)^2) + \r\n            sum((readability_sub[node1[gr2],]$target - pr2)^2) +\r\n            sum((readability_sub[node2,]$target - pr3)^2) \r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=splits2,y=sse2))+\r\n  xlab('Split Points (Feature 166) on Node 1')+\r\n  ylab('Sum of Squared Residuals')+\r\n  theme_bw()+\r\n  geom_vline(xintercept = 0.066,lty=2)+\r\n  annotate('text',x=0.06,y=12,label='0.066')\r\n\r\n\r\nmin(sse2)\r\n\r\n[1] 8.007196\r\n\r\nScenario 3:\r\n\r\n\r\n# Splits based on Feature 220 for node 2\r\n\r\nsplits3 <- seq(-0.24,0.56,0.001)\r\n\r\nsse3 <- c()\r\n\r\nfor(i in 1:length(splits3)){\r\n  \r\n  gr1 <- which(readability_sub[node2,]$V220 <= splits3[i])\r\n  gr2 <- which(readability_sub[node2,]$V220 > splits3[i])\r\n  gr3 <- node1\r\n  \r\n  pr1 <- mean(readability_sub[node2[gr1],]$target)\r\n  pr2 <- mean(readability_sub[node2[gr2],]$target)\r\n  pr3 <- mean(readability_sub[node1,]$target)\r\n\r\n    \r\n  sse3[i] = sum((readability_sub[node2[gr1],]$target - pr1)^2) + \r\n            sum((readability_sub[node2[gr2],]$target - pr2)^2) +\r\n            sum((readability_sub[node1,]$target - pr3)^2) \r\n}\r\n\r\n\r\nggplot()+\r\n  geom_line(aes(x=splits3,y=sse3))+\r\n  xlab('Split Points (Feature 220) on Node 2')+\r\n  ylab('Sum of Squared Residuals')+\r\n  theme_bw()+\r\n  geom_vline(xintercept = 0.178,lty=2)+\r\n  annotate('text',x=0.16,y=12,label='0.178')\r\n\r\n\r\nmin(sse3)\r\n\r\n[1] 10.9436\r\n\r\nScenario 4:\r\n\r\n\r\n# Splits based on Feature 166 for node 2\r\n\r\nsplits4 <-  seq(-0.02,0.22,.001)\r\n\r\nsse4 <- c()\r\n\r\nfor(i in 1:length(splits4)){\r\n  \r\n  gr1 <- which(readability_sub[node2,]$V166 <= splits4[i])\r\n  gr2 <- which(readability_sub[node2,]$V166 > splits4[i])\r\n  gr3 <- node1\r\n  \r\n  pr1 <- mean(readability_sub[node2[gr1],]$target)\r\n  pr2 <- mean(readability_sub[node2[gr2],]$target)\r\n  pr3 <- mean(readability_sub[node1,]$target)\r\n\r\n    \r\n  sse4[i] = sum((readability_sub[node2[gr1],]$target - pr1)^2) + \r\n            sum((readability_sub[node2[gr2],]$target - pr2)^2) +\r\n            sum((readability_sub[node1,]$target - pr3)^2) \r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=splits4,y=sse4))+\r\n  xlab('Split Points (Feature 166) on Node 2')+\r\n  ylab('Sum of Squared Residuals')+\r\n  theme_bw()+\r\n  geom_vline(xintercept = 0.067,lty=2)+\r\n  annotate('text',x=0.06,y=12,label='0.067')\r\n\r\n\r\nmin(sse4)\r\n\r\n[1] 9.964654\r\n\r\nBased on our search, the following splits provided the least\r\nSSEs:\r\n1st terminal node, Split variable: Feature220, Split Point:\r\n-0.19, SSE = 8.01\r\n1st terminal node, Split variable: Feature166, Split Point:\r\n0.066, SSE = 8.01\r\n2nd terminal node, Split variable: Feature220, Split Point:\r\n0.178, SSE = 10.94\r\n2nd terminal node, Split variable: Feature166, Split Point:\r\n0.067, SSE = 9.96\r\nWe can decide to continue with either Scenario 1 or 2 because they\r\nyield the minimum SSE. Assuming we decide Scenario 1, our tree model now\r\nlooks like this.\r\n\r\n\r\n\r\nTermination of Growth\r\nThe search process continues until a specific criterion is met to\r\nstop the algorithm. There may be several conditions where we may\r\nconstrain the growth and force the algorithm to stop searching for\r\nadditional growth in the tree model. Some of these are listed below:\r\nMinimizing SSE: the algorithm stops when there is no potential\r\nsplit in any of the existing nodes that would reduce the sum of squared\r\nerrors.\r\nThe minimum number of observations to split: the algorithm does\r\nnot attempt to split a node unless there is a certain number of\r\nobservations in the node.\r\nMaximum depth: The algorithm stops searching when the tree\r\nreaches a certain depth.\r\n1.1.5. Pruning a tree\r\nOverfitting and underfitting also occur as one develops a tree model.\r\nWhen the depth of a tree becomes unnecessarily large, there is a risk of\r\noverfitting (increased variance in model predictions across samples,\r\nless generalizable). On the other hand, when the depth of a tree is\r\nsmall, there is a risk of underfitting (increased bias in model\r\npredictions, underperforming model).\r\nTo balance the model variance and model bias, we considered\r\npenalizing significant coefficients in regression models yielding\r\ndifferent types of penalty terms on the regression coefficients. There\r\nis a similar approach that can be applied to tree models. In the context\r\nof tree models, the loss function with a penalty term can be specified\r\nas\r\n\\[ SSE = \\sum_{t=1}^{T} \\sum_{i \\epsilon\r\nR_t} (Y_i - \\hat{Y}_{R_{t}})^2 + \\alpha T.\\]\r\nThe penalty term \\(\\alpha\\) is known\r\nas the cost complexity parameter. The product term\r\n\\(\\alpha T\\) increases as the number of\r\nterminal nodes increases in the model, so this term penalizes the\r\nincreasing complexity of the tree model. By fine-tuning the value of\r\n\\(\\alpha\\) through cross-validation, we\r\ncan find a balance between model variance and bias, as we did for\r\nregression models. The process is called pruning because the terminal\r\nnodes from the tree are eliminated in a nested way for increasing levels\r\nof \\(\\alpha\\).\r\n1.2. Growing trees with\r\nthe rpart() function\r\n1.2.1. A simple example\r\nWe can develop a tree model using the rpart() function\r\nfrom the rpart package. Also, we will use the\r\nfancyRpartPlot function from the rattle\r\npackage to get a nice plot of a tree model.\r\n\r\n\r\n#install.packages('rpart')\r\n\r\n#install.packages('rattle')\r\n\r\nrequire(rpart)\r\nrequire(rattle)\r\n\r\n#?rpart\r\n#?fancyRpartPlot\r\n\r\n\r\nLet’s first replicate our search above to build a tree model\r\npredicting the readability scores from Feature 220 and Feature 166 in\r\nthe toy dataset.\r\n\r\n\r\ndt <- rpart(formula = target ~ V220 + V166,\r\n            data    = readability_sub,\r\n            method  = \"anova\",\r\n            control = list(minsplit  = 1,\r\n                           cp        = 0,\r\n                           minbucket = 1,\r\n                           maxdepth  = 2)\r\n            )\r\n\r\n\r\nThe formula argument works similarly as in the\r\nregression models. The variable name on the left side of ~\r\nrepresents the outcome variable, and the variable names on the right\r\nside of ~ represent the predictor variables.\r\nThe data argument provides the name of the dataset\r\nto find these variables specified in the formula.\r\nThe method argument is set to anova to\r\nindicate that the outcome is a continuous variable and this is a\r\nregression problem.\r\nThe control argument is a list object with several\r\nsettings to be used during tree-building. For more information, check\r\n?rpart.control. You can accept the default values by not\r\nspecifying it at all. Here, I specified a few important ones.\r\nminsplit=1 forces algorithm not to attempt to split\r\na node unless there is at least one observation.\r\nminbucket=1 forces algorithm to have at least one\r\nobservation for any node.\r\nmaxdepth = 2 forces algorithm to stop when the depth\r\nof a tree reaches to 2.\r\ncp=0 indicates that we do not want to apply any\r\npenalty term (\\(\\lambda = 0\\)) during\r\nthe model building process.\r\n\r\n\r\nfancyRpartPlot(dt,type=2,sub='')\r\n\r\n\r\n  # ?prp\r\n  # check for a lot of settings for modifying this plot\r\n\r\n\r\nAs you can see, we got the same tree model we built in our search\r\nprocess in the earlier section. The algorithm went further and split the\r\nsecond node based on Feature 166.\r\nYou can also ask for more specific information about the\r\nmodel-building process by running the summary()\r\nfunction.\r\n\r\n\r\nsummary(dt)\r\n\r\nCall:\r\nrpart(formula = target ~ V220 + V166, data = readability_sub, \r\n    method = \"anova\", control = list(minsplit = 1, cp = 0, minbucket = 1, \r\n        maxdepth = 2))\r\n  n= 20 \r\n\r\n         CP nsplit rel error   xerror      xstd\r\n1 0.3121563      0 1.0000000 1.145020 0.5190773\r\n2 0.2363040      1 0.6878437 1.546951 0.5899322\r\n3 0.1259195      2 0.4515397 1.480240 0.5000914\r\n4 0.0000000      3 0.3256202 1.405163 0.4851406\r\n\r\nVariable importance\r\nV220 V166 \r\n  81   19 \r\n\r\nNode number 1: 20 observations,    complexity param=0.3121563\r\n  mean=-0.7633224, MSE=0.8866547 \r\n  left son=2 (3 obs) right son=3 (17 obs)\r\n  Primary splits:\r\n      V220 < -0.02634472 to the left,  improve=0.31215630, (0 missing)\r\n      V166 < 0.1893695   to the right, improve=0.06252757, (0 missing)\r\n\r\nNode number 2: 3 observations,    complexity param=0.236304\r\n  mean=-2.015676, MSE=1.811347 \r\n  left son=4 (2 obs) right son=5 (1 obs)\r\n  Primary splits:\r\n      V220 < -0.191364   to the right, improve=0.7711389, (0 missing)\r\n      V166 < 0.0660439   to the right, improve=0.7711389, (0 missing)\r\n\r\nNode number 3: 17 observations,    complexity param=0.1259195\r\n  mean=-0.5423187, MSE=0.3978562 \r\n  left son=6 (4 obs) right son=7 (13 obs)\r\n  Primary splits:\r\n      V166 < 0.06651001  to the left,  improve=0.3301433, (0 missing)\r\n      V220 < 0.1786538   to the left,  improve=0.1854056, (0 missing)\r\n\r\nNode number 4: 2 observations\r\n  mean=-2.85138, MSE=0.6218203 \r\n\r\nNode number 5: 1 observations\r\n  mean=-0.3442698, MSE=0 \r\n\r\nNode number 6: 4 observations\r\n  mean=-1.195684, MSE=0.2982949 \r\n\r\nNode number 7: 13 observations\r\n  mean=-0.3412833, MSE=0.2567257 \r\n\r\n1.2.2. Tree\r\npruning by increasing the complexity parameter\r\nWe will now expand the model by adding more predictors to be\r\nconsidered for the tree model. Suppose that we now have six different\r\npredictors to be considered for the tree model. We will provide the same\r\nsettings with six predictors except for the complexity parameter. We\r\nwill fit three models by setting the complexity parameter at 0, 0.05,\r\nand 0.1 to see what happens to the tree model as we increase the\r\ncomplexity parameter.\r\n\r\n\r\ndt <- rpart(formula = target ~ V78 + V166 + V220 + V375 + V562 + V568,\r\n            data    = readability_sub,\r\n            method  = \"anova\",\r\n            control = list(minsplit=1,\r\n                           cp=0,\r\n                           minbucket = 1,\r\n                           maxdepth = 5)\r\n            )\r\n\r\n\r\nfancyRpartPlot(dt,type=2,sub='')\r\n\r\n\r\n\r\n\r\n\r\ndt <- rpart(formula = target ~ V78 + V166 + V220 + V375 + V562 + V568,\r\n            data    = readability_sub,\r\n            method  = \"anova\",\r\n            control = list(minsplit=1,\r\n                           cp=0.05,\r\n                           minbucket = 1,\r\n                           maxdepth = 5)\r\n            )\r\n\r\n\r\nfancyRpartPlot(dt,type=2,sub='')\r\n\r\n\r\n\r\n\r\n\r\ndt <- rpart(formula = target ~ V78 + V166 + V220 + V375 + V562 + V568,\r\n            data    = readability_sub,\r\n            method  = \"anova\",\r\n            control = list(minsplit=1,\r\n                           cp=0.1,\r\n                           minbucket = 1,\r\n                           maxdepth = 5)\r\n            )\r\n\r\n\r\nfancyRpartPlot(dt,type=2,sub='')\r\n\r\n\r\n\r\n2.\r\nPredicting a continuous outcome with a Decision Tree via\r\ncaret:train()\r\nPlease review the following notebook that builds a prediction model\r\nusing a decision tree algorithm for the readability dataset.\r\nBuilding\r\na Prediction Model using a Decision Tree\r\n3.\r\nPredicting a binary outcome with a Decision Tree via\r\ncaret:train()\r\nDeveloping the decision trees to predict a binary outcome follows a\r\nvery similar approach. The only difference is that the loss function and\r\nthe metric used while selecting the best split is different while\r\ngrowing the tree model. A more special metric (e.g., gini index or\r\nentropy) is used to decide the best split that improves the accuracy of\r\npredictions.\r\nThis\r\ndocument provides a detailed technical discussion of how\r\nrpart builds a decision tree.\r\nPlease review the following notebook that builds a classification\r\nmodel using a decision tree algorithm for the full recidivism\r\ndataset.\r\nBuilding\r\na Classification Model using a Decision Tree\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:10:07-08:00"
    },
    {
      "path": "lecture-7a.html",
      "title": "Bagged Trees and Random Forests",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "11/23/2022",
      "contents": "\r\n\r\nContents\r\n1. The Concept of\r\nBootstrap Aggregation (BAGGING)\r\n1.1. BAGGING: Do\r\nIt Yourself with the rpart package\r\n1.2. BAGGING\r\nwith the ranger and caret::train()\r\npackages\r\n1.3. Tuning the\r\nNumber of Tree Models in Bagging\r\n\r\n2. Random\r\nForests\r\n3.\r\nPredicting Recidivism using Bagges Trees and Random Forests\r\n3.1. Bagged\r\nTrees\r\n3.2. Random\r\nForests\r\n\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:10:08 ]\r\n1. The Concept of\r\nBootstrap Aggregation (BAGGING)\r\nThe concept of bagging is based on the idea that predictions from an\r\nensemble of models are better than any single model predictions. Suppose\r\nwe randomly draw multiple samples from a population and then develop a\r\nprediction model for an outcome using each sample. The aggregated\r\npredictions from these multiple models would perform better due to the\r\nreduced model variance (aggregation would reduce noise due to\r\nsampling).\r\nDue to the lack of access to the population (even if we assume there\r\nis a well-defined population), we can mimic the sampling from a\r\npopulation by replacing it with bootstrapping. A\r\nBootstrap sample is a random sample with replacement\r\nfrom the sample data.\r\nSuppose there is sample data with ten observations and three\r\npredictors. Below are five bootstrap samples from this sample data.\r\n\r\n\r\nd <- data.frame(x1 =c(20,25,30,42,10,60,65,55,80,90),\r\n                x2 =c(10,15,12,20,45,75,70,80,85,90),\r\n                x3 =c(25,30,35,20,40,80,85,90,92,95),\r\n                label= c('A','B','C','D','E','F','G','H','I','J'))\r\n\r\nd\r\n\r\n   x1 x2 x3 label\r\n1  20 10 25     A\r\n2  25 15 30     B\r\n3  30 12 35     C\r\n4  42 20 20     D\r\n5  10 45 40     E\r\n6  60 75 80     F\r\n7  65 70 85     G\r\n8  55 80 90     H\r\n9  80 85 92     I\r\n10 90 90 95     J\r\n\r\nset.seed(11232022)\r\n\r\n# Bootstrap sample 1\r\n\r\nd[sample(1:10,replace = TRUE),]\r\n\r\n     x1 x2 x3 label\r\n2    25 15 30     B\r\n2.1  25 15 30     B\r\n6    60 75 80     F\r\n1    20 10 25     A\r\n8    55 80 90     H\r\n10   90 90 95     J\r\n10.1 90 90 95     J\r\n8.1  55 80 90     H\r\n10.2 90 90 95     J\r\n3    30 12 35     C\r\n\r\n# Bootstrap sample 2\r\n\r\nd[sample(1:10,replace = TRUE),]\r\n\r\n     x1 x2 x3 label\r\n4    42 20 20     D\r\n9    80 85 92     I\r\n9.1  80 85 92     I\r\n10   90 90 95     J\r\n6    60 75 80     F\r\n10.1 90 90 95     J\r\n3    30 12 35     C\r\n2    25 15 30     B\r\n9.2  80 85 92     I\r\n4.1  42 20 20     D\r\n\r\n# Bootstrap sample 3\r\n\r\nd[sample(1:10,replace = TRUE),]\r\n\r\n    x1 x2 x3 label\r\n9   80 85 92     I\r\n3   30 12 35     C\r\n6   60 75 80     F\r\n3.1 30 12 35     C\r\n4   42 20 20     D\r\n5   10 45 40     E\r\n4.1 42 20 20     D\r\n8   55 80 90     H\r\n10  90 90 95     J\r\n3.2 30 12 35     C\r\n\r\n# Bootstrap sample 4\r\n\r\nd[sample(1:10,replace = TRUE),]\r\n\r\n     x1 x2 x3 label\r\n8    55 80 90     H\r\n7    65 70 85     G\r\n3    30 12 35     C\r\n1    20 10 25     A\r\n10   90 90 95     J\r\n2    25 15 30     B\r\n9    80 85 92     I\r\n10.1 90 90 95     J\r\n6    60 75 80     F\r\n10.2 90 90 95     J\r\n\r\n# Bootstrap sample 5\r\n\r\nd[sample(1:10,replace = TRUE),]\r\n\r\n    x1 x2 x3 label\r\n3   30 12 35     C\r\n10  90 90 95     J\r\n5   10 45 40     E\r\n8   55 80 90     H\r\n6   60 75 80     F\r\n3.1 30 12 35     C\r\n1   20 10 25     A\r\n4   42 20 20     D\r\n9   80 85 92     I\r\n8.1 55 80 90     H\r\n\r\nThe process of bagging is building separate models for each bootstrap\r\nsample and then applying all these models to a new observation for\r\npredicting the outcome. Finally, these predictions are aggregated in\r\nsome form (e.g., taking the average) to obtain a final prediction for\r\nthe new observation. The idea of bagging can technically be applied to\r\nany prediction model (e.g., CNN’s, regression models). During the model\r\nprocess from each bootstrap sample, no regularization was applied, and\r\nmodels were developed to their full complexity. So, we obtain so many\r\nunbiased models. While each model has a significant sample variance, we\r\nhope to reduce this sampling variance by aggregating the predictions\r\nfrom all these models at the end.\r\n   \r\n\r\n\r\n\r\n1.1. BAGGING: Do\r\nIt Yourself with the rpart package\r\nIn this section, we will apply the bagging idea to decision trees to\r\npredict the readability scores. First, we import and prepare data for\r\nmodeling. Then, we split the data into training and test pieces.\r\n\r\n\r\n# Import the dataset\r\n\r\nreadability <- read.csv(here('data/readability_features.csv'),header=TRUE)\r\n\r\n# Write the recipe\r\n\r\nrequire(recipes)\r\n\r\nblueprint_readability <- recipe(x     = readability,\r\n                    vars  = colnames(readability),\r\n                    roles = c(rep('predictor',768),'outcome')) %>%\r\n             step_zv(all_numeric()) %>%\r\n             step_nzv(all_numeric()) %>%\r\n             step_normalize(all_numeric_predictors())\r\n             \r\n# Train/Test Split\r\n\r\nset.seed(10152021)  # for reproducibility\r\n  \r\nloc      <- sample(1:nrow(readability), round(nrow(readability) * 0.9))\r\nread_tr  <- readability[loc, ]\r\nread_te  <- readability[-loc, ]\r\n\r\ndim(read_tr)\r\n\r\ndim(read_te)\r\n\r\n\r\nThe code below will take a\r\nbootstrap sample from training data,\r\n\r\ndevelop a full tree model with no pruning, and\r\n\r\nsave the model object as an element of a list.\r\n\r\nWe will repeat this process ten times.\r\n\r\n\r\nrequire(caret)\r\n\r\nbag.models <- vector('list',10)\r\n\r\nfor(i in 1:10){\r\n\r\n  # Bootstrap sample\r\n  \r\n    temp_rows <- sample(1:nrow(read_tr),nrow(read_tr),replace=TRUE)\r\n  \r\n    temp <- read_tr[temp_rows,]\r\n\r\n  # Train the tree model with no pruning and no cross validation\r\n    \r\n    grid <- data.frame(cp=0)\r\n    cv <- trainControl(method = \"none\")\r\n        \r\n    bag.models[[i]] <- caret::train(blueprint_readability,\r\n                                    data      = temp,\r\n                                    method    = 'rpart',\r\n                                    tuneGrid  = grid,\r\n                                    trControl = cv,\r\n                                    control   = list(minsplit=20,\r\n                                                     minbucket = 2,\r\n                                                     maxdepth = 60))\r\n\r\n}\r\n\r\n\r\nNow, we will use each of these models to predict the readability\r\nscore for the test data. We will also average these predictions. Then,\r\nwe will save the predictions in a matrix form to compare.\r\n\r\n\r\n\r\n\r\n\r\npreds <- data.frame(obs = read_te[,c('target')])\r\n\r\npreds$model1  <- predict(bag.models[[1]],read_te)\r\npreds$model2  <- predict(bag.models[[2]],read_te)\r\npreds$model3  <- predict(bag.models[[3]],read_te)\r\npreds$model4  <- predict(bag.models[[4]],read_te)\r\npreds$model5  <- predict(bag.models[[5]],read_te)\r\npreds$model6  <- predict(bag.models[[6]],read_te)\r\npreds$model7  <- predict(bag.models[[7]],read_te)\r\npreds$model8  <- predict(bag.models[[8]],read_te)\r\npreds$model9  <- predict(bag.models[[9]],read_te)\r\npreds$model10 <- predict(bag.models[[10]],read_te)\r\n\r\npreds$average <- rowMeans(preds[,2:11])\r\n\r\nhead(round(preds,3))\r\n\r\n     obs model1 model2 model3 model4 model5 model6 model7 model8\r\n1  0.246 -0.827  0.355  0.619 -0.925 -0.956  0.260 -0.095 -0.224\r\n2 -0.188 -1.399 -0.963 -0.458  0.635 -0.415 -0.684 -0.101 -0.963\r\n3 -0.135 -0.235  1.149  0.827  0.663  0.015  0.865  0.693 -0.013\r\n4  0.395  0.690  0.355  1.361  0.025 -0.271  0.119 -0.406 -0.044\r\n5 -0.371 -1.232 -0.901 -1.968 -1.147 -1.420 -1.393 -1.104  0.114\r\n6 -1.156 -0.173 -0.531 -0.694 -1.390 -0.508 -0.690 -0.769  0.360\r\n  model9 model10 average\r\n1 -0.610   0.433  -0.197\r\n2 -0.610   0.259  -0.470\r\n3  0.242  -0.253   0.395\r\n4  0.005   0.739   0.257\r\n5 -1.024  -1.125  -1.120\r\n6 -0.252  -0.894  -0.554\r\n\r\nNow, let’s compute the RMSE for each model’s predicted scores and the\r\nRMSE for the average of predicted scores from all ten tree models.\r\n\r\n\r\np1 <- sqrt(mean((preds$obs - preds$model1)^2))\r\np2 <- sqrt(mean((preds$obs - preds$model2)^2))\r\np3 <- sqrt(mean((preds$obs - preds$model3)^2))\r\np4 <- sqrt(mean((preds$obs - preds$model4)^2))\r\np5 <- sqrt(mean((preds$obs - preds$model5)^2))\r\np6 <- sqrt(mean((preds$obs - preds$model6)^2))\r\np7 <- sqrt(mean((preds$obs - preds$model7)^2))\r\np8 <- sqrt(mean((preds$obs - preds$model8)^2))\r\np9 <- sqrt(mean((preds$obs - preds$model9)^2))\r\np10 <- sqrt(mean((preds$obs - preds$model10)^2))\r\n\r\np.ave <- sqrt(mean((preds$obs - preds$average)^2))\r\n\r\n\r\nggplot()+\r\n  geom_point(aes(x = 1:11,y=c(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p.ave)))+\r\n  xlab('Model Predictions') +\r\n  ylab('RMSE') +\r\n  ylim(0,1) + \r\n  scale_x_continuous(breaks = 1:11,\r\n                     labels=c('Model 1','Model 2', 'Model 3', 'Model 4', \r\n                              'Model 5','Model 6', 'Model 7', 'Model 8',\r\n                              'Model 9','Model 10','Bagged'))+\r\n  theme_bw()+\r\n  annotate('text',\r\n           x = 1:11,\r\n           y=c(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p.ave)*1.03,\r\n           label = round(c(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p.ave),3),\r\n           cex=3)\r\n\r\n\r\n\r\nAs it is evident, the bagging of 10 different tree models\r\nsignificantly improved the predictions on the test dataset.\r\n1.2. BAGGING\r\nwith the ranger and caret::train()\r\npackages\r\nInstead of writing your code to implement the idea of bagging for\r\ndecision trees, we can use the ranger method via\r\ncaret::train().\r\n\r\n\r\nrequire(ranger)\r\n\r\ngetModelInfo()$ranger$parameters\r\n\r\n      parameter     class                         label\r\n1          mtry   numeric #Randomly Selected Predictors\r\n2     splitrule character                Splitting Rule\r\n3 min.node.size   numeric             Minimal Node Size\r\n\r\nThe caret::train() allows us manipulate three parameters\r\nwhile using the ranger method:\r\nsplitrule: set this to ‘variance’ for regression\r\nproblems with continuous outcome. Other alternatives are\r\nextratrees, maxstat, and\r\nbeta.\r\nmin.node.size.: this is identical to\r\nminbucket argument in the rpart method and\r\nindicates the minimum number of observations for each node.\r\nmtry: this is the most critical parameter for this\r\nmethod and indicates the number of predictors to consider for developing\r\ntree models. For bagged decision trees, you can set this to the number\r\nof all predictors in your model\r\n\r\n\r\n# Cross validation settings \r\n    \r\n    read_tr = read_tr[sample(nrow(read_tr)),]\r\n  \r\n    # Create 10 folds with equal size\r\n    \r\n    folds = cut(seq(1,nrow(read_tr)),breaks=10,labels=FALSE)\r\n    \r\n    # Create the list for each fold \r\n    \r\n    my.indices <- vector('list',10)\r\n    for(i in 1:10){\r\n      my.indices[[i]] <- which(folds!=i)\r\n    }\r\n    \r\n    cv <- trainControl(method = \"cv\",\r\n                       index  = my.indices)\r\n\r\n# Grid, running with all predictors in the data (768)\r\n\r\ngrid <- expand.grid(mtry = 768,splitrule='variance',min.node.size=2)\r\ngrid\r\n\r\n  mtry splitrule min.node.size\r\n1  768  variance             2\r\n\r\n# Bagging with 10 tree models\r\n\r\nbagged.trees <- caret::train(blueprint_readability,\r\n                             data      = read_tr,\r\n                             method    = 'ranger',\r\n                             trControl = cv,\r\n                             tuneGrid  = grid,\r\n                             num.trees = 10,\r\n                             max.depth = 60)\r\n\r\n\r\nLet’s check the cross-validated performance metrics.\r\n\r\n\r\nbagged.trees$results\r\n\r\n  mtry splitrule min.node.size      RMSE  Rsquared      MAE\r\n1  768  variance             2 0.6804536 0.5683141 0.543069\r\n      RMSESD RsquaredSD      MAESD\r\n1 0.02011178 0.02586802 0.01373682\r\n\r\nThe performance is very similar to what we got in our DIY\r\ndemonstration.\r\nA couple of things to note:\r\nWhen you set max.depth= argument within the\r\ncaret::train function, it passes this to the\r\nranger function. Try to set this number to as large as\r\npossible, so you develop each tree to its full capacity.\r\nThe penalty term is technically zero (cp parameter\r\nin the rpart function) while building each tree model. In\r\nBagging, we deal with the model variance differently. Instead of\r\napplying a penalty term, we ensemble many unpenalized tree models to\r\nreduce the model variance.\r\nIt is a little tricky to obtain reproducible results from this\r\nprocedure. See this link to\r\nlearn more about accomplishing that.\r\nThe number of trees (bootstrap samples) is a hyperparameter to\r\ntune. Conceptually, the model performance will improve as you increase\r\nthe number of tree models used in Bagging; however, the performance will\r\nstabilize at some point. It is a tuning process to find the minimum\r\nnumber of tree models in Bagging to obtain the maximal model\r\nperformance.\r\n1.3. Tuning the\r\nNumber of Tree Models in Bagging\r\nUnfortunately, caret::train does not let us define the\r\nnum.trees argument as a hyperparameter in the grid search.\r\nSo, the only way to search for the optimal number of trees is to use the\r\nranger method via caret::train function and\r\niterate over a set of values for the num.trees argument.\r\nThen, compare the model performance and pick the optimal number of tree\r\nmodels.\r\nThe code below implements this idea and saves the results from each\r\niteration in a list object.\r\n\r\n\r\n# Run the bagged trees by iterating over num.trees using the \r\n# values 5, 20, 40, 60,  ..., 200\r\n  \r\n  nbags <- c(5,seq(from = 20,to = 200, by = 20))\r\n    \r\n  bags <- vector('list',length(nbags))\r\n    \r\n    for(i in 1:length(nbags)){\r\n      \r\n      bags[[i]] <- caret::train(blueprint_readability,\r\n                                data      = read_tr,\r\n                                method    = 'ranger',\r\n                                trControl = cv,\r\n                                tuneGrid  = grid,\r\n                                num.trees = nbags[i],\r\n                                max.depth = 60)\r\n      \r\n      print(i)\r\n      \r\n    }\r\n\r\n    # This can take a few hours to run.\r\n\r\n\r\nLet’s check the cross-validated RMSE for the bagged tree models with\r\ndifferent number of trees.\r\n\r\n\r\n\r\n\r\n\r\nrmses <- c()\r\n\r\nfor(i in 1:length(nbags)){\r\n  \r\n  rmses[i] = bags[[i]]$results$RMSE\r\n  \r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=nbags,y=rmses))+\r\n  xlab('Number of Trees')+\r\n  ylab('RMSE')+\r\n  ylim(c(0.6,0.75))+\r\n  theme_bw()\r\n\r\n\r\nnbags[which.min(rmses)]\r\n\r\n[1] 180\r\n\r\nIt indicates that the RMSE stabilizes after roughly 60 tree models.\r\nWe can see that a bagged tree model with 180 trees gave the best result.\r\nLet’s see how well this model performs on the test data.\r\n\r\n\r\n# Predictions from a Bagged tree model with 180 trees\r\n\r\npredicted_te <- predict(bags[[10]],read_te)\r\n\r\n# MAE\r\n\r\nmean(abs(read_te$target - predicted_te))\r\n\r\n[1] 0.4776075\r\n\r\n# RMSE\r\n\r\nsqrt(mean((read_te$target - predicted_te)^2))\r\n\r\n[1] 0.6003086\r\n\r\n# R-square\r\n\r\ncor(read_te$target,predicted_te)^2\r\n\r\n[1] 0.6646118\r\n\r\nNow, we can add this to our comparison list to remember how well this\r\nperforms compared to other methods.\r\n\r\nR-square\r\nMAE\r\nRMSE\r\nLinear Regression\r\n0.658\r\n0.499\r\n0.620\r\nRidge Regression\r\n0.727\r\n0.432\r\n0.536\r\nLasso Regression\r\n0.721\r\n0.433\r\n0.542\r\nElastic Net\r\n0.726\r\n0.433\r\n0.539\r\nKNN\r\n0.611\r\n0.519\r\n0.648\r\nDecision Tree\r\n0.499\r\n0.574\r\n0.724\r\nBagged Trees\r\n0.664\r\n0.478\r\n0.600\r\n2. Random Forests\r\nRandom Forests is an idea very similar to Bagging with an extra\r\nfeature. In Random Forests, while we take a bootstrap sample of\r\nobservations (a random sample of rows in training data with\r\nreplacement), we also take a random sample of columns for each split\r\nwhile developing a tree model. It allows us to develop tree models more\r\nindependently of each other.\r\nWhen specific important predictors are related to the outcome, the\r\ntree models developed using all predictors will be very similar,\r\nparticularly at the top nodes, although we take bootstrap samples. These\r\ntrees will be correlated to each other, which may reduce the efficiency\r\nof BAGGING in reducing the variance. We can diversify the tree models by\r\nrandomly sampling a certain number of predictors while developing each\r\ntree. It turns out that a diverse group of tree models does much better\r\nin predicting the outcome than a group of tree models similar to each\r\nother.\r\n\r\n\r\n\r\nWe can use the same ranger package to fit the random\r\nforests models by only changing the mtry argument in our\r\ngrid. Below, we will fit a random forests model with ten trees by\r\nrandomly sampling from rows for each tree. In addition, when we develop\r\neach tree model, we will also randomly sample 300 predictors. I set\r\nmtry=300 in the grid object, indicating that\r\nit will randomly sample 300 predictors to consider for each split when\r\ndeveloping each tree.\r\n\r\n\r\n# Grid, randomly sample 300 predictors\r\n\r\ngrid <- expand.grid(mtry = 300,splitrule='variance',min.node.size=2)\r\ngrid\r\n\r\n  mtry splitrule min.node.size\r\n1  300  variance             2\r\n\r\n# Random Forest with 10 tree models\r\n\r\nrforest <- caret::train(blueprint_readability,\r\n                        data      = read_tr,\r\n                        method    = 'ranger',\r\n                        trControl = cv,\r\n                        tuneGrid  = grid,\r\n                        num.trees = 10,\r\n                        max.depth = 60)\r\n \r\nrforest$times\r\n\r\n$everything\r\n   user  system elapsed \r\n 177.76    1.08  133.97 \r\n\r\n$final\r\n   user  system elapsed \r\n  15.87    0.05   11.38 \r\n\r\n$prediction\r\n[1] NA NA NA\r\n\r\nLet’s check the cross-validated performance metrics.\r\n\r\n\r\nrforest$results\r\n\r\n  mtry splitrule min.node.size      RMSE  Rsquared       MAE\r\n1  300  variance             2 0.6829234 0.5674509 0.5430574\r\n      RMSESD RsquaredSD      MAESD\r\n1 0.01628784 0.02833995 0.01213438\r\n\r\nFor random forests, there are two hyperparameters to tune:\r\nmtry, the number of predictors to choose for each\r\nsplit during the tree model development\r\nnum.trees, the number of trees.\r\nAs mentioned before, unfortunately, the caret::train\r\nonly allows mtry in the grid search. For the number of\r\ntrees, one should embed it in a for loop to iterate over a\r\nset of values. The code below hypothetically implements this idea by\r\ntrying ten different mtry values\r\n(100,150,200,250,300,350,400,450,500,550) and saves the results from\r\neach iteration in a list object. However, I haven’t run it, which may\r\ntake a long time.\r\n\r\n\r\n# Grid Settings  \r\n\r\n    grid <- expand.grid(mtry = c(100,150,200,250,300,350,400,450,500,550),\r\n                        splitrule='variance',\r\n                        min.node.size=2)\r\n\r\n# Run the bagged trees by iterating over num.trees values from 1 to 200\r\n  \r\n    bags <- vector('list',200)\r\n    \r\n    for(i in 1:200){\r\n      \r\n      bags[[i]] <- caret::train(blueprint_readability,\r\n                                data      = read_tr,\r\n                                method    = 'ranger',\r\n                                trControl = cv,\r\n                                tuneGrid  = grid,\r\n                                num.trees = i,\r\n                                max.depth = 60,)\r\n      \r\n    }\r\n\r\n\r\nInstead, I run this by fixing mtry=300 and then\r\niterating over the number of trees for values of 5, 20, 40, 60, 80, …,\r\n200 (as we did for bagged trees).\r\n\r\n\r\n\r\n\r\n\r\nrmses <- c()\r\n\r\nfor(i in 1:length(nbags)){\r\n  \r\n  rmses[i] = bags[[i]]$results$RMSE\r\n  \r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=nbags,y=rmses))+\r\n  xlab('Number of Trees')+\r\n  ylab('RMSE')+\r\n  ylim(c(0.6,0.75))+\r\n  theme_bw()\r\n\r\n\r\nnbags[which.min(rmses)]\r\n\r\n[1] 160\r\n\r\nRMSE similarly stabilized after roughly 60 trees. Let’s see how well\r\nthe model with 200 trees perform.\r\n\r\n\r\n# Predictions from a Random Forest model with 160 trees\r\n\r\npredicted_te <- predict(bags[[11]],read_te)\r\n\r\n# MAE\r\n\r\nmean(abs(read_te$target - predicted_te))\r\n\r\n[1] 0.4756419\r\n\r\n# RMSE\r\n\r\nsqrt(mean((read_te$target - predicted_te)^2))\r\n\r\n[1] 0.6003527\r\n\r\n# R-square\r\n\r\ncor(read_te$target,predicted_te)^2\r\n\r\n[1] 0.6690829\r\n\r\nBelow is our comparison table with Random Forests added. As you see,\r\nthere is a slight improvement over Bagged Trees, and we can improve this\r\na little more by trying different values of mtry and\r\nfinding an optimal number.\r\n\r\nR-square\r\nMAE\r\nRMSE\r\nLinear Regression\r\n0.658\r\n0.499\r\n0.620\r\nRidge Regression\r\n0.727\r\n0.432\r\n0.536\r\nLasso Regression\r\n0.721\r\n0.433\r\n0.542\r\nElastic Net\r\n0.726\r\n0.433\r\n0.539\r\nKNN\r\n0.611\r\n0.519\r\n0.648\r\nDecision Tree\r\n0.499\r\n0.574\r\n0.724\r\nBagged Trees\r\n0.664\r\n0.478\r\n0.600\r\nRandom Forests\r\n0.669\r\n0.476\r\n0.600\r\n3.\r\nPredicting Recidivism using Bagges Trees and Random Forests\r\nIn this section, I provide the R code to predict recidivism using\r\nBagged Trees and Random Forests.\r\nImport the recidivism dataset and pre-process the\r\nvariables\r\n\r\n\r\n# Import data\r\n\r\nrecidivism <- read.csv('./data/recidivism_y1 removed and recoded.csv',header=TRUE)\r\n\r\n# Write the recipe\r\n\r\n  # List of variable types \r\n  \r\n  outcome <- c('Recidivism_Arrest_Year2')\r\n  \r\n  id      <- c('ID')\r\n  \r\n  categorical <- c('Residence_PUMA',\r\n                   'Prison_Offense',\r\n                   'Age_at_Release',\r\n                   'Supervision_Level_First',\r\n                   'Education_Level',\r\n                   'Prison_Years',\r\n                   'Gender',\r\n                   'Race',\r\n                   'Gang_Affiliated',\r\n                   'Prior_Arrest_Episodes_DVCharges',\r\n                   'Prior_Arrest_Episodes_GunCharges',\r\n                   'Prior_Conviction_Episodes_Viol',\r\n                   'Prior_Conviction_Episodes_PPViolationCharges',\r\n                   'Prior_Conviction_Episodes_DomesticViolenceCharges',\r\n                   'Prior_Conviction_Episodes_GunCharges',\r\n                   'Prior_Revocations_Parole',\r\n                   'Prior_Revocations_Probation',\r\n                   'Condition_MH_SA',\r\n                   'Condition_Cog_Ed',\r\n                   'Condition_Other',\r\n                   'Violations_ElectronicMonitoring',\r\n                   'Violations_Instruction',\r\n                   'Violations_FailToReport',\r\n                   'Violations_MoveWithoutPermission',\r\n                   'Employment_Exempt') \r\n\r\n  numeric   <- c('Supervision_Risk_Score_First',\r\n                 'Dependents',\r\n                 'Prior_Arrest_Episodes_Felony',\r\n                 'Prior_Arrest_Episodes_Misd',\r\n                 'Prior_Arrest_Episodes_Violent',\r\n                 'Prior_Arrest_Episodes_Property',\r\n                 'Prior_Arrest_Episodes_Drug',\r\n                 'Prior_Arrest_Episodes_PPViolationCharges',\r\n                 'Prior_Conviction_Episodes_Felony',\r\n                 'Prior_Conviction_Episodes_Misd',\r\n                 'Prior_Conviction_Episodes_Prop',\r\n                 'Prior_Conviction_Episodes_Drug',\r\n                 'Delinquency_Reports',\r\n                 'Program_Attendances',\r\n                 'Program_UnexcusedAbsences',\r\n                 'Residence_Changes',\r\n                 'Avg_Days_per_DrugTest',\r\n                 'Jobs_Per_Year')\r\n  \r\n  props      <- c('DrugTests_THC_Positive',\r\n                  'DrugTests_Cocaine_Positive',\r\n                  'DrugTests_Meth_Positive',\r\n                  'DrugTests_Other_Positive',\r\n                  'Percent_Days_Employed')\r\n  \r\n  # Convert all nominal, ordinal, and binary variables to factors\r\n  \r\n  for(i in categorical){\r\n    \r\n    recidivism[,i] <- as.factor(recidivism[,i])\r\n    \r\n  }\r\n\r\n  # Blueprint for processing variables\r\n      \r\n  require(recipes)\r\n  \r\n  blueprint_recidivism <- recipe(x  = recidivism,\r\n                      vars  = c(categorical,numeric,props,outcome,id),\r\n                      roles = c(rep('predictor',48),'outcome','ID')) %>%\r\n    step_indicate_na(all_of(categorical),all_of(numeric),all_of(props)) %>%\r\n    step_zv(all_numeric()) %>%\r\n    step_impute_mean(all_of(numeric),all_of(props)) %>%\r\n    step_impute_mode(all_of(categorical)) %>%\r\n    step_logit(all_of(props),offset=.001) %>%\r\n    step_poly(all_of(numeric),all_of(props),degree=2) %>%\r\n    step_normalize(paste0(numeric,'_poly_1'),\r\n                   paste0(numeric,'_poly_2'),\r\n                   paste0(props,'_poly_1'),\r\n                   paste0(props,'_poly_2')) %>%\r\n    step_dummy(all_of(categorical),one_hot=TRUE) %>%\r\n    step_num2factor(Recidivism_Arrest_Year2,\r\n                    transform = function(x) x + 1,\r\n                    levels=c('No','Yes'))\r\n  \r\n  blueprint_recidivism\r\n\r\n\r\n2. Train/Test Split\r\n\r\n\r\n  loc <- which(recidivism$Training_Sample==1)\r\n\r\n  # Training dataset\r\n\r\n  recidivism_tr  <- recidivism[loc, ]\r\n  dim(recidivism_tr)\r\n  \r\n  # Test dataset\r\n\r\n  recidivism_te  <- recidivism[-loc, ]\r\n  dim(recidivism_te)\r\n\r\n\r\n3.1. Bagged Trees\r\n\r\n\r\n# Cross validation settings \r\n    \r\n    set.seed(10302021) # for reproducibility\r\n    \r\n    recidivism_tr = recidivism_tr[sample(nrow(recidivism_tr)),]\r\n  \r\n  # Create 10 folds with equal size\r\n  \r\n    folds = cut(seq(1,nrow(recidivism_tr)),breaks=10,labels=FALSE)\r\n  \r\n  # Create the list for each fold \r\n  \r\n    my.indices <- vector('list',10)\r\n    for(i in 1:10){\r\n      my.indices[[i]] <- which(folds!=i)\r\n    }\r\n    \r\n      \r\n  cv <- trainControl(method = \"cv\",\r\n                     index  = my.indices,\r\n                     classProbs = TRUE,\r\n                     summaryFunction = mnLogLoss)\r\n\r\n# Grid settings\r\n\r\n  # Notice that I use **'gini'** for splitrule because this is \r\n  # now a classification problem.\r\n  \r\n  grid <- expand.grid(mtry = 142,\r\n                    splitrule='gini',\r\n                    min.node.size=2)\r\n  grid\r\n\r\n# Run the BAGGED Trees with different number of trees \r\n# 5, 20, 40, 60, ..., 200\r\n  \r\n    nbags <- c(5,seq(20,200,20))\r\n    \r\n    bags <- vector('list',length(nbags))\r\n\r\n    for(i in 1:length(nbags)){\r\n      \r\n      bags[[i]] <- caret::train(blueprint_recidivism,\r\n                                data      = recidivism_tr,\r\n                                method    = 'ranger',\r\n                                trControl = cv,\r\n                                tuneGrid  = grid,\r\n                                metric    = 'logLoss',\r\n                                num.trees = nbags[i],\r\n                                max.depth = 60)\r\n    }\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nlogLoss_ <- c()\r\n\r\nfor(i in 1:length(nbags)){\r\n  \r\n  logLoss_[i] = bags[[i]]$results$logLoss\r\n  \r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=nbags,y=logLoss_))+\r\n  xlab('Number ofs')+\r\n  ylab('Negative LogLoss')+\r\n  ylim(c(0.4,2))+\r\n  theme_bw()\r\n\r\n\r\nnbags[which.min(logLoss_)]\r\n\r\n[1] 200\r\n\r\n\r\n\r\n# Predict the probabilities for the observations in the test dataset\r\n\r\npredicted_te <- predict(bags[[11]], recidivism_te, type='prob')\r\n\r\ndim(predicted_te)\r\n\r\n[1] 5460    2\r\n\r\nhead(predicted_te)\r\n\r\n      No    Yes\r\n1 0.9900 0.0100\r\n2 0.6050 0.3950\r\n3 0.6150 0.3850\r\n4 0.5675 0.4325\r\n5 0.7725 0.2275\r\n6 0.6900 0.3100\r\n\r\n# Compute the AUC\r\n\r\nrequire(cutpointr)\r\n\r\ncut.obj <- cutpointr(x     = predicted_te$Yes,\r\n                     class = recidivism_te$Recidivism_Arrest_Year2)\r\n\r\nauc(cut.obj)\r\n\r\n[1] 0.7241845\r\n\r\n# Confusion matrix assuming the threshold is 0.5\r\n\r\npred_class <- ifelse(predicted_te$Yes>.5,1,0)\r\n\r\nconfusion <- table(recidivism_te$Recidivism_Arrest_Year2,pred_class)\r\n\r\nconfusion\r\n\r\n   pred_class\r\n       0    1\r\n  0 3957  189\r\n  1 1125  189\r\n\r\n# True Negative Rate\r\n\r\nconfusion[1,1]/(confusion[1,1]+confusion[1,2])\r\n\r\n[1] 0.9544139\r\n\r\n# False Positive Rate\r\n\r\nconfusion[1,2]/(confusion[1,1]+confusion[1,2])\r\n\r\n[1] 0.04558611\r\n\r\n# True Positive Rate\r\n\r\nconfusion[2,2]/(confusion[2,1]+confusion[2,2])\r\n\r\n[1] 0.1438356\r\n\r\n# Precision\r\n\r\nconfusion[2,2]/(confusion[1,2]+confusion[2,2])\r\n\r\n[1] 0.5\r\n\r\n\r\n-LL\r\nAUC\r\nACC\r\nTPR\r\nTNR\r\nFPR\r\nPRE\r\nBagged Trees\r\n0.506\r\n0.724\r\n0.759\r\n0.144\r\n0.954\r\n0.046\r\n0.500\r\nLogistic Regression\r\n0.510\r\n0.719\r\n0.755\r\n0.142\r\n0.949\r\n0.051\r\n0.471\r\nLogistic Regression with Ridge Penalty\r\n0.511\r\n0.718\r\n0.754\r\n0.123\r\n0.954\r\n0.046\r\n0.461\r\nLogistic Regression with Lasso Penalty\r\n0.509\r\n0.720\r\n0.754\r\n0.127\r\n0.952\r\n0.048\r\n0.458\r\nLogistic Regression with Elastic Net\r\n0.509\r\n0.720\r\n0.753\r\n0.127\r\n0.952\r\n0.048\r\n0.456\r\nKNN\r\n?\r\n?\r\n?\r\n?\r\n?\r\n?\r\n?\r\nDecision Tree\r\n0.558\r\n0.603\r\n0.757\r\n0.031\r\n0.986\r\n0.014\r\n0.423\r\n3.2. Random Forests\r\n\r\n\r\n# Grid settings\r\n\r\ngrid <- expand.grid(mtry = 80,splitrule='gini',min.node.size=2)\r\n\r\n    # The only difference for random forests is that I set mtry = 80\r\n\r\n# Run the Random Forests with different number of trees \r\n# 5, 20, 40, 60, ..., 200\r\n  \r\n   nbags <- c(5,seq(20,200,20))\r\n   bags <- vector('list',length(nbags))\r\n  \r\n    for(i in 1:length(nbags)){\r\n      \r\n      bags[[i]] <- caret::train(blueprint_recidivism,\r\n                                data      = recidivism_tr,\r\n                                method    = 'ranger',\r\n                                trControl = cv,\r\n                                tuneGrid  = grid,\r\n                                metric    = 'logLoss',\r\n                                num.trees = nbags[i],\r\n                                max.depth = 60)\r\n    }\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nlogLoss_ <- c()\r\n\r\nfor(i in 1:length(nbags)){\r\n  \r\n  logLoss_[i] = bags[[i]]$results$logLoss\r\n  \r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=nbags,y=logLoss_))+\r\n  xlab('Number ofs')+\r\n  ylab('Negative LogLoss')+\r\n  ylim(c(0.4,2))+\r\n  theme_bw()\r\n\r\n\r\nnbags[which.min(logLoss_)]\r\n\r\n[1] 180\r\n\r\n\r\n\r\n# Predict the probabilities for the observations in the test dataset\r\n\r\npredicted_te <- predict(bags[[10]], recidivism_te, type='prob')\r\n\r\n# Compute the AUC\r\n\r\ncut.obj <- cutpointr(x     = predicted_te$Yes,\r\n                     class = recidivism_te$Recidivism_Arrest_Year2)\r\n\r\nauc(cut.obj)\r\n\r\n[1] 0.7251904\r\n\r\n# Confusion matrix assuming the threshold is 0.5\r\n\r\npred_class <- ifelse(predicted_te$Yes>.5,1,0)\r\n\r\nconfusion <- table(recidivism_te$Recidivism_Arrest_Year2,pred_class)\r\n\r\nconfusion\r\n\r\n   pred_class\r\n       0    1\r\n  0 3956  190\r\n  1 1113  201\r\n\r\n# True Negative Rate\r\n\r\nconfusion[1,1]/(confusion[1,1]+confusion[1,2])\r\n\r\n[1] 0.9541727\r\n\r\n# False Positive Rate\r\n\r\nconfusion[1,2]/(confusion[1,1]+confusion[1,2])\r\n\r\n[1] 0.0458273\r\n\r\n# True Positive Rate\r\n\r\nconfusion[2,2]/(confusion[2,1]+confusion[2,2])\r\n\r\n[1] 0.152968\r\n\r\n# Precision\r\n\r\nconfusion[2,2]/(confusion[1,2]+confusion[2,2])\r\n\r\n[1] 0.5140665\r\n\r\n\r\n-LL\r\nAUC\r\nACC\r\nTPR\r\nTNR\r\nFPR\r\nPRE\r\nRandom Forests\r\n0.507\r\n0.725\r\n0.761\r\n0.153\r\n0.954\r\n0.046\r\n0.514\r\nBagged Trees\r\n0.506\r\n0.724\r\n0.759\r\n0.144\r\n0.954\r\n0.046\r\n0.500\r\nLogistic Regression\r\n0.510\r\n0.719\r\n0.755\r\n0.142\r\n0.949\r\n0.051\r\n0.471\r\nLogistic Regression with Ridge Penalty\r\n0.511\r\n0.718\r\n0.754\r\n0.123\r\n0.954\r\n0.046\r\n0.461\r\nLogistic Regression with Lasso Penalty\r\n0.509\r\n0.720\r\n0.754\r\n0.127\r\n0.952\r\n0.048\r\n0.458\r\nLogistic Regression with Elastic Net\r\n0.509\r\n0.720\r\n0.753\r\n0.127\r\n0.952\r\n0.048\r\n0.456\r\nKNN\r\n?\r\n?\r\n?\r\n?\r\n?\r\n?\r\n?\r\nDecision Tree\r\n0.558\r\n0.603\r\n0.757\r\n0.031\r\n0.986\r\n0.014\r\n0.423\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:16:58-08:00"
    },
    {
      "path": "lecture-7b.html",
      "title": "Gradient Boosting Trees (GBM)",
      "author": [
        {
          "name": "Cengiz Zopluoglu",
          "url": {}
        }
      ],
      "date": "11/23/2022",
      "contents": "\r\n\r\nContents\r\n1.\r\nUnderstanding the Machinery of GBM with Sequential Development of Trees\r\nfrom Residuals\r\nIteration 0\r\nIteration 1\r\nIteration 2\r\n2. A more\r\nformal introduction of Gradient Boosting Trees\r\n\r\n3.\r\nFitting Gradient Boosting Trees using the gbm\r\npackage\r\n4.\r\nFitting Gradient Boosting Trees using the caret package and\r\nHyperparameter Tuning\r\n5.\r\nPredicting Readability Scores Using Gradient Boosting Trees\r\n6.\r\nPredicting Recidivism Using Gradient Boosting Trees\r\n\r\n[Updated: Sun, Nov 27, 2022 - 14:16:59 ]\r\nIn Bagged Trees or Random Forests models, the trees are developed\r\nindependently by taking a random sample of rows and columns from the\r\ndataset. The main difference between Gradient Boosting Trees and Bagged\r\nTrees or Random Forests is that the trees are developed sequentially,\r\nand each tree model is built upon the errors of the previous tree\r\nmodels. The sequential process of model building and predictions in\r\ngradient-boosted trees can be conceptually demonstrated below.\r\n\r\n\r\n\r\n\r\n\r\n1.\r\nUnderstanding the Machinery of GBM with Sequential Development of Trees\r\nfrom Residuals\r\nLet’s try to implement this idea in a toy dataset we use to predict a\r\nreadability score from the number of sentences.\r\n\r\n\r\nreadability_sub <- read.csv('./data/readability_sub.csv',header=TRUE)\r\n\r\nreadability_sub[,c('V220','V166','target')]\r\n\r\n          V220        V166      target\r\n1  -0.13908258  0.19028091 -2.06282395\r\n2   0.21764143  0.07101288  0.58258607\r\n3   0.05812133  0.03993277 -1.65313060\r\n4   0.02526429  0.18845809 -0.87390681\r\n5   0.22430885  0.06200715 -1.74049148\r\n6  -0.07795373  0.10754109 -3.63993555\r\n7   0.43400714  0.12202360 -0.62284268\r\n8  -0.24364550  0.02454670 -0.34426981\r\n9   0.15893717  0.10422343 -1.12298826\r\n10  0.14496475  0.02339597 -0.99857142\r\n11  0.34222975  0.22065343 -0.87656742\r\n12  0.25219145  0.10865010 -0.03304643\r\n13  0.03532625  0.07549474 -0.49529863\r\n14  0.36410633  0.18675801  0.12453660\r\n15  0.29988593  0.11618323  0.09678258\r\n16  0.19837037  0.08272671  0.38422270\r\n17  0.07807041  0.10235218 -0.58143038\r\n18  0.07935690  0.11618605 -0.34324576\r\n19  0.57000953 -0.02385423 -0.39054205\r\n20  0.34523284  0.09299514 -0.67548411\r\n\r\nIteration 0\r\nWe start with a simple model that uses the average target outcome to\r\npredict the readability for all observations in this toy dataset. We\r\ncalculate the predictions and residuals from this initial\r\nintercept-only model.\r\n\r\n\r\nreadability_sub$pred0 <- mean(readability_sub$target)\r\nreadability_sub$res0  <- readability_sub$target - readability_sub$pred\r\n\r\n\r\nround(readability_sub[,c('V166','V220','target','pred0','res0')],3)\r\n\r\n     V166   V220 target  pred0   res0\r\n1   0.190 -0.139 -2.063 -0.763 -1.300\r\n2   0.071  0.218  0.583 -0.763  1.346\r\n3   0.040  0.058 -1.653 -0.763 -0.890\r\n4   0.188  0.025 -0.874 -0.763 -0.111\r\n5   0.062  0.224 -1.740 -0.763 -0.977\r\n6   0.108 -0.078 -3.640 -0.763 -2.877\r\n7   0.122  0.434 -0.623 -0.763  0.140\r\n8   0.025 -0.244 -0.344 -0.763  0.419\r\n9   0.104  0.159 -1.123 -0.763 -0.360\r\n10  0.023  0.145 -0.999 -0.763 -0.235\r\n11  0.221  0.342 -0.877 -0.763 -0.113\r\n12  0.109  0.252 -0.033 -0.763  0.730\r\n13  0.075  0.035 -0.495 -0.763  0.268\r\n14  0.187  0.364  0.125 -0.763  0.888\r\n15  0.116  0.300  0.097 -0.763  0.860\r\n16  0.083  0.198  0.384 -0.763  1.148\r\n17  0.102  0.078 -0.581 -0.763  0.182\r\n18  0.116  0.079 -0.343 -0.763  0.420\r\n19 -0.024  0.570 -0.391 -0.763  0.373\r\n20  0.093  0.345 -0.675 -0.763  0.088\r\n\r\n# SSE at the end of Iteration 0\r\n\r\nsum(readability_sub$res0^2)\r\n\r\n[1] 17.73309\r\n\r\nIteration 1\r\nNow, we fit a tree model to predict the residuals of Iteration 0 from\r\nthe two predictors (Features V220 and V166). Notice that we fix the\r\nvalue of specific parameters while fitting the tree model (e.g.,\r\ncp, minsplit,maxdepth).\r\n\r\n\r\nrequire(rpart)\r\nrequire(rattle)\r\n\r\nmodel1 <- rpart(formula = res0 ~ V166 + V220,\r\n                data    = readability_sub,\r\n                method  = \"anova\",\r\n                control = list(minsplit=2,\r\n                               cp=0,\r\n                               minbucket = 2,\r\n                               maxdepth = 2)\r\n                )\r\n\r\n\r\nfancyRpartPlot(model1,type=2,sub='')\r\n\r\n\r\n\r\nLet’s see the predictions of residuals from Model 1.\r\n\r\n\r\npr.res <- predict(model1, readability_sub)\r\npr.res \r\n\r\n         1          2          3          4          5          6 \r\n-1.2523541  0.4220391 -0.4323615  0.4220391 -0.4323615 -1.2523541 \r\n         7          8          9         10         11         12 \r\n 0.4220391 -1.2523541  0.4220391 -0.4323615  0.4220391  0.4220391 \r\n        13         14         15         16         17         18 \r\n 0.4220391  0.4220391  0.4220391  0.4220391  0.4220391  0.4220391 \r\n        19         20 \r\n-0.4323615  0.4220391 \r\n\r\nNow, let’s add the predicted residuals from Iteration 1 to the\r\npredictions from Iteration 0 to obtain the new predictions.\r\n\r\n\r\nreadability_sub$pred1  <- readability_sub$pred0 + pr.res\r\nreadability_sub$res1   <- readability_sub$target - readability_sub$pred1\r\n\r\nround(readability_sub[,c('V166','V220','target','pred0','res0','pred1','res1')],3)\r\n\r\n     V166   V220 target  pred0   res0  pred1   res1\r\n1   0.190 -0.139 -2.063 -0.763 -1.300 -2.016 -0.047\r\n2   0.071  0.218  0.583 -0.763  1.346 -0.341  0.924\r\n3   0.040  0.058 -1.653 -0.763 -0.890 -1.196 -0.457\r\n4   0.188  0.025 -0.874 -0.763 -0.111 -0.341 -0.533\r\n5   0.062  0.224 -1.740 -0.763 -0.977 -1.196 -0.545\r\n6   0.108 -0.078 -3.640 -0.763 -2.877 -2.016 -1.624\r\n7   0.122  0.434 -0.623 -0.763  0.140 -0.341 -0.282\r\n8   0.025 -0.244 -0.344 -0.763  0.419 -2.016  1.671\r\n9   0.104  0.159 -1.123 -0.763 -0.360 -0.341 -0.782\r\n10  0.023  0.145 -0.999 -0.763 -0.235 -1.196  0.197\r\n11  0.221  0.342 -0.877 -0.763 -0.113 -0.341 -0.535\r\n12  0.109  0.252 -0.033 -0.763  0.730 -0.341  0.308\r\n13  0.075  0.035 -0.495 -0.763  0.268 -0.341 -0.154\r\n14  0.187  0.364  0.125 -0.763  0.888 -0.341  0.466\r\n15  0.116  0.300  0.097 -0.763  0.860 -0.341  0.438\r\n16  0.083  0.198  0.384 -0.763  1.148 -0.341  0.726\r\n17  0.102  0.078 -0.581 -0.763  0.182 -0.341 -0.240\r\n18  0.116  0.079 -0.343 -0.763  0.420 -0.341 -0.002\r\n19 -0.024  0.570 -0.391 -0.763  0.373 -1.196  0.805\r\n20  0.093  0.345 -0.675 -0.763  0.088 -0.341 -0.334\r\n\r\n# SSE at the end of Iteration 1\r\n\r\nsum(readability_sub$res1^2)\r\n\r\n[1] 9.964654\r\n\r\n\r\n\r\n\r\nIteration 2\r\nWe repeat Iteration 1, but the only difference is that we now fit a\r\ntree model to predict the residuals at the end of Iteration 1.\r\n\r\n\r\nmodel2 <- rpart(formula = res1 ~  V166 + V220,\r\n                data    = readability_sub,\r\n                method  = \"anova\",\r\n                control = list(minsplit=2,\r\n                               cp=0,\r\n                               minbucket = 2,\r\n                               maxdepth = 2)\r\n                )\r\n\r\n\r\nfancyRpartPlot(model2,type=2,sub='')\r\n\r\n\r\n\r\nLet’s see the predictions of residuals from Model 2.\r\n\r\n\r\npr.res <- predict(model2, readability_sub)\r\npr.res\r\n\r\n         1          2          3          4          5          6 \r\n-0.4799134  0.1295162 -0.4799134 -0.4799134  0.1295162 -0.4799134 \r\n         7          8          9         10         11         12 \r\n 0.1295162  0.8912203 -0.4799134  0.8912203  0.1295162  0.1295162 \r\n        13         14         15         16         17         18 \r\n-0.4799134  0.1295162  0.1295162  0.1295162 -0.4799134 -0.4799134 \r\n        19         20 \r\n 0.8912203  0.1295162 \r\n\r\nNow, add the predicted residuals from Iteration 2 to the predictions\r\nfrom Iteration 1 to obtain the new predictions.\r\n\r\n\r\nreadability_sub$pred2  <- readability_sub$pred1 + pr.res\r\nreadability_sub$res2   <- readability_sub$target - readability_sub$pred2\r\n\r\n\r\nround(readability_sub[,c('V166','V220','target',\r\n                         'pred0','res0','pred1','res1',\r\n                         'pred2','res2')],3)\r\n\r\n     V166   V220 target  pred0   res0  pred1   res1  pred2   res2\r\n1   0.190 -0.139 -2.063 -0.763 -1.300 -2.016 -0.047 -2.496  0.433\r\n2   0.071  0.218  0.583 -0.763  1.346 -0.341  0.924 -0.212  0.794\r\n3   0.040  0.058 -1.653 -0.763 -0.890 -1.196 -0.457 -1.676  0.022\r\n4   0.188  0.025 -0.874 -0.763 -0.111 -0.341 -0.533 -0.821 -0.053\r\n5   0.062  0.224 -1.740 -0.763 -0.977 -1.196 -0.545 -1.066 -0.674\r\n6   0.108 -0.078 -3.640 -0.763 -2.877 -2.016 -1.624 -2.496 -1.144\r\n7   0.122  0.434 -0.623 -0.763  0.140 -0.341 -0.282 -0.212 -0.411\r\n8   0.025 -0.244 -0.344 -0.763  0.419 -2.016  1.671 -1.124  0.780\r\n9   0.104  0.159 -1.123 -0.763 -0.360 -0.341 -0.782 -0.821 -0.302\r\n10  0.023  0.145 -0.999 -0.763 -0.235 -1.196  0.197 -0.304 -0.694\r\n11  0.221  0.342 -0.877 -0.763 -0.113 -0.341 -0.535 -0.212 -0.665\r\n12  0.109  0.252 -0.033 -0.763  0.730 -0.341  0.308 -0.212  0.179\r\n13  0.075  0.035 -0.495 -0.763  0.268 -0.341 -0.154 -0.821  0.326\r\n14  0.187  0.364  0.125 -0.763  0.888 -0.341  0.466 -0.212  0.336\r\n15  0.116  0.300  0.097 -0.763  0.860 -0.341  0.438 -0.212  0.309\r\n16  0.083  0.198  0.384 -0.763  1.148 -0.341  0.726 -0.212  0.596\r\n17  0.102  0.078 -0.581 -0.763  0.182 -0.341 -0.240 -0.821  0.240\r\n18  0.116  0.079 -0.343 -0.763  0.420 -0.341 -0.002 -0.821  0.478\r\n19 -0.024  0.570 -0.391 -0.763  0.373 -1.196  0.805 -0.304 -0.086\r\n20  0.093  0.345 -0.675 -0.763  0.088 -0.341 -0.334 -0.212 -0.464\r\n\r\n# SSE at the end of Iteration 2\r\n\r\nsum(readability_sub$res2^2)\r\n\r\n[1] 5.588329\r\n\r\n\r\n\r\n\r\nWe can keep iterating and add tree models as long as we find a tree\r\nmodel that improves our predictions (minimizing SSE).\r\n2. A more\r\nformal introduction of Gradient Boosting Trees\r\nLet \\(\\mathbf{x}_i =\r\n(x_{i1},x_{i2},x_{i3},...,x_{ij})\\) represent a vector of\r\nobserved values for the \\(i^{th}\\)\r\nobservation on \\(j\\) predictor\r\nvariables, and \\(y_i\\) is the value of\r\nthe target outcome for the \\(i^{th}\\)\r\nobservation. A gradient-boosted tree model is an ensemble of \\(T\\) different tree models sequentially\r\ndeveloped, and the final prediction of the outcome is obtained by using\r\nan additive function as\r\n\\[ \\hat{y_i} =\r\n\\sum_{t=1}^{T}f_t(\\mathbf{x}_i),\\]\r\nwhere \\(f_t\\) is a tree model\r\nobtained at Iteration \\(t\\) from the\r\nresiduals at Iteration \\(t-1\\).\r\nThe algorithm optimizes an objective function \\(\\mathfrak{L}(\\mathbf{y},\\mathbf{\\hat{y}})\\)\r\nin an additive manner. This objective loss function can be defined as\r\nthe sum of squared errors when the outcome is continuous or logistic\r\nloss when the outcome is categorical.\r\nThe algorithm starts with a constant prediction. For instance, we\r\nstart with the average outcome in the above example. Then, a new tree\r\nmodel that minimizes the objective loss function is searched and added\r\nat each iteration.\r\n\\[\\hat{y}_{i}^{(0)} = \\bar{y}\\]\r\n\\[\\hat{y}_{i}^{(1)} = \\hat{y}_{i}^{(0)} +\r\n\\alpha f_1(\\mathbf{x}_i)\\]\r\n\\[\\hat{y}_{i}^{(2)} = \\hat{y}_{i}^{(1)} +\r\n\\alpha f_2(\\mathbf{x}_i)\\] \\[.\\] \\[.\\] \\[.\\] \\[\\hat{y}_{i}^{(t)} = \\hat{y}_{i}^{(t-1)} + \\alpha\r\nf_t(\\mathbf{x}_i)\\]\r\nNotice that I added a multiplier, \\(\\alpha\\) while adding our predictions at\r\neach iteration. In the above example, we fixed this multiplier to 1,\r\n\\(\\alpha = 1\\), as we added a whole new\r\nprediction to the previous prediction. This multiplier in machine\r\nlearning literature is called the learning rate. We\r\ncould also choose to add only a fraction of new predictions (e.g., \\(\\alpha = 0.1,0.05,0.01,0.001\\)) at each\r\niteration.\r\nThe smaller the learning rate, the more iterations (more tree models)\r\nwe will need to achieve the same level of performance. So, the number of\r\niterations (number of tree models, \\(T\\)) and the learning rate (\\(\\alpha\\)) play in tandem. These two\r\nparameters are known as the boosting hyperparameters\r\nand need to be tuned.\r\nThink about choosing a learning rate as choosing\r\nyour speed on a highway and number of trees as the time\r\nit takes to arrive at your destination. Suppose you are traveling from\r\nEugene to Portland on I-5. If you drive 40 miles/hour, you are less\r\nlikely to involve in an accident because you are more aware of your\r\nsurroundings, but it will take 3-4 hours to arrive at your destination.\r\nIf you are 200 miles/hour, it will only take an hour to arrive at your\r\ndestination, assuming you will not have an accident on the way (which is\r\nvery likely). So, you try to find a speed level that is fast enough to\r\narrive at your destination and safe enough not to have an accident.\r\n\r\n\r\nTECHNICAL NOTE\r\n\r\nWhy do people call it Gradient Boosting? It turns\r\nout that the updates at each iteration based on the residuals from a\r\nprevious model are related to the concept of negative gradient (first\r\nderivative) of the objective loss function with respect to the predicted\r\nvalues from the previous step.\r\n\\[-g_i^t = -\\frac{\\partial\r\n\\mathfrak{L}(y_i,\\hat{y}_i^{t-1})}{\\partial \\hat{y}_i^{t-1}} =\r\n\\hat{y}_{i}^{(t)} - \\hat{y}_{i}^{(t-1)} \\]\r\nThe general logic of gradient boosting works as\r\ntake a differentiable loss function, \\(\\mathfrak{L}(\\mathbf{y},\\mathbf{\\hat{y}})\\),\r\nthat summarizes the distance between observed and predicted\r\nvalues,\r\nstart with an initial model to obtain initial predictions, \\(f_0(\\mathbf{x}_i)\\),\r\niterate until termination:\r\ncalculate the negative gradients of the loss function with\r\nrespect to predictions from the previous step\r\nfit a tree model to the negative gradients\r\nupdate the predictions (with a multiplier, a.k.a learning\r\nrate).\r\n\r\nMost software uses mathematical approximations and computational\r\nhocus pocus to do these computations for faster implementation.\r\n\r\n3.\r\nFitting Gradient Boosting Trees using the gbm package\r\nThe gradient boosting trees can be fitted using the gbm\r\nfunction from the `gbm package. The code below tries to replicate our\r\nexample above using the toy dataset.\r\n\r\n\r\nrequire(gbm)\r\n\r\ngbm.model <-   gbm(formula           = target ~ V166 + V220,\r\n                   data              = readability_sub,\r\n                   distribution       = 'gaussian',\r\n                   n.trees           = 2,\r\n                   shrinkage         = 1,\r\n                   interaction.depth = 2,\r\n                   n.minobsinnode    = 2,\r\n                   bag.fraction      = 1,\r\n                   cv.folds          = 0,\r\n                   n.cores           = 1)\r\n\r\n\r\nModel and Data:\r\nformula, a description of the outcome and predictive\r\nvariables in the model using column names\r\ndata, the name of the data object to look for the\r\nvariables in the formula statement\r\ndistribution, a character to specify the type of\r\nobjective loss function to optimize. ‘gaussian’ is typically used for\r\ncontinuous outcomes(minimize the squared error), and ‘bernoulli’ is\r\ntypically used for the binary outcomes (minimizes the logistic\r\nloss)\r\nHyperparameters:\r\nn.trees, number of trees to fit (the number of\r\niterations)\r\nshrinkage, learning rate.\r\ninteraction.depth, the maximum depth of each tree\r\ndeveloped at each iteration\r\nn.minobsinnode, the minimum number of observations\r\nin each terminal note of tree models at each iteration\r\nStochastic Gradient Boosting:\r\nbag.fraction, the proportion of observations to be\r\nrandomly selected for developing a new tree at each iteration.\r\nIn Gradient Boosting Trees, we use all observations (100% of rows)\r\nwhen we develop a new tree model at each iteration. So, we can set\r\nbag.fraction=1, and gbm fits a gradient\r\nboosting tree model. On the other hand, adding a random component may\r\nhelp yield better performance. You can think about this as a marriage of\r\nBagging and Boosting. So, we may want to take a random sample of\r\nobservations to develop a tree model at each iteration. For instance, if\r\nyou set bag.fraction=.9, the algorithm will randomly sample\r\n90% of the observations at each iteration before fitting the new tree\r\nmodel to residuals from the previous step. When\r\nbag.fraction is lower than 1, this is called\r\nStochastic Gradient Boosting Trees.\r\nbag.fraction can also be considered a hyperparameter to\r\ntune by trying different values to find an optimal value, or it can be\r\nfixed to a certain number.\r\nCross-validation:\r\ncv.folds, number of cross-validation folds to\r\nperform.\r\nParallel Processing:\r\nn.cores, the number of CPU cores to use.\r\n\r\n\r\n# Obtain predictions from the model\r\n\r\npredict(gbm.model)\r\n\r\n [1] -2.4955898 -0.2117671 -1.6755972 -0.8211966 -1.0661677 -2.4955898\r\n [7] -0.2117671 -1.1244561 -0.8211966 -0.3044636 -0.2117671 -0.2117671\r\n[13] -0.8211966 -0.2117671 -0.2117671 -0.2117671 -0.8211966 -0.8211966\r\n[19] -0.3044636 -0.2117671\r\n\r\n# Plot the final model\r\n\r\nplot(gbm.model,i.var = 1)\r\n\r\n\r\nplot(gbm.model,i.var = 2)\r\n\r\n\r\n\r\n4.\r\nFitting Gradient Boosting Trees using the caret package and\r\nHyperparameter Tuning\r\nThe gbm algorithm is available in the caret\r\npackage. Let’s check the hyperparameters available to tune.\r\n\r\n\r\nrequire(caret)\r\n\r\ngetModelInfo()$gbm$parameters\r\n\r\n          parameter   class                   label\r\n1           n.trees numeric   # Boosting Iterations\r\n2 interaction.depth numeric          Max Tree Depth\r\n3         shrinkage numeric               Shrinkage\r\n4    n.minobsinnode numeric Min. Terminal Node Size\r\n\r\nThe four most critical parameters are all available to tune. It is\r\nvery challenging to find the best combination of values for all these\r\nfour hyperparameters unless you implement a full grid search which may\r\ntake a very long time. You may apply a general sub-optimal strategy to\r\ntune the hyperparameters step by step, either in pairs or one by one.\r\nBelow is one way to implement such a strategy:\r\nFix the interaction.depth and\r\nn.minobsinnode to a certain value (e.g., interaction.depth\r\n= 5, n.minobsinnode = 10),\r\nPick a small value of learning rate (shrinkage),\r\nsuch as 0.05 or 0.1,\r\nDo a grid search and find the optimal number of trees\r\n(n.trees) using the fixed values at #1 and #2,\r\nFix the n.trees at its optimal value from #3, keep\r\nshrinkage the same as in #2, and do a two-dimensional grid\r\nsearch for interaction.depth and\r\nn.minobsinnode and find the optimal number of depth and\r\nminimum observation in a terminal node,\r\nFix the interaction.depth and `n.minobsinnode’at\r\ntheir optimal values from #4, lower the learning rate and increase the\r\nnumber of trees to see if the model performance can be further\r\nimproved.\r\nFix interaction.depth,n.minobsinnode,\r\nshrinkage, and n.trees at their optimal values\r\nfrom previus steps, and do a grid search for\r\nbag.fraction.\r\nYou will find an interactive app you can play at the link below to\r\nunderstand the dynamics among these hyperparameters and optimize them in\r\ntoy examples.\r\nhttp://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html\r\n5.\r\nPredicting Readability Scores Using Gradient Boosting Trees\r\n\r\n\r\n\r\nFirst, we import and prepare data for modeling. Then, we split the\r\ndata into training and test pieces.\r\n\r\n\r\nrequire(recipes)\r\nrequire(caret)\r\n                                                     \r\n# Import the dataset\r\n                                                     \r\nreadability <- read.csv(here('data/readability_features.csv'),header=TRUE)\r\n                                                     \r\n                                                     \r\n# Write the recipe\r\n                                                     \r\nblueprint_readability <- recipe(x     = readability,\r\n                                vars  = colnames(readability),\r\n                                roles = c(rep('predictor',768),'outcome')) %>%\r\n                                step_zv(all_numeric()) %>%\r\n                                step_nzv(all_numeric()) %>%\r\n                                step_normalize(all_numeric_predictors())\r\n                                                     \r\n# Train/Test Split\r\n                                                     \r\nset.seed(10152021)  # for reproducibility\r\n                                                     \r\nloc      <- sample(1:nrow(readability), round(nrow(readability) * 0.9))\r\n\r\nread_tr  <- readability[loc, ]\r\nread_te  <- readability[-loc, ]\r\n                                                     \r\ndim(read_tr)\r\n                                                     \r\ndim(read_te)\r\n\r\n\r\n\r\n[1] 2551  769\r\n[1] 283 769\r\n\r\nPrepare the data partitions for 10-fold cross validation.\r\n\r\n\r\n# Cross validation settings \r\n                                                     \r\nread_tr = read_tr[sample(nrow(read_tr)),]\r\n                                                     \r\n# Create 10 folds with equal size\r\n                                                     \r\nfolds = cut(seq(1,nrow(read_tr)),breaks=10,labels=FALSE)\r\n                                                     \r\n# Create the list for each fold \r\n                                                     \r\nmy.indices <- vector('list',10)\r\n\r\nfor(i in 1:10){\r\n  my.indices[[i]] <- which(folds!=i)\r\n}\r\n                                                     \r\ncv <- trainControl(method = \"cv\",\r\n                   index  = my.indices)\r\n\r\n\r\nSet the multiple cores for parallel processing.\r\n\r\n\r\nrequire(doParallel)\r\n                                                     \r\nncores <- 10\r\n                                                     \r\ncl <- makePSOCKcluster(ncores)\r\n                                                     \r\nregisterDoParallel(cl)\r\n\r\n\r\nStep 1: Tune the number of trees\r\nNow, we will fix the learning rate at 0.1\r\n(shrinkage=0.1), interaction depth at 5\r\n(interaction.depth=5), and the minimum number of\r\nobservations at 10 (n.minobsinnode = 10). We will do a grid\r\nsearch for the number of trees from 1 to 500\r\n(n.trees = 1:500). Note that I fix the bag fraction at one\r\nand passed it as an argument in the caret::train function\r\nbecause it is not allowed in the hyperparameter grid.\r\n\r\n\r\n# Grid Settings  \r\n                                                     \r\ngrid <- expand.grid(shrinkage         = 0.1,\r\n                    n.trees           = 1:500,\r\n                    interaction.depth = 5,\r\n                    n.minobsinnode    = 10)\r\n                                                     \r\n                                                     \r\ngbm1 <- caret::train(blueprint_readability,\r\n                    data         = read_tr,\r\n                    method       = 'gbm',\r\n                    trControl    = cv,\r\n                    tuneGrid     = grid,\r\n                    bag.fraction = 1,\r\n                    verbose      = FALSE)\r\n                                                     \r\ngbm1$times\r\n\r\n\r\n\r\n$everything\r\n   user  system elapsed \r\n  65.00    1.59  160.42 \r\n\r\n$final\r\n   user  system elapsed \r\n  52.47    0.14   52.61 \r\n\r\n$prediction\r\n[1] NA NA NA\r\n\r\nIt took about 3 minutes to run. We can now look at the plot and\r\nexamine how the cross-validated RMSE changes as a function of the number\r\nof trees.\r\n\r\n\r\nplot(gbm1,type='l')\r\n\r\n\r\n\r\nIt indicates there is not much improvement after 300 trees with these\r\nsettings (this is just eyeballing, there is nothing specific about how\r\nto come up with this number). So, I will fix the number of trees to 300\r\nfor the next step.\r\nStep 2: Tune the interaction depth and minimum number of\r\nobservations\r\nNow, we will fix the number of trees at 300\r\n(n.trees = 300) and the learning rate at 0.1\r\n(shrinkage=0.1).\r\nThen, we will do a grid search by assigning values for the\r\ninteraction depth from 1 to 15 and values for the minimum number of\r\nobservations at 5, 10, 20, 30, 40, and 50. We still keep the bag\r\nfraction as 1.\r\n\r\n\r\ngrid <- expand.grid(shrinkage         = 0.1,\r\n                    n.trees           = 300,\r\n                    interaction.depth = 1:15,\r\n                    n.minobsinnode    = c(5,10,20,30,40,50))\r\n                                                     \r\n                                                     \r\ngbm2 <- caret::train(blueprint_readability,\r\n                     data      = read_tr,\r\n                     method    = 'gbm',\r\n                     trControl = cv,\r\n                     tuneGrid  = grid,\r\n                     bag.fraction = 1,\r\n                     verbose = FALSE)\r\n                                                     \r\ngbm2$times\r\n\r\n\r\n\r\n$everything\r\n   user  system elapsed \r\n  87.53    1.55 7433.68 \r\n\r\n$final\r\n   user  system elapsed \r\n  75.17    0.14   75.30 \r\n\r\n$prediction\r\n[1] NA NA NA\r\n\r\nThis search took about 1 hour and 10 minutes. If we look at the\r\ncross-validates RMSE for all these 90 possible conditions, we see that\r\nthe best result comes out when the interaction depth is equal to 9, and\r\nthe minimum number of observations is equal to 50.\r\n\r\n\r\nplot(gbm2,type='l')\r\n\r\n\r\ngbm2$bestTune\r\n\r\n   n.trees interaction.depth shrinkage n.minobsinnode\r\n54     300                 9       0.1             50\r\n\r\ngbm2$results[which.min(gbm2$results$RMSE),]\r\n\r\n   shrinkage interaction.depth n.minobsinnode n.trees      RMSE\r\n54       0.1                 9             50     300 0.5759197\r\n    Rsquared       MAE     RMSESD RsquaredSD      MAESD\r\n54 0.6904758 0.4582852 0.02670278 0.03351747 0.02441325\r\n\r\nStep 3: Lower the learning rate and increase the number of\r\ntrees\r\nNow, we will fix the interaction depth at 9\r\n(interaction.depth = 9) and the minimum number of\r\nobservations at 50 (n.minobsinnode = 50). We will lower the\r\nlearning rate to 0.01 (shrinkage=0.01) and increase the\r\nnumber of trees to 8000 (n.trees = 1:8000) to explore if a\r\nlower learning rate improves the performance.\r\n\r\n\r\ngrid <- expand.grid(shrinkage         = 0.01,\r\n                    n.trees           = 1:8000,\r\n                    interaction.depth = 9,\r\n                    n.minobsinnode    = 50)\r\n                                                     \r\n                                                     \r\ngbm3 <- caret::train(blueprint_readability,\r\n                     data      = read_tr,\r\n                     method    = 'gbm',\r\n                     trControl = cv,\r\n                     tuneGrid  = grid,\r\n                     bag.fraction = 1,\r\n                     verbose= FALSE)\r\n                                                     \r\ngbm3$times\r\n\r\n\r\n\r\n$everything\r\n   user  system elapsed \r\n1703.83    0.94 3821.78 \r\n\r\n$final\r\n   user  system elapsed \r\n1689.69    0.09 1719.50 \r\n\r\n$prediction\r\n[1] NA NA NA\r\n\r\n\r\n\r\nplot(gbm3,type='l')\r\n\r\n\r\ngbm3$bestTune\r\n\r\n     n.trees interaction.depth shrinkage n.minobsinnode\r\n7948    7948                 9      0.01             50\r\n\r\ngbm3$results[which.min(gbm3$results$RMSE),]\r\n\r\n     shrinkage interaction.depth n.minobsinnode n.trees     RMSE\r\n7948      0.01                 9             50    7948 0.568526\r\n      Rsquared      MAE     RMSESD RsquaredSD      MAESD\r\n7948 0.6990292 0.453076 0.02218744 0.02806958 0.01829231\r\n\r\nThis run took about another 40 minutes. The best performance was\r\nobtained with a model of 7948 trees, and yielded an RMSE value of\r\n0.5685. We can stop here and decide that this is our final model. Or, we\r\ncan play with bag.fraction and see if we can improve the\r\nperformance a little more.\r\nStep 4: Tune Bag Fraction\r\nTo play with the bag.fraction, we should write our own\r\nsyntax as caret::train does not allow it to be manipulated\r\nas a hyperparameter.\r\nNotice that I fixed the values of shrinkage,\r\nn.trees,interaction.depth,n.minobsinnode\r\nat their optimal values.\r\nThen, I write a for loop to iterate over different\r\nvalues of bag.fraction from 0.1 to 1 with increments of\r\n0.05. I save the model object from each iteration in a list object.\r\n\r\n\r\ngrid <- expand.grid(shrinkage         = 0.01,\r\n                    n.trees           = 7948,\r\n                    interaction.depth = 9,\r\n                    n.minobsinnode    = 50)\r\n                                                     \r\nbag.fr <- seq(0.1,1,.05)\r\n                                                     \r\nmy.models <- vector('list',length(bag.fr))\r\n                                                     \r\n  for(i in 1:length(bag.fr)){\r\n                                                         \r\n  my.models[[i]] <- caret::train(blueprint_readability,\r\n                                 data      = read_tr,\r\n                                 method    = 'gbm',\r\n                                 trControl = cv,\r\n                                 tuneGrid  = grid,\r\n                                 bag.fraction = bag.fr[i],\r\n                                 verbose= FALSE)\r\n  }\r\n\r\n\r\n\r\n\r\n\r\nIt took about 17 hours to complete with ten cores. Let’s check if it\r\nimproved the performance.\r\n\r\n\r\ncv.rmse <- c()\r\n\r\nfor(i in 1:length(bag.fr)){\r\n  cv.rmse[i] <- my.models[[i]]$results$RMSE\r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=bag.fr,y=cv.rmse))+\r\n  theme_bw()+\r\n  xlab('Bag Fraction')+\r\n  ylab('RMSE (Cross-validated)')+\r\n  scale_x_continuous(breaks = bag.fr)\r\n\r\n\r\n\r\nThe best performance was obtained when bag.fr is equal\r\nto 0.40.\r\nFinally, we can check the performance of the final model with these\r\nsettings on the test dataset and compare it to other methods.\r\n\r\n\r\nfinal.gbm <- my.models[[7]]\r\n\r\n# Predictions from a Bagged tree model with 158 trees\r\n\r\npredicted_te <- predict(final.gbm,read_te)\r\n\r\n# MAE\r\n\r\nmean(abs(read_te$target - predicted_te))\r\n\r\n[1] 0.4479047\r\n\r\n# RMSE\r\n\r\nsqrt(mean((read_te$target - predicted_te)^2))\r\n\r\n[1] 0.5509891\r\n\r\n# R-square\r\n\r\ncor(read_te$target,predicted_te)^2\r\n\r\n[1] 0.7135852\r\n\r\n\r\nR-square\r\nMAE\r\nRMSE\r\nRidge Regression\r\n0.727\r\n0.435\r\n0.536\r\nLasso Regression\r\n0.725\r\n0.434\r\n0.538\r\nGradient Boosting\r\n0.714\r\n0.448\r\n0.551\r\nRandom Forests\r\n0.671\r\n0.471\r\n0.596\r\nBagged Trees\r\n0.656\r\n0.481\r\n0.604\r\nLinear Regression\r\n0.644\r\n0.522\r\n0.644\r\nKNN\r\n0.623\r\n0.500\r\n0.629\r\nDecision Tree\r\n0.497\r\n0.577\r\n0.729\r\n6.\r\nPredicting Recidivism Using Gradient Boosting Trees\r\n\r\n\r\n\r\nThe code below implements a similar strategy and demonstrates how to\r\nfit a Gradient Boosting Tree model for the Recidivism dataset to predict\r\nrecidivism in Year 2.\r\nImport the dataset and initial data preparation\r\n\r\n\r\n# Import data\r\n\r\n  recidivism <- read.csv(here('data/recidivism_y1 removed and recoded.csv'),\r\n                         header=TRUE)\r\n\r\n# List of variable types in the dataset\r\n\r\n  outcome <- c('Recidivism_Arrest_Year2')\r\n  \r\n  id      <- c('ID')\r\n  \r\n  categorical <- c('Residence_PUMA',\r\n                   'Prison_Offense',\r\n                   'Age_at_Release',\r\n                   'Supervision_Level_First',\r\n                   'Education_Level',\r\n                   'Prison_Years',\r\n                   'Gender',\r\n                   'Race',\r\n                   'Gang_Affiliated',\r\n                   'Prior_Arrest_Episodes_DVCharges',\r\n                   'Prior_Arrest_Episodes_GunCharges',\r\n                   'Prior_Conviction_Episodes_Viol',\r\n                   'Prior_Conviction_Episodes_PPViolationCharges',\r\n                   'Prior_Conviction_Episodes_DomesticViolenceCharges',\r\n                   'Prior_Conviction_Episodes_GunCharges',\r\n                   'Prior_Revocations_Parole',\r\n                   'Prior_Revocations_Probation',\r\n                   'Condition_MH_SA',\r\n                   'Condition_Cog_Ed',\r\n                   'Condition_Other',\r\n                   'Violations_ElectronicMonitoring',\r\n                   'Violations_Instruction',\r\n                   'Violations_FailToReport',\r\n                   'Violations_MoveWithoutPermission',\r\n                   'Employment_Exempt') \r\n\r\n  numeric   <- c('Supervision_Risk_Score_First',\r\n                 'Dependents',\r\n                 'Prior_Arrest_Episodes_Felony',\r\n                 'Prior_Arrest_Episodes_Misd',\r\n                 'Prior_Arrest_Episodes_Violent',\r\n                 'Prior_Arrest_Episodes_Property',\r\n                 'Prior_Arrest_Episodes_Drug',\r\n                 'Prior_Arrest_Episodes_PPViolationCharges',\r\n                 'Prior_Conviction_Episodes_Felony',\r\n                 'Prior_Conviction_Episodes_Misd',\r\n                 'Prior_Conviction_Episodes_Prop',\r\n                 'Prior_Conviction_Episodes_Drug',\r\n                 'Delinquency_Reports',\r\n                 'Program_Attendances',\r\n                 'Program_UnexcusedAbsences',\r\n                 'Residence_Changes',\r\n                 'Avg_Days_per_DrugTest',\r\n                 'Jobs_Per_Year')\r\n  \r\n  props      <- c('DrugTests_THC_Positive',\r\n                  'DrugTests_Cocaine_Positive',\r\n                  'DrugTests_Meth_Positive',\r\n                  'DrugTests_Other_Positive',\r\n                  'Percent_Days_Employed')\r\n  \r\n  # Convert all nominal, ordinal, and binary variables to factors\r\n  \r\n  for(i in categorical){\r\n    \r\n    recidivism[,i] <- as.factor(recidivism[,i])\r\n    \r\n  }\r\n\r\n# Write the recipe\r\n  \r\n  require(recipes)\r\n  \r\n  blueprint_recidivism <- recipe(x  = recidivism,\r\n                      vars  = c(categorical,numeric,props,outcome,id),\r\n                      roles = c(rep('predictor',48),'outcome','ID')) %>%\r\n    step_indicate_na(all_of(categorical),all_of(numeric),all_of(props)) %>%\r\n    step_zv(all_numeric()) %>%\r\n    step_impute_mean(all_of(numeric),all_of(props)) %>%\r\n    step_impute_mode(all_of(categorical)) %>%\r\n    step_logit(all_of(props),offset=.001) %>%\r\n    step_poly(all_of(numeric),all_of(props),degree=2) %>%\r\n    step_normalize(paste0(numeric,'_poly_1'),\r\n                   paste0(numeric,'_poly_2'),\r\n                   paste0(props,'_poly_1'),\r\n                   paste0(props,'_poly_2')) %>%\r\n    step_dummy(all_of(categorical),one_hot=TRUE) %>%\r\n    step_num2factor(Recidivism_Arrest_Year2,\r\n                    transform = function(x) x + 1,\r\n                    levels=c('No','Yes'))\r\n  \r\n  blueprint_recidivism\r\n\r\n\r\nTrain/Test Split and Cross-validation Settings\r\n\r\n\r\n# Train/Test Split\r\n  \r\n  loc <- which(recidivism$Training_Sample==1)\r\n\r\n  recidivism_tr  <- recidivism[loc, ]\r\n  recidivism_te  <- recidivism[-loc, ]\r\n  \r\n# Cross validation settings \r\n    \r\n    set.seed(10302021) # for reproducibility\r\n    \r\n    recidivism_tr = recidivism_tr[sample(nrow(recidivism_tr)),]\r\n  \r\n  # Create 10 folds with equal size\r\n  \r\n    folds = cut(seq(1,nrow(recidivism_tr)),breaks=10,labels=FALSE)\r\n  \r\n  # Create the list for each fold \r\n  \r\n    my.indices <- vector('list',10)\r\n    for(i in 1:10){\r\n      my.indices[[i]] <- which(folds!=i)\r\n    }\r\n    \r\n      \r\n  cv <- trainControl(method = \"cv\",\r\n                     index  = my.indices,\r\n                     classProbs = TRUE,\r\n                     summaryFunction = mnLogLoss)\r\n\r\n\r\nStep 1: Initial model fit to tune the number of\r\ntrees\r\nWe fix the learning rate at 0.1 (shrinkage=0.1),\r\ninteraction depth at 5 (interaction.depth=5), and the minimum number of\r\nobservations at 10 (n.minobsinnode = 10). We do a grid\r\nsearch for the optimal number of trees from 1 to 1000\r\n(n.trees = 1:1000).\r\n\r\n\r\ngrid <- expand.grid(shrinkage         = 0.1,\r\n                    n.trees           = 1:1000,\r\n                    interaction.depth = 5,\r\n                    n.minobsinnode    = 10)\r\n\r\n\r\ngbm1 <- caret::train(blueprint_recidivism,\r\n                     data         = recidivism_tr,\r\n                     method       = 'gbm',\r\n                     trControl    = cv,\r\n                     tuneGrid     = grid,\r\n                     bag.fraction = 1,\r\n                     metric       = 'logLoss')\r\n\r\nplot(gbm1,type='l')\r\n\r\ngbm1$bestTune\r\n\r\n\r\n\r\n\r\n    n.trees interaction.depth shrinkage n.minobsinnode\r\n178     178                 5       0.1             10\r\n\r\nIt indicates that a model with 178 trees is optimal at the initial\r\nsearch.\r\nStep 2: Tune the interaction depth and minimum number of\r\nobservations\r\nWe fix the number of trees at 178 (n.trees = 178) and\r\nthe learning rate at 0.1 (shrinkage=0.1). Then, we do a\r\ngrid search by assigning values for the interaction depth from 1 to 15\r\nand the minimum number of observations at 5, 10, 20, 30, 40, and 50.\r\n\r\n\r\ngrid <- expand.grid(shrinkage         = 0.1,\r\n                    n.trees           = 178,\r\n                    interaction.depth = 1:15,\r\n                    n.minobsinnode    = c(5,10,20,30,40,50))\r\n\r\ngbm2 <- caret::train(blueprint_recidivism,\r\n                     data         = recidivism_tr,\r\n                     method       = 'gbm',\r\n                     trControl    = cv,\r\n                     tuneGrid     = grid,\r\n                     bag.fraction = 1,\r\n                     metric       = 'logLoss')\r\n\r\nplot(gbm2)\r\n\r\ngbm2$bestTune\r\n\r\n\r\n\r\n\r\n   n.trees interaction.depth shrinkage n.minobsinnode\r\n41     178                 7       0.1             40\r\n\r\nThe search indicates that the best performance is obtained when the\r\ninteraction depth equals 7 and the minimum number of observations equals\r\n40.\r\nStep 3: Lower the learning rate and increase the number of\r\ntrees\r\nWe fix the interaction depth at 7\r\n(interaction.depth = 4) and the minimum number of\r\nobservations at 40 (n.minobsinnode = 40). We will lower the\r\nlearning rate to 0.01 (shrinkage=0.01) and increase the number of trees\r\nto 10000 (n.trees = 1:10000) to explore if a lower learning\r\nrate improves the performance.\r\n\r\n\r\ngrid <- expand.grid(shrinkage         = 0.01,\r\n                    n.trees           = 1:10000,\r\n                    interaction.depth = 7,\r\n                    n.minobsinnode    = 40)\r\n\r\n\r\ngbm3 <- caret::train(blueprint_recidivism,\r\n                     data         = recidivism_tr,\r\n                     method       = 'gbm',\r\n                     trControl    = cv,\r\n                     tuneGrid     = grid,\r\n                     bag.fraction = 1,\r\n                     metric       = 'logLoss')\r\n\r\nplot(gbm3,type='l')\r\n\r\ngbm3$bestTune\r\n\r\n\r\n\r\n\r\n     n.trees interaction.depth shrinkage n.minobsinnode\r\n1731    1731                 7      0.01             40\r\n\r\nStep 4: Tune the bag fraction\r\n\r\n\r\ngrid <- expand.grid(shrinkage         = 0.01,\r\n                    n.trees           = 1731,\r\n                    interaction.depth = 7,\r\n                    n.minobsinnode    = 40)\r\n\r\nbag.fr <- seq(0.1,1,.05)\r\n\r\nmy.models <- vector('list',length(bag.fr))\r\n\r\nfor(i in 1:length(bag.fr)){\r\n  \r\n  my.models[[i]] <- caret::train(blueprint_recidivism,\r\n                                 data      = recidivism_tr,\r\n                                 method    = 'gbm',\r\n                                 trControl = cv,\r\n                                 tuneGrid  = grid,\r\n                                 bag.fraction = bag.fr[i])\r\n}\r\n\r\n\r\n\r\n\r\ncv.LogL <- c()\r\n\r\nfor(i in 1:length(bag.fr)){\r\n  cv.LogL[i] <- my.models[[i]]$results$logLoss\r\n}\r\n\r\nggplot()+\r\n  geom_line(aes(x=bag.fr,y=cv.LogL))+\r\n  theme_bw()+\r\n  xlab('Bag Fraction')+\r\n  ylab('LogLoss (Cross-validated)')+\r\n  scale_x_continuous(breaks = bag.fr)\r\n\r\n\r\nbag.fr[which.min(cv.LogL)]\r\n\r\n[1] 0.7\r\n\r\nThe best result is obtained when the bag fraction is 0.7. So, we will\r\nproceed with that as our final model.\r\nStep 5: Final Predictions on Test Set\r\n\r\n\r\nfinal.gbm <- my.models[[13]]\r\n\r\n# Predict the probabilities for the observations in the test dataset\r\n\r\npredicted_te <- predict(final.gbm, recidivism_te, type='prob')\r\n\r\nhead(predicted_te)\r\n\r\n         No        Yes\r\n1 0.9479557 0.05204431\r\n2 0.6394094 0.36059065\r\n3 0.7002056 0.29979444\r\n4 0.6725847 0.32741529\r\n5 0.8364610 0.16353899\r\n6 0.7666956 0.23330437\r\n\r\n# Compute the AUC\r\n\r\nrequire(cutpointr)\r\n\r\ncut.obj <- cutpointr(x     = predicted_te$Yes,\r\n                     class = recidivism_te$Recidivism_Arrest_Year2)\r\n\r\nauc(cut.obj)\r\n\r\n[1] 0.7465408\r\n\r\n# Confusion matrix assuming the threshold is 0.5\r\n\r\npred_class <- ifelse(predicted_te$Yes>.5,1,0)\r\n\r\nconfusion <- table(recidivism_te$Recidivism_Arrest_Year2,pred_class)\r\n\r\nconfusion\r\n\r\n   pred_class\r\n       0    1\r\n  0 3957  189\r\n  1 1109  205\r\n\r\n# True Negative Rate\r\n\r\nconfusion[1,1]/(confusion[1,1]+confusion[1,2])\r\n\r\n[1] 0.9544139\r\n\r\n# False Positive Rate\r\n\r\nconfusion[1,2]/(confusion[1,1]+confusion[1,2])\r\n\r\n[1] 0.04558611\r\n\r\n# True Positive Rate\r\n\r\nconfusion[2,2]/(confusion[2,1]+confusion[2,2])\r\n\r\n[1] 0.1560122\r\n\r\n# Precision\r\n\r\nconfusion[2,2]/(confusion[1,2]+confusion[2,2])\r\n\r\n[1] 0.5203046\r\n\r\nComparison of Results\r\n\r\n-LL\r\nAUC\r\nACC\r\nTPR\r\nTNR\r\nFPR\r\nPRE\r\nGradient Boosting Trees\r\n0.499\r\n0.747\r\n0.763\r\n0.156\r\n0.954\r\n0.046\r\n0.520\r\nRandom Forests\r\n0.507\r\n0.725\r\n0.761\r\n0.153\r\n0.954\r\n0.046\r\n0.514\r\nBagged Trees\r\n0.506\r\n0.724\r\n0.759\r\n0.144\r\n0.954\r\n0.046\r\n0.500\r\nLogistic Regression\r\n0.510\r\n0.719\r\n0.755\r\n0.142\r\n0.949\r\n0.051\r\n0.471\r\nLogistic Regression with Ridge Penalty\r\n0.511\r\n0.718\r\n0.754\r\n0.123\r\n0.954\r\n0.046\r\n0.461\r\nLogistic Regression with Lasso Penalty\r\n0.509\r\n0.720\r\n0.754\r\n0.127\r\n0.952\r\n0.048\r\n0.458\r\nLogistic Regression with Elastic Net\r\n0.509\r\n0.720\r\n0.753\r\n0.127\r\n0.952\r\n0.048\r\n0.456\r\nKNN\r\n?\r\n?\r\n?\r\n?\r\n?\r\n?\r\n?\r\nDecision Tree\r\n0.558\r\n0.603\r\n0.757\r\n0.031\r\n0.986\r\n0.014\r\n0.423\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:18:24-08:00"
    },
    {
      "path": "LICENSE.html",
      "author": [],
      "contents": "\r\nAttribution 4.0 International\r\n=======================================================================\r\nCreative Commons Corporation (“Creative Commons”) is not a law firm\r\nand does not provide legal services or legal advice. Distribution of\r\nCreative Commons public licenses does not create a lawyer-client or\r\nother relationship. Creative Commons makes its licenses and related\r\ninformation available on an “as-is” basis. Creative Commons gives no\r\nwarranties regarding its licenses, any material licensed under their\r\nterms and conditions, or any related information. Creative Commons\r\ndisclaims all liability for damages resulting from their use to the\r\nfullest extent possible.\r\nUsing Creative Commons Public Licenses\r\nCreative Commons public licenses provide a standard set of terms and\r\nconditions that creators and other rights holders may use to share\r\noriginal works of authorship and other material subject to copyright and\r\ncertain other rights specified in the public license below. The\r\nfollowing considerations are for informational purposes only, are not\r\nexhaustive, and do not form part of our licenses.\r\n Considerations for licensors: Our public licenses are\r\n intended for use by those authorized to give the public\r\n permission to use material in ways otherwise restricted by\r\n copyright and certain other rights. Our licenses are\r\n irrevocable. Licensors should read and understand the terms\r\n and conditions of the license they choose before applying it.\r\n Licensors should also secure all rights necessary before\r\n applying our licenses so that the public can reuse the\r\n material as expected. Licensors should clearly mark any\r\n material not subject to the license. This includes other CC-\r\n licensed material, or material used under an exception or\r\n limitation to copyright. More considerations for licensors:\r\nwiki.creativecommons.org/Considerations_for_licensors\r\n\r\n Considerations for the public: By using one of our public\r\n licenses, a licensor grants the public permission to use the\r\n licensed material under specified terms and conditions. If\r\n the licensor's permission is not necessary for any reason--for\r\n example, because of any applicable exception or limitation to\r\n copyright--then that use is not regulated by the license. Our\r\n licenses grant only permissions under copyright and certain\r\n other rights that a licensor has authority to grant. Use of\r\n the licensed material may still be restricted for other\r\n reasons, including because others have copyright or other\r\n rights in the material. A licensor may make special requests,\r\n such as asking that all changes be marked or described.\r\n Although not required by our licenses, you are encouraged to\r\n respect those requests where reasonable. More considerations\r\n for the public: \r\nwiki.creativecommons.org/Considerations_for_licensees\r\n=======================================================================\r\nCreative Commons Attribution 4.0 International Public License\r\nBy exercising the Licensed Rights (defined below), You accept and\r\nagree to be bound by the terms and conditions of this Creative Commons\r\nAttribution 4.0 International Public License (“Public License”). To the\r\nextent this Public License may be interpreted as a contract, You are\r\ngranted the Licensed Rights in consideration of Your acceptance of these\r\nterms and conditions, and the Licensor grants You such rights in\r\nconsideration of benefits the Licensor receives from making the Licensed\r\nMaterial available under these terms and conditions.\r\nSection 1 – Definitions.\r\nAdapted Material means material subject to Copyright and Similar\r\nRights that is derived from or based upon the Licensed Material and in\r\nwhich the Licensed Material is translated, altered, arranged,\r\ntransformed, or otherwise modified in a manner requiring permission\r\nunder the Copyright and Similar Rights held by the Licensor. For\r\npurposes of this Public License, where the Licensed Material is a\r\nmusical work, performance, or sound recording, Adapted Material is\r\nalways produced where the Licensed Material is synched in timed relation\r\nwith a moving image.\r\nAdapter’s License means the license You apply to Your Copyright\r\nand Similar Rights in Your contributions to Adapted Material in\r\naccordance with the terms and conditions of this Public\r\nLicense.\r\nCopyright and Similar Rights means copyright and/or similar\r\nrights closely related to copyright including, without limitation,\r\nperformance, broadcast, sound recording, and Sui Generis Database\r\nRights, without regard to how the rights are labeled or categorized. For\r\npurposes of this Public License, the rights specified in Section\r\n2(b)(1)-(2) are not Copyright and Similar Rights.\r\nEffective Technological Measures means those measures that, in\r\nthe absence of proper authority, may not be circumvented under laws\r\nfulfilling obligations under Article 11 of the WIPO Copyright Treaty\r\nadopted on December 20, 1996, and/or similar international\r\nagreements.\r\nExceptions and Limitations means fair use, fair dealing, and/or\r\nany other exception or limitation to Copyright and Similar Rights that\r\napplies to Your use of the Licensed Material.\r\nLicensed Material means the artistic or literary work, database,\r\nor other material to which the Licensor applied this Public\r\nLicense.\r\nLicensed Rights means the rights granted to You subject to the\r\nterms and conditions of this Public License, which are limited to all\r\nCopyright and Similar Rights that apply to Your use of the Licensed\r\nMaterial and that the Licensor has authority to license.\r\nLicensor means the individual(s) or entity(ies) granting rights\r\nunder this Public License.\r\nShare means to provide material to the public by any means or\r\nprocess that requires permission under the Licensed Rights, such as\r\nreproduction, public display, public performance, distribution,\r\ndissemination, communication, or importation, and to make material\r\navailable to the public including in ways that members of the public may\r\naccess the material from a place and at a time individually chosen by\r\nthem.\r\nSui Generis Database Rights means rights other than copyright\r\nresulting from Directive 96/9/EC of the European Parliament and of the\r\nCouncil of 11 March 1996 on the legal protection of databases, as\r\namended and/or succeeded, as well as other essentially equivalent rights\r\nanywhere in the world.\r\nYou means the individual or entity exercising the Licensed Rights\r\nunder this Public License. Your has a corresponding meaning.\r\nSection 2 – Scope.\r\nLicense grant.\r\nSubject to the terms and conditions of this Public License, the\r\nLicensor hereby grants You a worldwide, royalty-free, non-sublicensable,\r\nnon-exclusive, irrevocable license to exercise the Licensed Rights in\r\nthe Licensed Material to:\r\nreproduce and Share the Licensed Material, in whole or in part;\r\nand\r\nproduce, reproduce, and Share Adapted Material.\r\n\r\nExceptions and Limitations. For the avoidance of doubt, where\r\nExceptions and Limitations apply to Your use, this Public License does\r\nnot apply, and You do not need to comply with its terms and\r\nconditions.\r\nTerm. The term of this Public License is specified in Section\r\n6(a).\r\nMedia and formats; technical modifications allowed. The Licensor\r\nauthorizes You to exercise the Licensed Rights in all media and formats\r\nwhether now known or hereafter created, and to make technical\r\nmodifications necessary to do so. The Licensor waives and/or agrees not\r\nto assert any right or authority to forbid You from making technical\r\nmodifications necessary to exercise the Licensed Rights, including\r\ntechnical modifications necessary to circumvent Effective Technological\r\nMeasures. For purposes of this Public License, simply making\r\nmodifications authorized by this Section 2(a)\r\nnever produces Adapted Material.\r\n\r\nDownstream recipients.\r\nOffer from the Licensor – Licensed Material. Every recipient of\r\nthe Licensed Material automatically receives an offer from the Licensor\r\nto exercise the Licensed Rights under the terms and conditions of this\r\nPublic License.\r\nNo downstream restrictions. You may not offer or impose any\r\nadditional or different terms or conditions on, or apply any Effective\r\nTechnological Measures to, the Licensed Material if doing so restricts\r\nexercise of the Licensed Rights by any recipient of the Licensed\r\nMaterial.\r\n\r\nNo endorsement. Nothing in this Public License constitutes or may\r\nbe construed as permission to assert or imply that You are, or that Your\r\nuse of the Licensed Material is, connected with, or sponsored, endorsed,\r\nor granted official status by, the Licensor or others designated to\r\nreceive attribution as provided in Section 3(a)(1)(A)(i).\r\n\r\nOther rights.\r\nMoral rights, such as the right of integrity, are not licensed\r\nunder this Public License, nor are publicity, privacy, and/or other\r\nsimilar personality rights; however, to the extent possible, the\r\nLicensor waives and/or agrees not to assert any such rights held by the\r\nLicensor to the limited extent necessary to allow You to exercise the\r\nLicensed Rights, but not otherwise.\r\nPatent and trademark rights are not licensed under this Public\r\nLicense.\r\nTo the extent possible, the Licensor waives any right to collect\r\nroyalties from You for the exercise of the Licensed Rights, whether\r\ndirectly or through a collecting society under any voluntary or waivable\r\nstatutory or compulsory licensing scheme. In all other cases the\r\nLicensor expressly reserves any right to collect such\r\nroyalties.\r\n\r\nSection 3 – License Conditions.\r\nYour exercise of the Licensed Rights is expressly made subject to the\r\nfollowing conditions.\r\nAttribution.\r\nIf You Share the Licensed Material (including in modified form),\r\nYou must:\r\nretain the following if it is supplied by the Licensor with the\r\nLicensed Material:\r\nidentification of the creator(s) of the Licensed Material and any\r\nothers designated to receive attribution, in any reasonable manner\r\nrequested by the Licensor (including by pseudonym if\r\ndesignated);\r\na copyright notice;\r\na notice that refers to this Public License;\r\na notice that refers to the disclaimer of warranties;\r\na URI or hyperlink to the Licensed Material to the extent\r\nreasonably practicable;\r\n\r\nindicate if You modified the Licensed Material and retain an\r\nindication of any previous modifications; and\r\nindicate the Licensed Material is licensed under this Public\r\nLicense, and include the text of, or the URI or hyperlink to, this\r\nPublic License.\r\n\r\nYou may satisfy the conditions in Section 3(a)(1) in any\r\nreasonable manner based on the medium, means, and context in which You\r\nShare the Licensed Material. For example, it may be reasonable to\r\nsatisfy the conditions by providing a URI or hyperlink to a resource\r\nthat includes the required information.\r\nIf requested by the Licensor, You must remove any of the\r\ninformation required by Section 3(a)(1)(A) to the extent reasonably\r\npracticable.\r\nIf You Share Adapted Material You produce, the Adapter’s License\r\nYou apply must not prevent recipients of the Adapted Material from\r\ncomplying with this Public License.\r\n\r\nSection 4 – Sui Generis Database Rights.\r\nWhere the Licensed Rights include Sui Generis Database Rights that\r\napply to Your use of the Licensed Material:\r\nfor the avoidance of doubt, Section 2(a)(1) grants You the right\r\nto extract, reuse, reproduce, and Share all or a substantial portion of\r\nthe contents of the database;\r\nif You include all or a substantial portion of the database\r\ncontents in a database in which You have Sui Generis Database Rights,\r\nthen the database in which You have Sui Generis Database Rights (but not\r\nits individual contents) is Adapted Material; and\r\nYou must comply with the conditions in Section 3(a) if You Share\r\nall or a substantial portion of the contents of the database.\r\nFor the avoidance of doubt, this Section 4 supplements and does not\r\nreplace Your obligations under this Public License where the Licensed\r\nRights include other Copyright and Similar Rights.\r\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\r\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE\r\nEXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND\r\nAS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND\r\nCONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY,\r\nOR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE,\r\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT,\r\nABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE\r\nOF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF\r\nWARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT\r\nAPPLY TO YOU.\r\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE\r\nTO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE)\r\nOR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL,\r\nCONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR\r\nDAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED\r\nMATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF\r\nSUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF\r\nLIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT\r\nAPPLY TO YOU.\r\nThe disclaimer of warranties and limitation of liability provided\r\nabove shall be interpreted in a manner that, to the extent possible,\r\nmost closely approximates an absolute disclaimer and waiver of all\r\nliability.\r\nSection 6 – Term and Termination.\r\nThis Public License applies for the term of the Copyright and\r\nSimilar Rights licensed here. However, if You fail to comply with this\r\nPublic License, then Your rights under this Public License terminate\r\nautomatically.\r\nWhere Your right to use the Licensed Material has terminated\r\nunder Section 6(a), it reinstates:\r\nautomatically as of the date the violation is cured, provided it\r\nis cured within 30 days of Your discovery of the violation; or\r\nupon express reinstatement by the Licensor.\r\nFor the avoidance of doubt, this Section 6(b) does not affect any\r\nright the Licensor may have to seek remedies for Your violations of this\r\nPublic License.\r\nFor the avoidance of doubt, the Licensor may also offer the\r\nLicensed Material under separate terms or conditions or stop\r\ndistributing the Licensed Material at any time; however, doing so will\r\nnot terminate this Public License.\r\nSections 1, 5, 6, 7, and 8 survive termination of this Public\r\nLicense.\r\nSection 7 – Other Terms and Conditions.\r\nThe Licensor shall not be bound by any additional or different\r\nterms or conditions communicated by You unless expressly\r\nagreed.\r\nAny arrangements, understandings, or agreements regarding the\r\nLicensed Material not stated herein are separate from and independent of\r\nthe terms and conditions of this Public License.\r\nSection 8 – Interpretation.\r\nFor the avoidance of doubt, this Public License does not, and\r\nshall not be interpreted to, reduce, limit, restrict, or impose\r\nconditions on any use of the Licensed Material that could lawfully be\r\nmade without permission under this Public License.\r\nTo the extent possible, if any provision of this Public License\r\nis deemed unenforceable, it shall be automatically reformed to the\r\nminimum extent necessary to make it enforceable. If the provision cannot\r\nbe reformed, it shall be severed from this Public License without\r\naffecting the enforceability of the remaining terms and\r\nconditions.\r\nNo term or condition of this Public License will be waived and no\r\nfailure to comply consented to unless expressly agreed to by the\r\nLicensor.\r\nNothing in this Public License constitutes or may be interpreted\r\nas a limitation upon, or waiver of, any privileges and immunities that\r\napply to the Licensor or You, including from the legal processes of any\r\njurisdiction or authority.\r\n=======================================================================\r\nCreative Commons is not a party to its public licenses.\r\nNotwithstanding, Creative Commons may elect to apply one of its public\r\nlicenses to material it publishes and in those instances will be\r\nconsidered the “Licensor.” The text of the Creative Commons public\r\nlicenses is dedicated to the public domain under the CC0 Public Domain\r\nDedication. Except for the limited purpose of indicating that material\r\nis shared under a Creative Commons public license or as otherwise\r\npermitted by the Creative Commons policies published at\r\ncreativecommons.org/policies, Creative Commons does not authorize the\r\nuse of the trademark “Creative Commons” or any other trademark or logo\r\nof Creative Commons without its prior written consent including, without\r\nlimitation, in connection with any unauthorized modifications to any of\r\nits public licenses or any other arrangements, understandings, or\r\nagreements concerning use of licensed material. For the avoidance of\r\ndoubt, this paragraph does not form part of the public licenses.\r\nCreative Commons may be contacted at creativecommons.org.\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:18:26-08:00"
    },
    {
      "path": "schedule.html",
      "title": "Schedule",
      "author": [],
      "contents": "\r\n\r\nContents\r\nWeek 1:\r\nIntroduction\r\nWeek 2: Data Preprocessing\r\nWeek\r\n3 & Week 4: Introduction to Linear Regression, Bias/Variance\r\nTradeoff, and Cross-validation\r\nWeek 5: Regularized Linear\r\nRegression\r\nWeek 6 & 7:\r\n(Regularized) Logistic Regression\r\nWeek\r\n8: Introduction to K-Nearest Neighbors and Decision Tree\r\nAlgorithms\r\nWeek\r\n9: Introduction to Bagged Trees, Random Forests, and Gradient Boosting\r\nTrees\r\n\r\n\r\n\r\n\r\nWeek 1: Introduction\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Supplemental Readings\r\n\r\n\r\nAssignments\r\n\r\n\r\n3-Oct\r\n\r\n\r\nLecture-1\r\n\r\n\r\nNotebook-1\r\n\r\n\r\nWeek\r\n1\r\n\r\n\r\nKaggle\r\nCommonLit Readability\r\nCompetitionNIJ\r\nRecidivism Challenge\r\n\r\n\r\n\r\n\r\n\r\nWeek 2: Data Preprocessing\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Supplemental Readings\r\n\r\n\r\nAssignments\r\n\r\n\r\n10-Oct\r\n\r\n\r\nLecture-2aLecture-2b\r\n\r\n\r\nNotebook-2aNotebook-2b\r\nThe {recipes} demo\r\n\r\n\r\nWeek\r\n2\r\n\r\n\r\nKuhn\r\n& Johnson, Ch.\r\n6Kuhn\r\n& Johnson, Ch.\r\n5Boehmke\r\n& Greenwell, Ch. 3\r\n\r\n\r\nAssignment\r\n1\r\n\r\n\r\nWeek\r\n3 & Week 4: Introduction to Linear Regression, Bias/Variance\r\nTradeoff, and Cross-validation\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Supplemental Readings\r\n\r\n\r\nAssignments\r\n\r\n\r\n17-Oct & 24-Oct\r\n\r\n\r\nLecture-3aLecture-3b\r\n\r\n\r\nNotebook-3a\r\nNotebook-3b\r\nBuilding a Linear Model with caret\r\n\r\n\r\nWeek\r\n3 & 4\r\n\r\n\r\n\r\nBoehmke & Greenwell, Ch.\r\n2Boehmke\r\n& Greenwell, Ch.\r\n4\r\nJames et\r\nal. Ch.3\r\nKuhn and Johnson, APM, Ch. 4, 5.1, and Ch.6\r\n\r\n\r\n\r\n\r\n\r\nWeek 5: Regularized Linear\r\nRegression\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Supplemental Readings\r\n\r\n\r\nAssignments\r\n\r\n\r\n31-Oct\r\n\r\n\r\nLecture-4\r\n\r\n\r\nNotebook-4\r\nBuilding a Ridge Regression\r\nModelBuilding\r\na Lasso Regression\r\nModel\r\nBuilding an Elastic Net\r\nModel\r\nUsing the Prediction Models for a New Text\r\n\r\n\r\nWeek\r\n5\r\n\r\n\r\n\r\nBoehmke & Greenwell, Ch.\r\n6\r\nJames et al., Ch.6.2\r\n\r\n\r\n\r\n\r\n\r\nWeek 6 & 7:\r\n(Regularized) Logistic Regression\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Supplemental Readings\r\n\r\n\r\nAssignments\r\n\r\n\r\n7-Nov & 14-Nov\r\n\r\n\r\nLecture-5\r\n\r\n\r\nBuilding\r\na Logistic Regression\r\nModelBuilding\r\na Classification Model with Ridge\r\nPenaltyBuilding\r\na Classification Model with Lasso\r\nPenaltyBuilding\r\na Classification Model with Elastic Net\r\n\r\n\r\nWeek\r\n6 & 7\r\n\r\n\r\nBoehmke\r\n& Greenwell, Ch.\r\n5\r\nJames et al., Ch.\r\n4.3\r\nAPM, Ch. 12.2 and 12.5\r\n\r\n\r\nAssignment\r\n2\r\n\r\n\r\nWeek\r\n8: Introduction to K-Nearest Neighbors and Decision Tree Algorithms\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Supplemental Readings\r\n\r\n\r\nAssignments\r\n\r\n\r\n21-Nov\r\n\r\n\r\nLecture-6aLecture-6b\r\n\r\n\r\nBuilding\r\na Prediction Model with K-nearest\r\nneighborsBuilding\r\na Classification Model with K-nearest\r\nneighborsBuilding\r\na Prediction Model with a Decision\r\nTreeBuilding\r\na Classification Model with a Decision Tree\r\n\r\n\r\nWeek\r\n8_Part\r\n1Week\r\n8_Part 2\r\n\r\n\r\nBoehmke\r\n& Greenwell, Ch.\r\n8\r\nBoehmke & Greenwell, Ch.\r\n9\r\nAPM Ch.\r\n13.5\r\nJames et al. Ch. 8.1\r\n\r\n\r\n\r\n\r\n\r\nWeek\r\n9: Introduction to Bagged Trees, Random Forests, and Gradient Boosting\r\nTrees\r\n\r\n\r\nDate\r\n\r\n\r\nNotes\r\n\r\n\r\nKaggle Notebooks\r\n\r\n\r\nSlides\r\n\r\n\r\nOptional Supplemental Readings\r\n\r\n\r\nAssignments\r\n\r\n\r\n28-Nov\r\n\r\n\r\nLecture-7aLecture-7b\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBoehmke\r\n& Greenwell, Ch.\r\n10\r\nBoehmke & Greenwell, Ch.\r\n11\r\nBoehmke & Greenwell, Ch.\r\n12\r\nAPM Ch. 8 &\r\n14\r\nJames et al. Ch. 8.2\r\n\r\n\r\nAssignment\r\n3\r\n\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:18:30-08:00"
    },
    {
      "path": "syllabus.html",
      "title": "Course Syllabus",
      "author": [],
      "contents": "\r\nCourse Title : Machine Learning for Educational Data\r\nScience\r\nCourse Number : EDLD 654\r\nTerm : Fall 2022\r\nDate/Time : Monday, 13:00 pm – 15:50 pm\r\nPlace : Lokey 176 (Click for map)\r\nInstructor : Dr. Cengiz Zopluoglu (Click for\r\npronunciation)\r\nE-mail : cengiz@uoregon.edu\r\nDownload\r\nthe complete course syllabus\r\n\r\n\r\n\r\n",
      "last_modified": "2022-11-27T14:18:32-08:00"
    }
  ],
  "collections": []
}
